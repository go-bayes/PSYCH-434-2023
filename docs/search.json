[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Welcome to the Psych 434 lecture website.\nYou can access the course lectures by navigating to the “Contents” section and selecting the appropriate tab"
  },
  {
    "objectID": "content/01.content.html",
    "href": "content/01.content.html",
    "title": "Psych 434",
    "section": "",
    "text": "here"
  },
  {
    "objectID": "content/01.content.html#assumptions",
    "href": "content/01.content.html#assumptions",
    "title": "Psych 434",
    "section": "Assumptions",
    "text": "Assumptions\nhere"
  },
  {
    "objectID": "content/01.content.html#slides",
    "href": "content/01.content.html#slides",
    "title": "Psych 434",
    "section": "Slides",
    "text": "Slides\nhere"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Preliminaries",
    "section": "",
    "text": "This website is for the lecture slides for psych 434.\n\n\n\nReuseMIT"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\nReuseMIT"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Figure 1: A caual graph"
  },
  {
    "objectID": "content/01-content.html#assumptions",
    "href": "content/01-content.html#assumptions",
    "title": "Causal effects",
    "section": "Assumptions",
    "text": "Assumptions\nhere"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/01-content.html#overview",
    "href": "content/01-content.html#overview",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "slides/01-slides.html",
    "href": "slides/01-slides.html",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Consider ethical problems that cultural research might present.\nUnderstand the special problems that cultural research presents for measurement.\nUnderstand the concept of confounding.\n\n\n\n\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/01-slides.html#why-questions",
    "href": "slides/01-slides.html#why-questions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Why questions",
    "text": "Why questions\nWhy why?"
  },
  {
    "objectID": "slides/01-slides.html#test",
    "href": "slides/01-slides.html#test",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "slides/01-slides.html#slide-here",
    "href": "slides/01-slides.html#slide-here",
    "title": "How to ask a Why Question",
    "section": "Slide here",
    "text": "Slide here\nSome code\nlibrary(ggdag)\nlibrary(dplyr)\n\ncoords <- read.table(header = TRUE, text = \"\n  x y name\n  0 1 U\n  1 0 L0\n  2 0 A0\n  3 0 Y0\n  4 0 A1\n  5 0 Y2\n\")\n\nd <- dagify(\n  A1 ~ L0 + A0,\n  L0 ~ U,\n  Y0 ~ U,\n  Y2 ~ A1,\n  Y2 ~ U + Y0,\n  coords = coords)\n\nggdag(d) +\n   theme_dag()"
  },
  {
    "objectID": "slides/01-slides.html#dags",
    "href": "slides/01-slides.html#dags",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Dags",
    "text": "Dags"
  },
  {
    "objectID": "slides/01-slides.html#code",
    "href": "slides/01-slides.html#code",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Code",
    "text": "Code\nlibrary(ggdag)\nlibrary(dplyr)\n\n\ncoords <- read.table(header = TRUE, text = \"\n  x y name\n  0 1 U\n  1 0 L0\n  2 0 A0\n  3 0 Y0\n  4 0 A1\n  5 0 Y2\n\")\n\nd <- dagify(\n  A1 ~ L0 + A0,\n  L0 ~ U,\n  Y0 ~ U,\n  Y2 ~ A1,\n  Y2 ~ U + Y0,\n  coords = coords)\n\nggdag(d) +\n   theme_dag()"
  },
  {
    "objectID": "slides/01-slides.html#test2",
    "href": "slides/01-slides.html#test2",
    "title": "How to ask a Why Question",
    "section": "Test2",
    "text": "Test2\n\\[ E[E[Y|A]] = E[E[A|Y]|Y] \\] ## Dags"
  },
  {
    "objectID": "slides/01-slides.html#test-dag",
    "href": "slides/01-slides.html#test-dag",
    "title": "Asking questions in cross-cultural psychology",
    "section": "test dag",
    "text": "test dag\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\",\"shape\"))\ndevtools::install_github(\"rmcelreath/rethinking\")\nlibrary(rethinking)\ndrawdag(d, arrow = \"gray\",  Z=list(\"U\"))"
  },
  {
    "objectID": "slides/01-slides.html#dag",
    "href": "slides/01-slides.html#dag",
    "title": "Asking questions in cross-cultural psychology",
    "section": "DAG",
    "text": "DAG"
  },
  {
    "objectID": "slides/01-slides.html#check-confounding",
    "href": "slides/01-slides.html#check-confounding",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Check confounding",
    "text": "Check confounding"
  },
  {
    "objectID": "slides/01-slides.html#visualise-confounding",
    "href": "slides/01-slides.html#visualise-confounding",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Visualise confounding",
    "text": "Visualise confounding"
  },
  {
    "objectID": "index.html#class-times-and-locations",
    "href": "index.html#class-times-and-locations",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Class times and locations",
    "text": "Class times and locations\nLectures Day/Time: Tuesday, 1:10-3:00pm\nLecture Location All classes are in Easterfield Building EA407"
  },
  {
    "objectID": "index.html#names-and-contact-details",
    "href": "index.html#names-and-contact-details",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Names and contact details",
    "text": "Names and contact details\nCourse Coordinator Prof Joseph Bulbulia joseph.bulbulia@vuw.ac.nz\nRoom EA324\nLecturers Dr.Inkuk Kim inkuk.kim@vuw.ac.nz\nLecturers Torven Schalk torven.schalk@vuw.ac.nz"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Course Description",
    "text": "Course Description\nThe official description:\nThis course will focus on theoretical and practical challenges for conducting research involving individuals from more than one cultural background or ethnicity. Topics are likely to include defining and measuring culture; developing culture-sensitive studies, choice of language and translation; communication styles and bias; questionnaire and interview design; qualitative and quantitative data analysis for cultural and cross-cultural research; minorities, power and ethics in cross-cultural research; and ethno-methodologies and indigenous research methodologies. Appropriate background for this course: PSYC 338."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Learning objectives",
    "text": "Learning objectives\nThis course is an introduction to conducting research in cross-cultural psychology. Our goal is to provide you with the tools you will need to ask, and answer, psychological questions about culture."
  },
  {
    "objectID": "index.html#course-learning-objectives",
    "href": "index.html#course-learning-objectives",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nEthical Reasoning. Beginning in week 1, we will explore questions that are typically reserved for philosophy courses but are central to the conduct of psychology: why do we need to think about right and wrong in science? What if different people have different ideas about what is right and wrong in science? Is there hope for ethical progress? Can science play a role in ethical progress? We will provide you with a set of strategies for addressing these questions.\nProgramming in R. Beginning in week 2, we will teach you the basics of programming in the statistical language R. The past several decades have brought extraordinary new tools to psychological scientists, many of which require literacy in computer programming. While some of you may find the thought of programming thrilling, others may find it terrifying or boring! We promise that programming can be fun. This course is designed to help you find that joy.\nUnderstanding measurement. Beginning in week 3 the course will turn its focus to developing skills for constructing and validating measures in cross-cultural research. Again these skills will be invaluable for a wide range of tasks you may face in psychological science and will help you to address problems that will arise in other areas of your research and work. Measurement will occupy our attention through week 6.\nUnderstanding causal inference. Beginning in week 7 and for the following four weeks, the course will impart skills that you require to disentangle causation from correlation. Again our focus will be on the special problems that arise for cultural datasets. Notably, in psychological science, causal inference remains underdeveloped, and the material in this part of the course will position you to make potentially important contributions, whether or not your interests lie in cross-cultural psychology."
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Getting started with R",
    "section": "Slides",
    "text": "Slides\nTorven’s slides will be on Nuku/Canvas."
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Dealing with Data (Partial Invariance)",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Future Horizons",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Ethics and ownership",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Causal Inference: Target Trial Emulation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/index.html#slides",
    "href": "content/index.html#slides",
    "title": "Preliminaries",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Lost in Translation? Measurement and Language",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Dealing with Data (Factor Analysis)",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Dealing with Data (Multi-Group CFA)",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "slides/01-slides.html#psychology-begins-with-questions",
    "href": "slides/01-slides.html#psychology-begins-with-questions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Psychology Begins with Questions",
    "text": "Psychology Begins with Questions"
  },
  {
    "objectID": "slides/01-slides.html#examples-of-psychological-questions",
    "href": "slides/01-slides.html#examples-of-psychological-questions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Examples of psychological questions?",
    "text": "Examples of psychological questions?\n\nHow does early childhood experience affect personality and behaviour?\nWhat are the effects of social media on self-esteem?\nWhy do some people believe in a God or gods and others do not?\nWhy are some people motivated to sacrifice for others?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/index.html#examples-of-psychological-questions",
    "href": "slides/index.html#examples-of-psychological-questions",
    "title": "Preliminaries",
    "section": "Examples of Psychological Questions",
    "text": "Examples of Psychological Questions"
  },
  {
    "objectID": "slides/index.html#where-do-we-meet",
    "href": "slides/index.html#where-do-we-meet",
    "title": "Preliminaries",
    "section": "Where do we meet?",
    "text": "Where do we meet?\n\nClass times and locations Lectures Day/Time: Tuesday, 1:10-3:00pm\nLecture Location: Easterfield Building EA407"
  },
  {
    "objectID": "slides/index.html#assignments-and-due-dates",
    "href": "slides/index.html#assignments-and-due-dates",
    "title": "Preliminaries",
    "section": "Assignments and due dates",
    "text": "Assignments and due dates\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n*\n10\nWeekly\n\n\nTake-home\n3,4\n10\nMarch 19 (w3)\n\n\nTheoretical application\n1,2,3\n30\n9 May (w9)\n\n\nPre-reg review\n1,2,3,4\n10\n28 May (w11)\n\n\nStatistical application\n1,2,3\n30\n11 June (w13)\n\n\nLetter to the reviewer\n1,2,3\n10\n11 June (w13)"
  },
  {
    "objectID": "slides/index.html#class-participation",
    "href": "slides/index.html#class-participation",
    "title": "Preliminaries",
    "section": "Class Participation",
    "text": "Class Participation\n\n\nyour willingness to ask/answer questions and generally contribute to class discussion; and\n\n\nthe quality of your contributions"
  },
  {
    "objectID": "slides/index.html#take-home-research-concepts-workbook",
    "href": "slides/index.html#take-home-research-concepts-workbook",
    "title": "Preliminaries",
    "section": "Take Home Research Concepts Workbook",
    "text": "Take Home Research Concepts Workbook\n\nThis test will help you revise core statistical and methodological issues. It is a chance to brush up on basic statistical knowledge and will help you re-familiarize yourself with material that we will build on later in this course.\nThe key focus is the revision of basic terms, including correlation, regression, and on basic R applications.\nThe test contains a theoretical and a practical component. Therefore, it is essential that you complete these questions since you will need to understand the material for the more advanced techniques and approaches that we cover in this course.\nThere is no word limit (be reasonable)\nYou can use any source that you find useful.\nArtificially Intelligence Chatbots mau be used but they occasionally make errors – sometimes spectacular errors. Currently they make these errors confidently\nGenerally, it is better practice, at least for now, to use peer-reviewed publications.\nWhatever resource you use must be cited. Failure to cite your source is a form of academic misconduct.\nYou may complete the test in your own study time. We ask that you work individually."
  },
  {
    "objectID": "slides/extra-slides.html",
    "href": "slides/extra-slides.html",
    "title": "How to ask a Why Question",
    "section": "",
    "text": "Why why?"
  },
  {
    "objectID": "slides/extra-slides.html#test",
    "href": "slides/extra-slides.html#test",
    "title": "How to ask a Why Question",
    "section": "Test",
    "text": "Test\nAt the core:\n[ Y|A=1 Y_{0} ]"
  },
  {
    "objectID": "slides/extra-slides.html#dags",
    "href": "slides/extra-slides.html#dags",
    "title": "How to ask a Why Question",
    "section": "Dags",
    "text": "Dags"
  },
  {
    "objectID": "slides/extra-slides.html#code",
    "href": "slides/extra-slides.html#code",
    "title": "How to ask a Why Question",
    "section": "Code",
    "text": "Code\nlibrary(ggdag)\nlibrary(dplyr)\n\n\ncoords <- read.table(header = TRUE, text = \"\n  x y name\n  0 1 U\n  1 0 L0\n  2 0 A0\n  3 0 Y0\n  4 0 A1\n  5 0 Y2\n\")\n\nd <- dagify(\n  A1 ~ L0 + A0,\n  L0 ~ U,\n  Y0 ~ U,\n  Y2 ~ A1,\n  Y2 ~ U + Y0,\n  coords = coords)\n\nggdag(d) +\n   theme_dag()"
  },
  {
    "objectID": "slides/extra-slides.html#dag",
    "href": "slides/extra-slides.html#dag",
    "title": "How to ask a Why Question",
    "section": "DAG",
    "text": "DAG"
  },
  {
    "objectID": "slides/extra-slides.html#check-confounding",
    "href": "slides/extra-slides.html#check-confounding",
    "title": "How to ask a Why Question",
    "section": "Check confounding",
    "text": "Check confounding"
  },
  {
    "objectID": "slides/extra-slides.html#visualise-confounding",
    "href": "slides/extra-slides.html#visualise-confounding",
    "title": "How to ask a Why Question",
    "section": "Visualise confounding",
    "text": "Visualise confounding"
  },
  {
    "objectID": "slides/extra-slides.html#test-dag",
    "href": "slides/extra-slides.html#test-dag",
    "title": "How to ask a Why Question",
    "section": "test dag",
    "text": "test dag\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\",\"shape\"))\ndevtools::install_github(\"rmcelreath/rethinking\")\nlibrary(rethinking)\ndrawdag(d, arrow = \"gray\",  Z=list(\"U\"))\n\nhttps://gist.github.com/andrewheiss/4ece621813a27dfdcaef7f1c2d773237"
  },
  {
    "objectID": "slides/extra-slides.html#why-questions",
    "href": "slides/extra-slides.html#why-questions",
    "title": "How to ask a Why Question",
    "section": "Why questions",
    "text": "Why questions\nWhy why?"
  },
  {
    "objectID": "slides/index.html#math",
    "href": "slides/index.html#math",
    "title": "Preliminaries",
    "section": "Math",
    "text": "Math\n Y^0 - Y^1 \\neq 0 \n A \\to Y"
  },
  {
    "objectID": "slides/index.html#tikz",
    "href": "slides/index.html#tikz",
    "title": "Preliminaries",
    "section": "Tikz",
    "text": "Tikz"
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Preliminaries",
    "section": "",
    "text": "Class times and locations Lectures Day/Time: Tuesday, 1:10-3:00pm\nLecture Location: Easterfield Building EA407"
  },
  {
    "objectID": "slides/index.html#assessment-1-class-participation",
    "href": "slides/index.html#assessment-1-class-participation",
    "title": "Preliminaries",
    "section": "Assessment 1: Class Participation",
    "text": "Assessment 1: Class Participation\n\n\nyour willingness to ask/answer questions and generally contribute to class discussion; and\n\n\nthe quality of your contributions\n\nNOTE: if you’re sick, you shouldn’t come to class. Just let me know in advance. Your participation will not be affected."
  },
  {
    "objectID": "slides/index.html#assessment-2-take-home-research-concepts-workbook",
    "href": "slides/index.html#assessment-2-take-home-research-concepts-workbook",
    "title": "Preliminaries",
    "section": "Assessment 2: Take Home Research Concepts Workbook",
    "text": "Assessment 2: Take Home Research Concepts Workbook\n\nThis test will help you revise core statistical and methodological issues. It is a chance to brush up on basic statistical knowledge and will help you re-familiarize yourself with material that we will build on later in this course.\nThe key focus is the revision of basic terms, including correlation, regression, and on basic R applications.\nThe test contains a theoretical and a practical component. Therefore, it is essential that you complete these questions since you will need to understand the material for the more advanced techniques and approaches that we cover in this course.\nThere is no word limit (be reasonable)\nYou can use any source that you find useful.\nWarning: AI chatbots may be used but they make errors confidently.\nGenerally, it is better practice, to use peer-reviewed publications.\nAny resource you use must be cited. Failure to cite your source is a form of academic misconduct.\nYou may complete the test in your own study time. We ask that you work individually."
  },
  {
    "objectID": "slides/index.html#assessment-3",
    "href": "slides/index.html#assessment-3",
    "title": "Preliminaries",
    "section": "Assessment 3:",
    "text": "Assessment 3:"
  },
  {
    "objectID": "slides/01-slides.html#references",
    "href": "slides/01-slides.html#references",
    "title": "Asking questions in cross-cultural psychology",
    "section": "References",
    "text": "References\nFor an account of the history of measurement in psychological research, see: (Briggs 2021)\nFor an account of key concepts and current debates in psychometrics, see: (Bandalos 2018)\nFor an account of special measurement problems that arise in cross-cultural research, see: (Van de Vijver and Leung 2021)\nFor an accessible introduction to causal inference and its history see: (Pearl and Mackenzie 2018)\nBibliography\n\n\n\n\n\n\nBandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. Guilford Publications.\n\n\nBriggs, Derek C. 2021. Historical and Conceptual Foundations of Measurement in the Human Sciences: Credos and Controversies. Routledge.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nVan de Vijver, Fons JR, and Kwok Leung. 2021. Methods and Data Analysis for Cross-Cultural Research. Vol. 116. Cambridge University Press."
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Readings",
    "text": "Readings\nVan de Vijver and Leung (2021)\nBerry (1989)"
  },
  {
    "objectID": "content/01-content.html#recommended-readings",
    "href": "content/01-content.html#recommended-readings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n(He and Vijver 2012)\n(Van de Vijver and Leung 2021)\n(Berry 1989)"
  },
  {
    "objectID": "content/01-content.html#background-readings",
    "href": "content/01-content.html#background-readings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Background Readings",
    "text": "Background Readings\n(He and Vijver 2012)\n(Van de Vijver and Leung 2021)\n(Berry 1989)"
  },
  {
    "objectID": "slides/01-slides.html#goals-of-this-lecture",
    "href": "slides/01-slides.html#goals-of-this-lecture",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Goals of this lecture",
    "text": "Goals of this lecture\nBy the end of this lecture you will be able to:\n\nConsider ethical problems that cultural research might present.\nUnderstand the special problems that cultural research presents for measurement.\nUnderstand the concept of confounding.\n\nKey Concepts\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/01-slides.html#examples-of-psychological-questions-1",
    "href": "slides/01-slides.html#examples-of-psychological-questions-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Examples of psychological questions",
    "text": "Examples of psychological questions\n\n\nHow does early childhood experience affect personality and behaviour?\nWhat are the effects of social media on self-esteem?\nWhy do some people believe in a God or gods and others do not?\nWhy are some people motivated to sacrifice for others?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "href": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Example of psychological questions in cross-cultural psychology",
    "text": "Example of psychological questions in cross-cultural psychology\n\nHow do early childhood experiences differ across cultures, and how do these differences impact personality and behavior development?\nAre there cultural differences in the way social media use affects self-esteem and body image?\nHow do cultural and religious beliefs shape individual attitudes towards the concept of God or gods?\nWhat are the cultural and individual factors that motivate people to engage in acts of altruism or sacrifice for others, and how do these factors vary across cultures?\nAre there cultural differences in the factors that contribute to marital satisfaction and happiness, and how do cultural expectations and values surrounding marriage play a role?"
  },
  {
    "objectID": "slides/01-slides.html#where-does-psychology-start",
    "href": "slides/01-slides.html#where-does-psychology-start",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Where does psychology start?",
    "text": "Where does psychology start?\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/01-slides.html#how-might-we-answer-this-question",
    "href": "slides/01-slides.html#how-might-we-answer-this-question",
    "title": "Asking questions in cross-cultural psychology",
    "section": "How might we answer this question?",
    "text": "How might we answer this question?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#how-might-psychological-scientists-answer-this-question-wrong-answers-only",
    "href": "slides/01-slides.html#how-might-psychological-scientists-answer-this-question-wrong-answers-only",
    "title": "Asking questions in cross-cultural psychology",
    "section": "How might psychological scientists answer this question (Wrong answers only)?",
    "text": "How might psychological scientists answer this question (Wrong answers only)?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#how-might-psychological-scientists-answer-this-question",
    "href": "slides/01-slides.html#how-might-psychological-scientists-answer-this-question",
    "title": "Asking questions in cross-cultural psychology",
    "section": "How might psychological scientists answer this question?",
    "text": "How might psychological scientists answer this question?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#wrong-answers",
    "href": "slides/01-slides.html#wrong-answers",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Wrong answers:",
    "text": "Wrong answers:\n\n\n“Ask my married parents if they are happy.”\n“Consult a palm reader.”\nAssertion: “Yes marriage always makes people happy”; “No marriage can’t possibly make anyone happy”\nIntuition: “It depends on the gender of the individual. Men are always happier in marriage, while women are never happier.”\nBias: “It depends on the cultural background of the individuals. Couples from Western cultures are always happier in marriage, while couples from Eastern cultures are never happier.”\nConduct a literature review of previous research on the association between marriage and happiness, including cross-cultural studies that compare different cultural attitudes and practices regarding marriage and their relationship to happiness.\nConduct a survey of a large and diverse sample of individuals to assess their happiness levels, as well as their marital status assess the relationship."
  },
  {
    "objectID": "slides/01-slides.html#data-driven-responses",
    "href": "slides/01-slides.html#data-driven-responses",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Data driven responses",
    "text": "Data driven responses\nConduct a survey of a large and diverse sample of individuals\n:::{.incremental} - Where to begin?\n::"
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-literature-review-of-previous-research",
    "href": "slides/01-slides.html#conduct-a-literature-review-of-previous-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a literature review of previous research",
    "text": "Conduct a literature review of previous research\n\nWhat would this tell us?\n\nwhat other researchers have found.\n\nWhat would this not tell us?\n\nwhat other researchers have not found.\nwhat other researchers got wrong."
  },
  {
    "objectID": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "href": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "title": "Asking questions in cross-cultural psychology",
    "section": "How might psychological scientists answer whether marriage makes people happy?",
    "text": "How might psychological scientists answer whether marriage makes people happy?"
  },
  {
    "objectID": "slides/01-slides.html#measurement-of-happiness-measurement-of-marriage",
    "href": "slides/01-slides.html#measurement-of-happiness-measurement-of-marriage",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Measurement of Happiness & Measurement of Marriage",
    "text": "Measurement of Happiness & Measurement of Marriage\n“On a scale of 1-7, how happy are you?”\n:::{.incremental}\n\nHow might this go wrong?\nWhat might be problems that arise fin different cultures? ::"
  },
  {
    "objectID": "slides/01-slides.html#the-happiness-question",
    "href": "slides/01-slides.html#the-happiness-question",
    "title": "Asking questions in cross-cultural psychology",
    "section": "The Happiness Question:",
    "text": "The Happiness Question:"
  },
  {
    "objectID": "slides/01-slides.html#wrong-answers-only",
    "href": "slides/01-slides.html#wrong-answers-only",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Wrong answers only",
    "text": "Wrong answers only\n\n“Ask my married parents if they are happy.”\n“Consult a palm reader.”\nAssertion: “Yes marriage always makes people happy”; “No marriage can’t possibly make anyone happy”\nIntuition: “It depends on the gender of the individual. Men are always happier in marriage, while women are never happier.”\nIntuition: “It depends on the cultural background of the individuals. Couples from Western cultures are always happier in marriage, while couples from Eastern cultures are never happier.”\nConduct a literature review of previous research on the association between marriage and happiness, including cross-cultural studies that compare different cultural attitudes and practices regarding marriage and their relationship to happiness.\nConduct a survey of a large and diverse sample of individuals to assess their happiness levels, as well as their marital status assess the relationship."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "href": "slides/01-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a survey of a large and diverse sample of individuals",
    "text": "Conduct a survey of a large and diverse sample of individuals\n\nWhere to begin?"
  },
  {
    "objectID": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "href": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "title": "Asking questions in cross-cultural psychology",
    "section": "“On a scale of 1-7, how happy are you?”",
    "text": "“On a scale of 1-7, how happy are you?”\nHow might this go wrong?\n\nAmbiguity: Respondents may interpret “happiness” in different ways. -e.g. Some people may equate happiness with a momentary positive emotion -e.g. Others may think of happiness as a long-term state of contentment.\nSocial Desirability Bias: Respondents may want to tell you what they think you want to hear. Others might want to frustrate you.\nMood: the context in which the question is asked and the respondent’s current mood might affect the answer (reliability)\nLimited range of scale\nWhat do the endpoints mean?\nCultural Differences:\n\nin meanings\nin constructs themselves"
  },
  {
    "objectID": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "href": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Concept of validity in psychometric research",
    "text": "Concept of validity in psychometric research\n\nContent Validity: The degree an instrument measures what it is intended to measure (the “construct”).\nConstruct Validity: Whether the construct assumed to measure is accurately defined and operationalised.\nCriterion Validity: Whether an instrument accurately predicts performance\nFace Validity: whether an instrument measure what it is intended to measure, as assessed by experts.\nEcological Validity: whether an instrument accurately reflects real-world situations and behavior (Bandalos 2018)"
  },
  {
    "objectID": "slides/01-slides.html#are-there-cultural-differences-in-marital-satisfaction-and-happiness",
    "href": "slides/01-slides.html#are-there-cultural-differences-in-marital-satisfaction-and-happiness",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Are there cultural differences in marital satisfaction and happiness?",
    "text": "Are there cultural differences in marital satisfaction and happiness?"
  },
  {
    "objectID": "slides/01-slides.html#are-there-cultural-differences-in-marital-satisfaction-and-happiness-1",
    "href": "slides/01-slides.html#are-there-cultural-differences-in-marital-satisfaction-and-happiness-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Are there cultural differences in marital satisfaction and happiness?",
    "text": "Are there cultural differences in marital satisfaction and happiness?"
  },
  {
    "objectID": "slides/01-slides.html#measurement",
    "href": "slides/01-slides.html#measurement",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Measurement",
    "text": "Measurement\nDefinitions:\n\n“Measurement is the numerical quantifcation of the attributes of an object or event, which can be used to compare with other objects or events” [Conventional]\n“Measurement is the assignment of numerals to objects or events according to rules.” [Psychological]\n“Measurement is the process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity.” [Metrological] from (Briggs 2021)"
  },
  {
    "objectID": "slides/01-slides.html#what-can-we-do-about-it",
    "href": "slides/01-slides.html#what-can-we-do-about-it",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What can we do about it?",
    "text": "What can we do about it?\nPart 1: Measurement matters\n\nWeek 2: Getting started with R (Torven Schalk) tools for probing validity\nWeek 3: Lost in Translation? Measurement and Language (Inkuk Kim) – tools for addressing differences in meanings and concepts\nWeek 4: Dealing with Data Factor Analysis (Inkuk Kim) - tools needed for modelling relationship of items in a scale.\nWeek 5: Dealing with Data (Multi-Group CFA) (In Kuk Kim) - test the factor structures across groups.\nWeek 6: Dealing with Data (Partial Invariance) (Torven Schalk) – test (a) whether there are differences in the measurement properties of variables between groups, but the overall structure of the measurement model is similar (b) investigate whether there are differences in the factor structure of a measure between groups. Interest is in equivalence\n\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/01-slides.html#measurement-is-sorted.-what-else-might-go-wrong",
    "href": "slides/01-slides.html#measurement-is-sorted.-what-else-might-go-wrong",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Measurement is sorted. What else might go wrong?",
    "text": "Measurement is sorted. What else might go wrong?"
  },
  {
    "objectID": "slides/01-slides.html#else",
    "href": "slides/01-slides.html#else",
    "title": "Asking questions in cross-cultural psychology",
    "section": "ELSE",
    "text": "ELSE"
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong",
    "href": "slides/01-slides.html#what-else-might-go-wrong",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nSuppose we are confident in measurement, administer survey, and find an relationship.\n\nDoes marriage cause happiness cross culturally?"
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong-1",
    "href": "slides/01-slides.html#what-else-might-go-wrong-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?"
  },
  {
    "objectID": "slides/01-slides.html#section",
    "href": "slides/01-slides.html#section",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Marriage \\to  Happiness\nBut perhaps::\nHappiness \\to Marriage\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | Statistic |     df |      p\n-------------------------------------------------------------------------------\n(Intercept) |        0.38 | 0.07 | [ 0.25,  0.52] |      5.56 | 221.60 | < .001\nA           |       -0.27 | 0.07 | [-0.41, -0.14] |     -3.92 | 201.35 | < .001\nM           |        0.07 | 0.05 | [-0.02,  0.16] |      1.54 | 223.48 | 0.126 \n\n\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | Statistic |     df |      p\n-------------------------------------------------------------------------------\n(Intercept) |        0.41 | 0.06 | [ 0.28,  0.54] |      6.35 | 264.74 | < .001\nA           |       -0.21 | 0.04 | [-0.30, -0.12] |     -4.64 | 274.40 | < .001"
  },
  {
    "objectID": "slides/01-slides.html#we-might-infer-that-marriage-causes-happiness",
    "href": "slides/01-slides.html#we-might-infer-that-marriage-causes-happiness",
    "title": "Asking questions in cross-cultural psychology",
    "section": "We might infer that Marriage causes Happiness",
    "text": "We might infer that Marriage causes Happiness"
  },
  {
    "objectID": "slides/01-slides.html#can-we-rule-out-that-happiness-causes-marriage",
    "href": "slides/01-slides.html#can-we-rule-out-that-happiness-causes-marriage",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Can we rule out that happiness causes marriage?",
    "text": "Can we rule out that happiness causes marriage?\nM -> H\n\n\n\n\n\n\nH -> M\n\n\n\n\n\n\nM -> H"
  },
  {
    "objectID": "slides/01-slides.html#but-can-we-rule-out-that-happiness-causes-marriage",
    "href": "slides/01-slides.html#but-can-we-rule-out-that-happiness-causes-marriage",
    "title": "Asking questions in cross-cultural psychology",
    "section": "But can we rule out that Happiness causes marriage?",
    "text": "But can we rule out that Happiness causes marriage?"
  },
  {
    "objectID": "slides/01-slides.html#many-psychologists-will-simply-control-for-age",
    "href": "slides/01-slides.html#many-psychologists-will-simply-control-for-age",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Many psychologists will simply “control for age”",
    "text": "Many psychologists will simply “control for age”\n\nThis doesn’t work if there is reverse causation: Regression coefficient for Marriage (M) is biased.\n\n\n\nCode\n## Simulate data \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 1000\n  A &lt;- rnorm(n, 1) # simulates age,\n  H &lt;- rnorm(n , A) #  simulates happy as function of age\n  M &lt;- rnorm(n , A) + .2 * H # simulate marriage as a function of age + happiness\n\n  \n# Simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  A = A, \n  H = H,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + A, data = simdat_B)\nsim_B  # returns output\n}\n\n# Replication 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results\nparameters::pool_parameters(r_lm_B)\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | Statistic |     df |      p\n------------------------------------------------------------------------------\n(Intercept) |    4.12e-03 | 0.06 | [-0.12, 0.13] |      0.07 | 220.99 | 0.947 \nM           |        0.19 | 0.04 | [ 0.10, 0.27] |      4.42 | 234.19 | &lt; .001\nA           |        0.77 | 0.07 | [ 0.64, 0.91] |     11.17 | 211.13 | &lt; .001"
  },
  {
    "objectID": "slides/01-slides.html#happiness-might-cause-marriage",
    "href": "slides/01-slides.html#happiness-might-cause-marriage",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Happiness might cause marriage",
    "text": "Happiness might cause marriage"
  },
  {
    "objectID": "slides/01-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "href": "slides/01-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Age might be a confounder of both Marriage and Happiness",
    "text": "Age might be a confounder of both Marriage and Happiness"
  },
  {
    "objectID": "slides/01-slides.html#what-can-we-do-about-it-1",
    "href": "slides/01-slides.html#what-can-we-do-about-it-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What can we do about it?",
    "text": "What can we do about it?\nWeek 7: Causal inference: Causal Graphs(Joseph Bulbulia)\nWeek 8: Causal inference: Target Trial Emulation (Joseph (Joseph Bulbulia)\nWeek 9: Causal inference: Reconsidering measurement and selection biases (Joseph Bulbulia)\nWeek 10 Causal inference: Cross-Cultural Experiments (Joseph Bulbulia & Guests)"
  },
  {
    "objectID": "slides/01-slides.html#course-conclusions",
    "href": "slides/01-slides.html#course-conclusions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Course conclusions",
    "text": "Course conclusions\nWeek 11 Ethics and Ownership (Joseph Bulbulia & Guests). Can causal inference help us to improve ethical reasoning? Why is data protection important? How do the ethics of data protection relate to the ethics of open science?\nWeek 12 Future Horizons (Joseph Bulbulia, Torven, Inkuk):"
  },
  {
    "objectID": "slides/01-slides.html#ethics-workshop",
    "href": "slides/01-slides.html#ethics-workshop",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Ethics workshop",
    "text": "Ethics workshop\n\nWhat are some ethical considerations that researchers need to take into account when conducting cross-cultural research? How do these considerations differ from those in other types of research?\nHow can cultural differences in values and beliefs impact the ethical considerations in cross-cultural research? What are some examples of cultural differences that researchers need to be aware of?\nHow can researchers ensure that their cross-cultural research is ethical, while still being scientifically rigorous and producing valuable insights? What are some best practices or guidelines that researchers can follow?"
  },
  {
    "objectID": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "href": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "title": "Asking questions in cross-cultural psychology",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\nWeek 7: Causal inference: Causal Graphs(Joseph Bulbulia). How to diagnosis sources of confounding and what to do about it.\nWeek 8: Causal inference: Target Trial Emulation (Joseph Bulbulia). How to emulate an experiment with observational data and how to identify the modification of effects by cultural groups.\nWeek 9: Causal inference: Reconsidering measurement and selection biases (Joseph Bulbulia). A discussion of problems in standard approaches to measurement.\nWeek 10 Causal inference: Cross-Cultural Experiments (Joseph Bulbulia & Guests). For whom to our results generalise?"
  },
  {
    "objectID": "slides/01-slides.html#ethics-brainstorm",
    "href": "slides/01-slides.html#ethics-brainstorm",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Ethics brainstorm",
    "text": "Ethics brainstorm\n\nWhat are some ethical considerations that researchers need to take into account when conducting cross-cultural research? How do these considerations differ from those in other types of research?\nHow can cultural differences in values and beliefs impact the ethical considerations in cross-cultural research? What are some examples of cultural differences that researchers need to be aware of?\nHow can researchers ensure that their cross-cultural research is ethical, while still being scientifically rigorous and producing valuable insights? What are some best practices or guidelines that researchers can follow?"
  },
  {
    "objectID": "slides/01-slides.html#group-discussion-ethics-brainstorm",
    "href": "slides/01-slides.html#group-discussion-ethics-brainstorm",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Group discussion: Ethics brainstorm",
    "text": "Group discussion: Ethics brainstorm\n\nWhat are some ethical considerations that researchers need to take into account when conducting cross-cultural research? How do these considerations differ from those in other types of research?\nHow can cultural differences in values and beliefs impact the ethical considerations in cross-cultural research? What are some examples of cultural differences that researchers need to be aware of?\nHow can researchers ensure that their cross-cultural research is ethical, while still being scientifically rigorous and producing valuable insights? What are some best practices or guidelines that researchers can follow?"
  },
  {
    "objectID": "slides/index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "href": "slides/index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "title": "Preliminaries",
    "section": "Assessment 3:Theoretical Application (5,000 word maximum including references)",
    "text": "Assessment 3:Theoretical Application (5,000 word maximum including references)\nChoose one particular psychological concept or variable of interest to you. Provide a brief introduction to the concept or variable. Provide a sketch of the methodology for your study (what analysis will you run: equivalence tests, regressions, etc.) following the provided pre-registration template. Identify and justify your research participants’ cultural background. This assessment is equivalent to the introduction and methods part of an empirical paper.\nThe word limit for this assignment is 5000 words max (including references), and it must be typed and double spaced with a 12point Times New Roman font (or similar)."
  },
  {
    "objectID": "slides/index.html#pre-registration-review-1000-words-maximum-including-references",
    "href": "slides/index.html#pre-registration-review-1000-words-maximum-including-references",
    "title": "Preliminaries",
    "section": "Pre-Registration Review (1000 words maximum, including references)",
    "text": "Pre-Registration Review (1000 words maximum, including references)\nYou will be invited to provide a review of one submitted pre-registration from another student. Using what you have learned about cross-cultural research provide feedback on both theory and suggested methods. Make sure to provide concrete and respectful feedback.\nStatistical Application (4,000 word maximum excluding tables and any references)\nOption 1 Measurement (see outline)\nOption 2 Causality (see outline)"
  },
  {
    "objectID": "slides/index.html#letter-to-the-reviewer",
    "href": "slides/index.html#letter-to-the-reviewer",
    "title": "Preliminaries",
    "section": "Letter to the Reviewer",
    "text": "Letter to the Reviewer\nUsing the reviewer letter you received, respond to the points raised by the reviewer. Where did you alter your methods and pre-registration, where do results support your reviewer, where do they not support their claims.\nSubmission and return of work"
  },
  {
    "objectID": "slides/index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "href": "slides/index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "title": "Preliminaries",
    "section": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days.",
    "text": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days."
  },
  {
    "objectID": "slides/index.html#extensions",
    "href": "slides/index.html#extensions",
    "title": "Preliminaries",
    "section": "Extensions",
    "text": "Extensions\nIf any due date does not suit you, you may negotiate a new due date, in writing, that suits you. Any request for revision must be submitted in writing (by email) to your instructor before class on March 14th, 2023"
  },
  {
    "objectID": "slides/index.html#penalties",
    "href": "slides/index.html#penalties",
    "title": "Preliminaries",
    "section": "Penalties",
    "text": "Penalties\nThe submission of late assignments is strongly discouraged. A penalty of one grade per day (e.g., B down to B-) from the hand-in date will be deducted from the final grade for any late work."
  },
  {
    "objectID": "slides/index.html#unforeseeable-events",
    "href": "slides/index.html#unforeseeable-events",
    "title": "Preliminaries",
    "section": "Unforeseeable events",
    "text": "Unforeseeable events\nIn general, we require evidence (medical certificate etc.) to grant an extension."
  },
  {
    "objectID": "slides/index.html#word-limits",
    "href": "slides/index.html#word-limits",
    "title": "Preliminaries",
    "section": "Word limits",
    "text": "Word limits\nAssignments that are over the word limit (see above) will be penalized. All submitted assessments are expected to be your own. You should neither give nor receive any aid on the assessments. Giving or receiving aid on assessments will be considered academic misconduct, resulting in registration in the Academic Misconduct Register (AMR). You may use any source, but the source must be cited. Penalties appropriate for the level of academic misconduct may be applied. Failure to cite your source will be considered academic misconduct and will result in registration in the Academic Misconduct Register (AMR)."
  },
  {
    "objectID": "slides/index.html#materials-and-equipment",
    "href": "slides/index.html#materials-and-equipment",
    "title": "Preliminaries",
    "section": "Materials and equipment",
    "text": "Materials and equipment\nYou should bring paper and a writing utensil or laptop/tablet to seminars. For the data analysis sessions, having your personal laptop with the relevant computer programs (R and RStudio) installed is essential. Most sessions will require data analysis.\n\nNote: If you do not have a laptop, let me know. We will find a solution."
  },
  {
    "objectID": "slides/index.html#course-learning-objectives",
    "href": "slides/index.html#course-learning-objectives",
    "title": "Preliminaries",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nEthical Reasoning. Beginning in week 1, we will explore questions that are typically reserved for philosophy courses but are central to the conduct of psychology: why do we need to think about right and wrong in science? What if different people have different ideas about what is right and wrong in science? Is there hope for ethical progress? Can science play a role in ethical progress? We will provide you with a set of strategies for addressing these questions.\nProgramming in R. Beginning in week 2, we will teach you the basics of programming in the statistical language R. The past several decades have brought extraordinary new tools to psychological scientists, many of which require literacy in computer programming. While some of you may find the thought of programming thrilling, others may find it terrifying or boring! We promise that programming can be fun. This course is designed to help you find that joy.\nUnderstanding measurement. Beginning in week 3 the course will turn its focus to developing skills for constructing and validating measures in cross-cultural research. Again these skills will be invaluable for a wide range of tasks you may face in psychological science and will help you to address problems that will arise in other areas of your research and work. Measurement will occupy our attention through week 6.\nUnderstanding causal inference. Beginning in week 7 and for the following four weeks, the course will impart skills that you require to disentangle causation from correlation. Again our focus will be on the special problems that arise for cultural datasets. Notably, in psychological science, causal inference remains underdeveloped, and the material in this part of the course will position you to make potentially important contributions, whether or not your interests lie in cross-cultural psychology."
  },
  {
    "objectID": "index.html#assignments-and-due-dates",
    "href": "index.html#assignments-and-due-dates",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assignments and due dates",
    "text": "Assignments and due dates\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n*\n10\nWeekly\n\n\nTake-home\n3,4\n10\nMarch 19 (w3)\n\n\nTheoretical application\n1,2,3\n30\n21 May (w9)\n\n\nPre-reg review\n1,2,3,4\n10\n28 May (w11)\n\n\nStatistical application\n1,2,3\n30\n11 June (w13)\n\n\nLetter to the reviewer\n1,2,3\n10\n11 June (w13)"
  },
  {
    "objectID": "index.html#assessment-1-class-participation",
    "href": "index.html#assessment-1-class-participation",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 1: Class Participation",
    "text": "Assessment 1: Class Participation\n\n\nyour willingness to ask/answer questions and generally contribute to class discussion; and\n\n\nthe quality of your contributions"
  },
  {
    "objectID": "index.html#assessment-2-take-home-research-concepts-workbook",
    "href": "index.html#assessment-2-take-home-research-concepts-workbook",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 2: Take Home Research Concepts Workbook",
    "text": "Assessment 2: Take Home Research Concepts Workbook\n\nThis test will help you revise core statistical and methodological issues. It is a chance to brush up on basic statistical knowledge and will help you re-familiarise yourself with material that we will build on later in this course.\nThe key focus is the revision of basic terms, including correlation, regression, and on basic R applications.\nThe test contains a theoretical and a practical component. Therefore, it is essential that you complete these questions since you will need to understand the material for the more advanced techniques and approaches that we cover in this course.\nThere is no word limit (be reasonable)\nYou can use any source that you find useful.\nWarning: AI chatbots may be used but they make errors confidently.\nGenerally, it is better practice, to use peer-reviewed publications.\nAny resource you use must be cited. Failure to cite your source is a form of academic misconduct.\nYou may complete the test in your own study time. We ask that you work individually."
  },
  {
    "objectID": "index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "href": "index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 3:Theoretical Application (5,000 word maximum including references)",
    "text": "Assessment 3:Theoretical Application (5,000 word maximum including references)\nChoose one particular psychological concept or variable of interest to you. Provide a brief introduction to the concept or variable. Provide a sketch of the methodology for your study (what analysis will you run: equivalence tests, regressions, etc.) following the provided pre-registration template. Identify and justify your research participants’ cultural background. This assessment is equivalent to the introduction and methods part of an empirical paper.\nThe word limit for this assignment is 5000 words max (including references), and it must be typed and double spaced with a 12point Times New Roman font (or similar)."
  },
  {
    "objectID": "index.html#pre-registration-review-1000-words-maximum-including-references",
    "href": "index.html#pre-registration-review-1000-words-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Pre-Registration Review (1000 words maximum, including references)",
    "text": "Pre-Registration Review (1000 words maximum, including references)\nYou will be invited to provide a review of one submitted pre-registration from another student. Using what you have learned about cross-cultural research provide feedback on both theory and suggested methods. Make sure to provide concrete and respectful feedback."
  },
  {
    "objectID": "index.html#letter-to-the-reviewer",
    "href": "index.html#letter-to-the-reviewer",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Letter to the Reviewer",
    "text": "Letter to the Reviewer\nUsing the reviewer letter you received, respond to the points raised by the reviewer. Where did you alter your methods and pre-registration, where do results support your reviewer, where do they not support their claims.\n\nSTATISTICAL ASSESSMENT (max 4,000 words)\n\nOption 1: Title: Cross-Cultural Psychology: Examining Measurement Invariance and Cultural Bias in Psychological Constructs\n\n\nObjective:\n\nConduct a statistical analysis to examine configural, metric, and scalar equivalence of your pre-registered study data, focusing on the technical aspects of measurement invariance testing, analysis, reporting, and discussion of findings in the context of cross-cultural psychology.\n\nInstructions: 1. Construct description and sample information: - Provide a minimal description of the construct(s) and the measure used in your study. - Name the samples/countries included in your analysis. - State the purpose of the analysis and the theoretical question being addressed.\n\nAnalysis procedure:\n\nDescribe the procedure you used to examine configural, metric, and scalar equivalence, including a brief description of each step.\nJustify any decisions you made during the process, explaining their rationale in the context of measurement invariance testing.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nResults reporting:\n\nReport your findings in a clear and concise manner, similar to the results section of an empirical paper.\nPresent factor loadings (optional) and other relevant indices in an appropriate table, following APA style.\n\nDiscussion:\n\nDiscuss your findings, focusing on levels of equivalence, biases identified (if any), biases ruled out, and potential remaining biases.\nProvide a concise discussion on the theoretical implications of your analysis and results, connecting the findings to theories about culture and cultural differences.\n\n\nRequirements: - Maximum length: 4,000 words (excluding tables, references, and code; provide word count on a separate title page) - Use APA style for citations, references, tables, and figures - Use 12-point Times New Roman font or similar - Submit your assignment as a single PDF file\nEvaluation Criteria: - Clarity and coherence of the construct description, sample information, and theoretical question - Appropriateness and rigor of the analysis procedure, including justification for decisions made - Correctness and replicability of the submitted code - Accurate and concise reporting of results, following APA style - Comprehensive and focused discussion of findings, including levels of equivalence, biases, and theoretical implications in the context of cross-cultural psychology - Overall organization, presentation, and adherence to formatting guidelines\n\n\nOPTION 2:\nTitle: Causal Inference in Psychology: Developing the Results and Discussion Sections for a Study on the Effect of Exposure on Dimensions of Well-being Modified by Key Sociodemographic Categories\nObjective: - Based on your previously developed study using the NZAVS longitudinal synthetic data set, create the Results and Discussion sections of a manuscript, detailing the findings of your causal inference analysis and their implications in the context of psychology.\nInstructions: 1. Data analysis and results presentation: - Conduct the data analysis as planned in your previous assignment, using appropriate statistical models and confounding control strategies. - Present the results in a clear and concise manner, similar to the results section of an empirical paper. - Include tables and/or figures as needed to support your findings, following APA style.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nDiscussion of findings:\n\nDiscuss the findings in the context of the exposure, outcome(s), and the chosen sociodemographic category.\nAddress the potential limitations of your study, including the robustness of assumptions and potential biases.\nConnect the findings to relevant theories in psychology, discussing their implications for the field.\n\nConclusion and future directions:\n\nSummarize the main findings of your study and their significance in the context of psychology and causal inference.\nSuggest potential future research directions based on your findings and their theoretical implications.\n\n\nRequirements: - Maximum length: 5,000 words (excluding tables, references, and code; provide word count on a separate title page) - Use APA style for citations, references, tables, and figures - Use 12-point Times New Roman font or similar - Submit your assignment as a single PDF file\nEvaluation Criteria: - Appropriateness and rigor of the data analysis, including the use of statistical models and confounding control strategies - Clear and concise presentation of results, following APA style - Correctness and replicability of the submitted code - Comprehensive and focused discussion of findings, including potential limitations, assumptions, biases, and theoretical implications in the context of psychology - Well-structured conclusion and suggestions for future research directions - Overall organization, presentation, and adherence to formatting guidelines"
  },
  {
    "objectID": "index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "href": "index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days.",
    "text": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days."
  },
  {
    "objectID": "index.html#extensions",
    "href": "index.html#extensions",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Extensions",
    "text": "Extensions\nIf any due date does not suit you, you may negotiate a new due date, in writing, that suits you. Any request for revision must be submitted in writing (by email) to your instructor before class on March 14th, 2023"
  },
  {
    "objectID": "index.html#penalties",
    "href": "index.html#penalties",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Penalties",
    "text": "Penalties\nThe submission of late assignments is strongly discouraged. A penalty of one grade per day (e.g., B down to B-) from the hand-in date will be deducted from the final grade for any late work."
  },
  {
    "objectID": "index.html#unforeseeable-events",
    "href": "index.html#unforeseeable-events",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Unforeseeable events",
    "text": "Unforeseeable events\nIn general, we require evidence (medical certificate etc.) to grant an extension."
  },
  {
    "objectID": "index.html#word-limits",
    "href": "index.html#word-limits",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Word limits",
    "text": "Word limits\nAssignments that are over the word limit (see above) will be penalised. All submitted assessments are expected to be your own. You should neither give nor receive any aid on the assessments. Giving or receiving aid on assessments will be considered academic misconduct, resulting in registration in the Academic Misconduct Register (AMR). You may use any source, but the source must be cited. Penalties appropriate for the level of academic misconduct may be applied. Failure to cite your source will be considered academic misconduct and will result in registration in the Academic Misconduct Register (AMR)."
  },
  {
    "objectID": "index.html#materials-and-equipment",
    "href": "index.html#materials-and-equipment",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Materials and equipment",
    "text": "Materials and equipment\nYou should bring paper and a writing utensil or laptop/tablet to seminars. For the data analysis sessions, having your personal laptop with the relevant computer programs (R and RStudio) installed is essential. Most sessions will require data analysis."
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Notes",
    "text": "Notes\nExample of a causal graph."
  },
  {
    "objectID": "slides/01-slides.html#what-might-go-wrong",
    "href": "slides/01-slides.html#what-might-go-wrong",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What might go wrong?",
    "text": "What might go wrong?"
  },
  {
    "objectID": "slides/01-slides.html#what-have-we-learned",
    "href": "slides/01-slides.html#what-have-we-learned",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What have we learned?",
    "text": "What have we learned?\nKey Concepts ::: {.fragment .highlight-red} 1. Measurement = quantification 2. Validity = accuracy 3. Confounder = interference :::"
  },
  {
    "objectID": "slides/01-slides.html#the-end",
    "href": "slides/01-slides.html#the-end",
    "title": "Asking questions in cross-cultural psychology",
    "section": "The end!",
    "text": "The end!\n\n\n\n\n\n\n\n\n\n\nBandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. Guilford Publications.\n\n\nBriggs, Derek C. 2021. Historical and Conceptual Foundations of Measurement in the Human Sciences: Credos and Controversies. Routledge.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nVan de Vijver, Fons JR, and Kwok Leung. 2021. Methods and Data Analysis for Cross-Cultural Research. Vol. 116. Cambridge University Press."
  },
  {
    "objectID": "slides/01-slides.html#bibliography",
    "href": "slides/01-slides.html#bibliography",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "slides/01-slides.html#goals",
    "href": "slides/01-slides.html#goals",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Goals",
    "text": "Goals\nBy the end of this seminaryou will be able to:\n\nConsider ethical problems that cultural research might present.\nUnderstand the special problems that cultural research presents for measurement.\nUnderstand the concept of confounding.\n\nKey Concepts\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/index.html#thanks",
    "href": "slides/index.html#thanks",
    "title": "Preliminaries",
    "section": "Thanks",
    "text": "Thanks\n["
  },
  {
    "objectID": "slides/index.html#thanks-to-johannes-karl-and-ron-fischer",
    "href": "slides/index.html#thanks-to-johannes-karl-and-ron-fischer",
    "title": "Preliminaries",
    "section": "Thanks to Johannes Karl and Ron Fischer",
    "text": "Thanks to Johannes Karl and Ron Fischer\nFor developing measurement components of the course\n\n\n\n\n\n\nJohannes Karl\n\n\n\n\n\n\n\n\n\n\n\nRonald Fischer"
  },
  {
    "objectID": "slides/index.html#help",
    "href": "slides/index.html#help",
    "title": "Preliminaries",
    "section": "Help",
    "text": "Help\nDr.Inkuk Kim inkuk.kim@vuw.ac.nz\n\n\n\n\n\n\nDr. In Kuk Kim\n\n\n\n\n\nTorven Schalk torven.schalk@vuw.ac.nz\n\n\n\n\n\n\nTorven Schalk\n\n\n\n\n\nJoseph Bulbulia joseph.bulbulia@vuw.ac.nz\n\n\n\n\n\n\nJoseph Bulbulia"
  },
  {
    "objectID": "slides/01-slides.html#in-survey-research",
    "href": "slides/01-slides.html#in-survey-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "In survey research",
    "text": "In survey research\nConstruct: e.g. Happiness - a subjective experience of positive emotions, such as joy, contentment, and satisfaction with life/\nItem: A question or statement used to measure an aspect of happiness. Example: “I feel content with my life.”\nScale: A collection of items designed to measure a construct. Example: a happiness scale might consist of items rated on a Likert-type scale, with an overall score reflecting the level of happiness being measured. Example:\n\n“In general, how happy do you feel?” (rated on a scale from 1 - not at all happy to 5 - extremely happy)\n“How often do you feel positive emotions, such as joy or contentment?” (rated on a scale from 1 - very rarely or never to 5 - very often or always)\n“How satisfied are you with your life as a whole?” (rated on a scale from 1 - very dissatisfied to 5 - very satisfied)"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Getting started with R",
    "section": "",
    "text": "This session is devoted to learning R. There are no required readings."
  },
  {
    "objectID": "content/02-content.html#optional-extra-material",
    "href": "content/02-content.html#optional-extra-material",
    "title": "Getting started with R",
    "section": "OPTIONAL extra material",
    "text": "OPTIONAL extra material\nProofread version:\n\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/ For now, only lecture 1 would be relevant. It’s worth noting that this lecture covers working with GitHub. While GitHub is a useful tool, it’s not a requirement for the current course."
  },
  {
    "objectID": "assignments/Explanation.html",
    "href": "assignments/Explanation.html",
    "title": "Assignment 1 review",
    "section": "",
    "text": "Understand why we are asking you about correlation, covariance, and regression.\n\n\n\n\n\ncorrelation\ncovariance\nregression\n\n\n\n\n\n\nWhat is a correlation? b) When do you use a correlation (what type of theoretical problem)? c) What is the theoretical range of a correlation coefficient? d) How can you interpret a correlation?"
  },
  {
    "objectID": "assignments/Explanation.html#references",
    "href": "assignments/Explanation.html#references",
    "title": "Assignment 1 review",
    "section": "References",
    "text": "References\nFor an account of the history of measurement in psychological research, see: [@briggs2021]\nFor an account of key concepts and current debates in psychometrics, see: [@bandalos2018]\nFor an account of special measurement problems that arise in cross-cultural research, see: [@vandevijver2021]\nFor an accessible introduction to causal inference and its history see: [@pearl2018]\n\nBibliography"
  },
  {
    "objectID": "content/07-content.html#what-is-a-cause-and-what-is-an-effect",
    "href": "content/07-content.html#what-is-a-cause-and-what-is-an-effect",
    "title": "Causal Inference: Causal Graphs",
    "section": "What is a Cause and What is an Effect?",
    "text": "What is a Cause and What is an Effect?"
  },
  {
    "objectID": "content/07-content.html#why-is-causal-inference-a-missing-data-problem",
    "href": "content/07-content.html#why-is-causal-inference-a-missing-data-problem",
    "title": "Causal Inference: Causal Graphs",
    "section": "Why is Causal Inference a “missing data” problem?",
    "text": "Why is Causal Inference a “missing data” problem?\n\nConfounding and Confounders\nCausal Graphs to identify sources of confounding"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "",
    "text": "Figure 1: Causal graph shows potential for selection bias from loss to follow up or non-response. To address this, we multiply impute missing values on the assumption that missing values are random conditional on the imputation model (MAR)."
  },
  {
    "objectID": "content/07-content.html#overview",
    "href": "content/07-content.html#overview",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "content/02-content.html#overview",
    "href": "content/02-content.html#overview",
    "title": "Getting started with R",
    "section": "",
    "text": "This session is devoted to learning R. There are no required readings."
  },
  {
    "objectID": "data/readme.html",
    "href": "data/readme.html",
    "title": "Getting started with R",
    "section": "",
    "text": "This is a synthetic data file. The data are simulated from the a sub sample of the New Zealand Attitudes and Values Study (NZAVS).\nThe code to create the synthetic data set, before noise was added to the variables is given below."
  },
  {
    "objectID": "data/readme.html#read-me",
    "href": "data/readme.html#read-me",
    "title": "Getting started with R",
    "section": "",
    "text": "This is a synthetic data file. The data are simulated from the a sub sample of the New Zealand Attitudes and Values Study (NZAVS).\nThe code to create the synthetic data set, before noise was added to the variables is given below."
  },
  {
    "objectID": "data/readme.html#where-can-i-go-for-further-information",
    "href": "data/readme.html#where-can-i-go-for-further-information",
    "title": "Getting started with R",
    "section": "Where can I go for further information?",
    "text": "Where can I go for further information?\nThe NZAVS website is maintained by Prof Chris Sibley. Here is a Link"
  },
  {
    "objectID": "data/readme.html#link-to-data-dictionary-and-questions",
    "href": "data/readme.html#link-to-data-dictionary-and-questions",
    "title": "Getting started with R",
    "section": "Link to data dictionary and questions",
    "text": "Link to data dictionary and questions\nLink to full data dictionary\nLink to questions only\n\ndat_long &lt;- dat |&gt;\n  arrange(id, wave) |&gt;\n  # select variables\n  rename(religion_religious = religious) |&gt;\n  select(\n    \"wave\",\n    \"year_measured\",\n    \"sample_frame\",# see NZAVS\n    \"id\",\n    \"edu\",\n    # Ordinal-Rank 0-10 NZREG codes (with overseas school quals coded as Level 3, and all other ancillary categories coded as missing) See:https://www.nzqa.govt.nz/assets/Studying-in-NZ/New-Zealand-Qualification-Framework/requirements-nzqf.pdf\n    \"male\",\n    # 0 = female, 0.5 = neither female nor male, 1 = male.\n    \"born_nz\",\n    # value label 0    No 1   Yes\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n    \"gen_cohort\",\n    #What is your gender? (open-ended)\n    \"household_inc\",\n    # Please estimate your total household income (before tax) for the last year.\n    \"nz_dep2018\",\n    # see nzavs materials\n    \"nzsei13\",\n    # see nzavs materials\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    # 0 = no, 1 = yes\n    \"pol_orient\",\n    #Please rate how politically liberal versus conservative you see yourself as being.\n    \"pol_wing\",\n    # Please rate how politically left-wing versus right-wing you see yourself as being.\n    \"rural_gch2018\",\n    # see NZAVS\n    \"agreeableness\",\n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # see mini ipip6\n    # I want people to know that I am an important person of high status,\n    # I am an ordinary person who is no better than others.\n    # I wouldn’t want people to treat me as though I were superior to them.\n    # I think that I am entitled to more respect than the average person is.\n    \"religion_religious\",\n    # Do you identify with a religion and/or spiritual group?\n    \"religion_identification_level\",\n    #How important is your religion to how you see yourself?\"\n    # \"religion_church_binary\", # at least 1 time per month = 1, 0.\n    #  \"religion_prayer\", # How many times did you pray in the last week?\n    #  \"religion_scripture\", # How many times did you read religious scripture in the last week?\n    #  \"religion_church2\", # How many times did you attend a church or place of worship in the last month?\n    \"religion_believe_spirit\",\n    #Do you believe in some form of spirit or lifeforce?\n    \"religion_believe_god\",\n    #Do you believe in a God\n    # \"religion_spiritual_identification\", #w8,w10,w12-13 \"I identify as a spiritual person.\"\n    \"religion_perceive_religious_discrim\",\n    #   I feel that I am often discriminated against because of my religious/spiritual beliefs.\n    \"bigger_doms\",\n    #What religion or spiritual group?#  Not_Rel, Anglican , Buddist, Catholic , Christian_nfd, Christian_Others, Hindu, Jewish           Muslim, PresbyCongReform, TheOthers\n    \"w_gend_age_euro\",\n    # sample_weights\n    \"alcohol_frequency\",\n    #\"How often do you have a drink containing alcohol?\"\n    \"alcohol_intensity\",\n    # How many drinks containing alcohol do you have on a typical day when drinking?\n    \"hlth_bmi\",\n    # \" What is your height? (metres)\\nWhat is your weight? (kg)\\nKg\n    \"hours_exercise\",\n    # Hours spent … exercising/physical activity\n    # \"sfhealth\",\n    \"sfhealth_your_health\",\n    # \"In general, would you say your health is...\n    \"sfhealth_get_sick_easier\",\n    #\\nI seem to get sick a little easier than other people.\n    \"sfhealth_expect_worse_health\",\n    #\\nI expect my health to get worse.\" ****\n    \"hlth_sleep_hours\",\n    #During the past month, on average, how many hours of actual sleep did you get per night?\n    \"smoker\",\n    #Do you currently smoke?\n    \"hlth_fatigue\",\n    #During the last 30 days, how often did.... you feel exhausted?\n    \"rumination\",\n    # During the last 30 days, how often did.... you have negative thoughts that repeated over and over?\n    \"kessler_depressed\",\n    #During the last 30 days, how often did.... you feel so depressed that nothing could cheer you up?\n    \"kessler_effort\",\n    #During the last 30 days, how often did.... you feel that everything was an effort?\n    \"kessler_hopeless\",\n    # During the last 30 days, how often did.... you feel hopeless?\n    \"kessler_nervous\",\n    #During the last 30 days, how often did.... you feel nervous?\n    \"kessler_restless\",\n    #During the last 30 days, how often did.... you feel restless or fidgety?\n    \"kessler_worthless\",\n    # During the last 30 days, how often did.... you feel worthless?\n    \"sexual_satisfaction\",\n    #  How satisfied are you with your sex life?\n    \"bodysat\",\n    ## Am satisfied with the appearance, size and shape of my body.\n    \"vengeful_rumin\",\n    # Sometimes I can't sleep because of thinking about past wrongs I have suffered.//# I can usually forgive and forget when someone does me wrong.# I find myself regularly thinking about past times that I have been wronged.\n    \"perfectionism\",\n    # # Doing my best never seems to be enough./# My performance rarely measures up to my standards.\n    # I am hardly ever satisfied with my performance.\n    \"power_self_nocontrol\",\n    # I do not have enough power or control over\\nimportant parts of my life.\n    \"power_others_control\",\n    # Other people have too much power or control over\\nimportant parts of my life\n    \"selfesteem_satself\",\n    #  On the whole am satisfied with myself.\n    \"selfesteem_postiveself\",\n    # Take a positive attitude toward myself\n    \"selfesteem_rfailure\",\n    # Am inclined to feel that I am a failure. (r)\n    \"self_control_have_lots\",\n    #In general, I have a lot of self-control.\n    \"self_control_wish_more_r\",\n    #I wish I had more self-discipline.(r)\n    \"emotion_regulation_out_control\",\n    # When I feel negative emotions, my emotions feel out of control. w10 - w13\n    \"emotion_regulation_hide_neg_emotions\",\n    # When I feel negative emotions, I suppress or hide my emotions. w10 - w13\n    \"emotion_regulation_change_thinking_to_calm\",\n    # When I feel negative emotions, I change the way I think to help me stay calm. w10 - w13\n    #  \"emp_work_life_balance\",# I have a good balance between work and other important things in my life.\n    \"gratitude\",\n    ## I have much in my life to be thankful for. # When I look at the world, I don’t see much to be grateful for. # I am grateful to a wide variety of people.\n    \"pwi_health\",\n    #Your health.\n    \"pwi_relationships\",\n    #Your personal relationships.\n    \"pwi_security\",\n    #Your future security.\n    \"pwi_standardliving\",\n    #Your standard of living.\n    \"lifesat_satlife\",\n    # I am satisfied with my life.\n    \"lifesat_ideal\",\n    # In most ways my life is close to ideal.\n    \"meaning_purpose\",\n    # My life has a clear sense of purpose.\n    \"meaning_sense\",\n    # I have a good sense of what makes my life meaningful.\n    \"permeability_individual\",\n    #I believe I am capable, as an individual\\nof improving my status in society.\n    \"impermeability_group\",\n    #The current income gap between New Zealand Europeans and other ethnic groups would be very hard to change.\n    \"neighbourhood_community\",\n    #I feel a sense of community with others in my local neighbourhood.\n    \"support_help\",\n    # 'There are people I can depend on to help me if I really need it.\n    \"support_turnto\",\n    # There is no one I can turn to for guidance in times of stress.\n    \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"belong_accept\",\n    #Know that people in my life accept and value me.\n    \"belong_routsider\",\n    # Feel like an outsider.\n    \"belong_beliefs\",\n    # Know that people around me share my attitudes and beliefs.\n    \"charity_donate\",\n    #How much money have you donated to charity in the last year?\n    \"hours_charity\",\n    #Hours spent in activities/Hours spent … voluntary/charitable work\n    \"nwi\",\n    # The economic situation in New Zealand./# The social conditions in New Zealand. # Business in New Zealand.\n  ) |&gt;\n  dplyr::rename(sample_weights = w_gend_age_euro) |&gt;\n  dplyr::filter((wave == 2018 & year_measured  == 1) |\n                  (wave == 2019  &\n                     year_measured  == 1) |\n                  (wave == 2020  &  year_measured  == 1)\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(id) |&gt;\n  filter(n_distinct(wave) == 3) |&gt; # not used to avoid missing data -- for teaching purposes only\n  # dplyr::mutate(k_18 =  ifelse(wave == 2018, 1, 0)) |&gt;   # creating an indicator for the first wave\n  # dplyr::mutate(h_18 = mean(k_18, na.rm = TRUE)) |&gt;   # hack\n  # dplyr::mutate(k_19 =  ifelse(wave == 2019 &\n  #                                year_measured == 1, 1, 0)) |&gt;   # creating an indicator for the first wave; note that we allow people t\n  # dplyr::mutate(h_19 = mean(k_19, na.rm = TRUE)) |&gt;  # Hack\n  # dplyr::filter(h_18 &gt; 0) |&gt;  # hack to enable repeat of baseline\n  # dplyr::filter(h_19 &gt; 0) |&gt;  # hack to enable repeat of baseline\n  ungroup() |&gt;\n  droplevels() |&gt;\n  drop_na() |&gt;\n  arrange(id, wave)"
  },
  {
    "objectID": "content/07-content.html#downloads",
    "href": "content/07-content.html#downloads",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Downloads",
    "text": "Downloads\nBelow is a link to the R script that will allow you to download the data and exercises. Copy the contents on your screen to a new R script, and run the script from the begging. Before class, it will be useful for you to:\n\nRun “source()” file.\nDownload the synthetic data.\nCreate a folder in your Rstudio project called “data”\nPlace the downloaded file in your data folder.\n\nlink to script for this week"
  },
  {
    "objectID": "index.html#assessment-theoretical-application-5000-word-maximum-including-references",
    "href": "index.html#assessment-theoretical-application-5000-word-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment: THEORETICAL APPLICATION (5,000 word maximum including references)",
    "text": "Assessment: THEORETICAL APPLICATION (5,000 word maximum including references)\n\nOption 1: Theoretical Application: Cross-Cultural Psychology: Investigating a Psychological Concept or Variable of Interest in Diverse Cultural Contexts (5,000 word maximum including references)\n\nObjective:\n\nConduct a study focusing on cross-cultural measurement of a specific psychological concept or variable of interest, with attention to methodological rigor, cultural diversity, and the cultural background of research participants.\n\n\n\nInstructions:\n\nIntroduction to the concept or variable:\n\nChoose a psychological concept or variable of interest in the context of cross-cultural psychology.\nProvide a brief overview of the concept or variable, including its relevance and importance within the field of psychology and cross-cultural research.\n\nMethodology and analysis:\n\nOutline the methodology for your study, specifying the type of analysis you plan to conduct (e.g., equivalence tests, regressions, etc.).\nExplain the rationale for choosing the specific analysis technique, and how it will help address your research question in the context of cross-cultural psychology.\n\nCultural background of research participants:\n\nIdentify the cultural background(s) of your research participants.\nJustify your selection of participants, discussing the importance of studying this concept or variable in the chosen cultural context(s) and its relevance to cross-cultural psychology.\n\nValidity and reliability considerations:\n\nAddress potential challenges in measuring the chosen concept or variable across different cultural contexts.\nDescribe the strategies you will employ to ensure validity and reliability in your cross-cultural measurement.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (including references)\nUse APA style for citations and references\nInclude tables and/or figures as needed to support your analysis\nSubmit your assignment as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit the code used for data analysis in a format appropriate for your chosen software (e.g. your R script).\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the introduction to the chosen psychological concept or variable in the context of cross-cultural psychology\nAppropriateness and rigor of the chosen methodology and analysis technique for cross-cultural research\nWell-founded justification for the selection of research participants and their cultural background(s)\nThorough consideration of validity and reliability challenges in cross-cultural measurement\nOverall organization, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript.\n\n\n\nOption 2: Theoretical Application Causal Inference in Psychology: Examining the Effect of Exposure on Dimensions of Well-being Modified by Key Cultural or Sociodemographic Categories\n\nObjective:\n\nUtilize the NZAVS longitudinal synthetic data set to develop a study quantifying the causal effect of a specific exposure on a dimension or dimensions of well-being, as modified by one of the following sociodemographic categories: born_nz, eth_cat, big_doms, or gen_cohort.\n\n\n\nInstructions:\n\nTheoretical interest and research question:\n\nBriefly describe the significance of your chosen exposure and its potential effects on the selected outcome(s) (as modified by the chosen cultural or sociodemographic category).\nExplain the theoretical relevance of understanding these relationships within the field of psychology.\nClearly state your research question.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG representing the relationships between the exposure, outcome(s), chosen sociodemographic category, and potential sources of bias.\nClearly label all variables and arrows in the DAG to ensure easy interpretation.\n\nConfounding control strategy:\n\nDetail your approach to addressing confounding in the analysis.\nJustify your choice of confounders and their relationship to the exposure, outcome(s), and chosen sociodemographic category.\n\nAssumptions and statistical models:\n\nDiscuss the assumptions underlying your approach to causal inference and the statistical model you will employ.\nCritically evaluate the robustness of these assumptions and the potential limitations of your chosen model.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (excluding tables and references)\nAdhere to APA style for citations and references\nIncorporate tables and/or figures as needed to support your analysis\nSubmit as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit any code used for preliminary data analysis in a format appropriate for your chosen software (e.g. your R script, LaTeX)\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the theoretical framework, research question, and study design\nQuality and appropriateness of the DAG for representing relationships and sources of bias\nRigor and validity of the confounding control strategy\nComprehensive discussion of assumptions and statistical models, including potential limitations\nOverall organization, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript."
  },
  {
    "objectID": "index.html#assessment-3-theoretical-application-5000-word-maximum-including-references",
    "href": "index.html#assessment-3-theoretical-application-5000-word-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 3: THEORETICAL APPLICATION (5,000 word maximum including references)",
    "text": "Assessment 3: THEORETICAL APPLICATION (5,000 word maximum including references)\n\nOption 1: Theoretical Application: Cross-Cultural Psychology: Investigating a Psychological Concept or Variable of Interest in Diverse Cultural Contexts (5,000 word maximum including references)\n\nObjective:\n\nConduct a study focusing on cross-cultural measurement of a specific psychological concept or variable of interest, with attention to methodological rigor, cultural diversity, and the cultural background of research participants.\n\n\n\nInstructions:\n\nIntroduction to the concept or variable:\n\nChoose a psychological concept or variable of interest in the context of cross-cultural psychology.\nProvide a brief overview of the concept or variable, including its relevance and importance within the field of psychology and cross-cultural research.\n\nMethodology and analysis:\n\nOutline the methodology for your study, specifying the type of analysis you plan to conduct (e.g., equivalence tests, regressions, etc.).\nExplain the rationale for choosing the specific analysis technique, and how it will help address your research question in the context of cross-cultural psychology.\n\nCultural background of research participants:\n\nIdentify the cultural background(s) of your research participants.\nJustify your selection of participants, discussing the importance of studying this concept or variable in the chosen cultural context(s) and its relevance to cross-cultural psychology.\n\nValidity and reliability considerations:\n\nAddress potential challenges in measuring the chosen concept or variable across different cultural contexts.\nDescribe the strategies you will employ to ensure validity and reliability in your cross-cultural measurement.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (including references)\nUse APA style for citations and references\nInclude tables and/or figures as needed to support your analysis\nSubmit your assignment as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit the code used for data analysis in a format appropriate for your chosen software (e.g. your R script).\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the introduction to the chosen psychological concept or variable in the context of cross-cultural psychology\nAppropriateness and rigor of the chosen methodology and analysis technique for cross-cultural research\nWell-founded justification for the selection of research participants and their cultural background(s)\nThorough consideration of validity and reliability challenges in cross-cultural measurement\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript.\n\n\n\nOption 2: Theoretical Application Causal Inference in Psychology: Examining the Effect of Exposure on Dimensions of Well-being Modified by Key Cultural or Sociodemographic Categories\n\nObjective:\n\nUtilise the NZAVS longitudinal synthetic data set to develop a study quantifying the causal effect of a specific exposure on a dimension or dimensions of well-being, as modified by one of the following sociodemographic categories: born_nz, eth_cat, big_doms, or gen_cohort.\n\n\n\nInstructions:\n\nTheoretical interest and research question:\n\nBriefly describe the significance of your chosen exposure and its potential effects on the selected outcome(s) (as modified by the chosen cultural or sociodemographic category).\nExplain the theoretical relevance of understanding these relationships within the field of psychology.\nClearly state your research question.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG representing the relationships between the exposure, outcome(s), chosen sociodemographic category, and potential sources of bias.\nClearly label all variables and arrows in the DAG to ensure easy interpretation.\n\nConfounding control strategy:\n\nDetail your approach to addressing confounding in the analysis.\nJustify your choice of confounders and their relationship to the exposure, outcome(s), and chosen sociodemographic category.\n\nAssumptions and statistical models:\n\nDiscuss the assumptions underlying your approach to causal inference and the statistical model you will employ.\nCritically evaluate the robustness of these assumptions and the potential limitations of your chosen model.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (excluding tables and references)\nAdhere to APA style for citations and references\nIncorporate tables and/or figures as needed to support your analysis\nSubmit as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit any code used for preliminary data analysis in a format appropriate for your chosen software (e.g. your R script, LaTeX)\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the theoretical framework, research question, and study design\nQuality and appropriateness of the DAG for representing relationships and sources of bias\nRigor and validity of the confounding control strategy\nComprehensive discussion of assumptions and statistical models, including potential limitations\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript."
  },
  {
    "objectID": "index.html#assessment-4-letter-to-the-reviewer",
    "href": "index.html#assessment-4-letter-to-the-reviewer",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "ASSESSMENT 4 LETTER TO THE REVIEWER",
    "text": "ASSESSMENT 4 LETTER TO THE REVIEWER\nUsing the reviewer letter you received, respond to the points raised by the reviewer. Where did you alter your methods and pre-registration, where do results support your reviewer, where do they not support their claims."
  },
  {
    "objectID": "index.html#assessment-5-statistical-assessment-max-4000-words",
    "href": "index.html#assessment-5-statistical-assessment-max-4000-words",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "ASSESSMENT 5 Statistical Assessment (max 4,000 words)",
    "text": "ASSESSMENT 5 Statistical Assessment (max 4,000 words)\n\nOption 1: Cross-Cultural Psychology: Examining Measurement Invariance and Cultural Bias in Psychological Constructs\n\nObjective:\n\nConduct a statistical analysis to examine configural, metric, and scalar equivalence of your pre-registered study data, focusing on the technical aspects of measurement invariance testing, analysis, reporting, and discussion of findings in the context of cross-cultural psychology.\n\n\n\nInstructions:\n\nConstruct description and sample information:\n\nProvide a minimal description of the construct(s) and the measure used in your study.\nName the samples/countries included in your analysis.\nState the purpose of the analysis and the theoretical question being addressed.\n\nAnalysis procedure:\n\nDescribe the procedure you used to examine configural, metric, and scalar equivalence, including a brief description of each step.\nJustify any decisions you made during the process, explaining their rationale in the context of measurement invariance testing.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nResults reporting:\n\nReport your findings in a clear and concise manner, similar to the results section of an empirical paper.\nPresent factor loadings (optional) and other relevant indices in an appropriate table, following APA style.\n\nDiscussion:\n\nDiscuss your findings, focusing on levels of equivalence, biases identified (if any), biases ruled out, and potential remaining biases.\nProvide a concise discussion on the theoretical implications of your analysis and results, connecting the findings to theories about culture and cultural differences.\n\n\n\n\nRequirements:\n\nMaximum length: 4,000 words (excluding tables, references, and code; provide word count on a separate title page)\nUse APA style for citations, references, tables, and figures\nUse 12-point Times New Roman font or similar\nSubmit your assignment as a single PDF file\n\n\nEvaluation Criteria:\n\nClarity and coherence of the construct description, sample information, and theoretical question\nAppropriateness and rigor of the analysis procedure, including justification for decisions made\nCorrectness and replicability of the submitted code\nAccurate and concise reporting of results, following APA style\nComprehensive and focused discussion of findings, including levels of equivalence, biases, and theoretical implications in the context of cross-cultural psychology\nOverall organisation, presentation, and adherence to formatting guidelines\n\n\n\n\n\nOption 2: Causal Inference in Psychology: Developing the Results and Discussion Sections for a Study on the Effect of Exposure on Dimensions of Well-being Modified by Key Sociodemographic Categories\n\nObjective:\n\nBased on your previously developed study using the NZAVS longitudinal synthetic data set, create the Results and Discussion sections of a manuscript, detailing the findings of your causal inference analysis and their implications in the context of psychology.\n\n\n\nInstructions:\n\nData analysis and results presentation:\n\nConduct the data analysis as planned in your previous assignment, using appropriate statistical models and confounding control strategies.\nPresent the results in a clear and concise manner, similar to the results section of an empirical paper.\nInclude tables and/or figures as needed to support your findings, following APA style.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nDiscussion of findings:\n\nDiscuss the findings in the context of the exposure, outcome(s), and the chosen sociodemographic category.\nAddress the potential limitations of your study, including the robustness of assumptions and potential biases.\nConnect the findings to relevant theories in psychology, discussing their implications for the field.\n\nConclusion and future directions:\n\nSummarise the main findings of your study and their significance in the context of psychology and causal inference.\nSuggest potential future research directions based on your findings and their theoretical implications.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (excluding tables, references, and code; provide word count on a separate title page)\nUse APA style for citations, references, tables, and figures\nUse 12-point Times New Roman font or similar\nSubmit your assignment as a single PDF file\n\n\n\nEvaluation Criteria:\n\nAppropriateness and rigor of the data analysis, including the use of statistical models and confounding control strategies\nClear and concise presentation of results, following APA style\nCorrectness and replicability of the submitted code\nComprehensive and focused discussion of findings, including potential limitations, assumptions, biases, and theoretical implications in the context of psychology\nWell-structured conclusion and suggestions for future research directions\nOverall organisation, presentation, and adherence to formatting guidelines"
  },
  {
    "objectID": "index.html#assessment-4-constructive-review-of-a-pre-registration-study-in-cross-cultural-psychology-or-causal-inference-1000-words-maximum-including-references",
    "href": "index.html#assessment-4-constructive-review-of-a-pre-registration-study-in-cross-cultural-psychology-or-causal-inference-1000-words-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 4: Constructive Review of a Pre-Registration Study in Cross-Cultural Psychology or Causal Inference (1,000 words maximum, including references)",
    "text": "Assessment 4: Constructive Review of a Pre-Registration Study in Cross-Cultural Psychology or Causal Inference (1,000 words maximum, including references)\n\nObjective:\nReview and provide constructive feedback on the Introduction and Methods sections of a pre-registration study submitted by a fellow student, focusing on the theoretical foundations and methodological rigour in the context of cross-cultural psychology or causal inference research. Instructions:\n\nFamiliarise yourself with the assigned study:\n\nRead and understand the submitted pre-registration study, including its theoretical background, research question, and methodology. Evaluate the theoretical foundations:\nAssess the clarity, coherence, and relevance of the study’s theoretical background and research question. Consider the significance of the chosen psychological concept or exposure variable and its potential effects on the selected outcome(s).\n\n\n\nBriefly review the methodology:\n\nCritically examine the proposed methodology, including the choice of statistical analysis techniques, the cultural background of research participants (if applicable), and confounding control strategies (if applicable).\nAssess the appropriateness and rigour of the chosen methodology for addressing the research question in the context of cross-cultural psychology or causal inference.\n\n\n\nProvide concrete and respectful feedback:\n\nOffer specific, actionable suggestions for improving the study’s theoretical foundations, research question, and methodology.\nMaintain a respectful and professional tone throughout your review, acknowledging the efforts of the student while providing constructive criticism.\n\n\n\nRequirements:\n\nMaximum length: 1,000 words (including references)\nUse APA style for citations and references\nSubmit your review as a single PDF file\n\n\n\nEvaluation Criteria:\n\nThorough understanding and critical assessment of the assigned pre-registration study\nClear and coherent evaluation of the study’s theoretical foundations and research question\nrigorous review of the study’s methodology in the context of cross-cultural psychology or causal inference\nSpecific, actionable, and respectful feedback provided to improve the study\nAdherence to word limit and formatting guidelines"
  },
  {
    "objectID": "index.html#assessment-6-letter-to-the-reviewer-1000-words",
    "href": "index.html#assessment-6-letter-to-the-reviewer-1000-words",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 6: Letter to the reviewer (1,000 words)",
    "text": "Assessment 6: Letter to the reviewer (1,000 words)\n\nObjective:\n\nRespond to the feedback provided by the reviewer of your pre-registration study, detailing any changes made to your methods and pre-registration, and discussing how the results support or refute the reviewer’s claims.\n\n\n\nInstructions:\n\nAddress the feedback received:\n\nCarefully read and consider the feedback provided by the reviewer of your pre-registration study.\n\nDetail the changes made:\n\nDescribe any alterations you made to your study’s methods, theoretical foundations, or pre-registration based on the reviewer’s feedback.\nExplain the rationale behind each change, discussing how it strengthens your study.\n\nPresent the results in relation to the reviewer’s feedback:\n\nSummarise the main findings of your study, focusing on aspects that directly address the points raised by the reviewer.\nDiscuss how your results support or refute the reviewer’s claims, providing evidence and reasoning for your conclusions.\n\nRespond professionally and respectfully:\n\nMaintain a respectful and professional tone throughout your letter, acknowledging the reviewer’s efforts in providing constructive feedback and their potential concerns.\nClearly address each point raised by the reviewer, providing a thorough and reasoned response.\n\n\n\n\nRequirements:\n\nMaximum length: 1,000 words (including references)\nUse APA style for citations and references\nSubmit your letter as a single PDF file\n\n\n\nEvaluation Criteria:\n\nThorough and thoughtful consideration of the reviewer’s feedback\nClear and coherent description of changes made to the study based on the reviewer’s suggestions\nConvincing presentation of results in relation to the reviewer’s feedback, with well-supported conclusions\nProfessional and respectful tone maintained throughout the letter\nAdherence to word limit and formatting guidelines"
  },
  {
    "objectID": "index.html#assessment-3-theoretical-application5000-word-maximum-including-references",
    "href": "index.html#assessment-3-theoretical-application5000-word-maximum-including-references",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessment 3: Theoretical Application(5,000 word maximum including references)",
    "text": "Assessment 3: Theoretical Application(5,000 word maximum including references)\n\nOption 1: Cross-Cultural Psychology: Investigating a Psychological Concept or Variable of Interest in Diverse Cultural Contexts (5,000 word maximum including references)\n\nObjective:\n\nConduct a study focusing on cross-cultural measurement of a specific psychological concept or variable of interest, with attention to methodological rigour, cultural diversity, and the cultural background of research participants.\n\n\n\nInstructions:\n\nIntroduction to the concept or variable:\n\nChoose a psychological concept or variable of interest in the context of cross-cultural psychology.\nProvide a brief overview of the concept or variable, including its relevance and importance within the field of psychology and cross-cultural research.\n\nMethodology and analysis:\n\nOutline the methodology for your study, specifying the type of analysis you plan to conduct (e.g., equivalence tests, regressions, etc.).\nExplain the rationale for choosing the specific analysis technique, and how it will help address your research question in the context of cross-cultural psychology.\n\nCultural background of research participants:\n\nIdentify the cultural background(s) of your research participants.\nJustify your selection of participants, discussing the importance of studying this concept or variable in the chosen cultural context(s) and its relevance to cross-cultural psychology.\n\nValidity and reliability considerations:\n\nAddress potential challenges in measuring the chosen concept or variable across different cultural contexts.\nDescribe the strategies you will employ to ensure validity and reliability in your cross-cultural measurement.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (including references)\nUse APA style for citations and references\nInclude tables and/or figures as needed to support your analysis\nSubmit your assignment as a single PDF file\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the introduction to the chosen psychological concept or variable in the context of cross-cultural psychology\nAppropriateness and rigour of the chosen methodology and analysis technique for cross-cultural research\nWell-founded justification for the selection of research participants and their cultural background(s)\nThorough consideration of validity and reliability challenges in cross-cultural measurement\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript.\n\n\n\nOption 2: Causal Inference in Cultural Psychology: Examining the Effect of Exposure on Dimensions of Well-being Modified by Key Cultural or Sociodemographic Categories\n\nObjective:\n\nUtilise the NZAVS longitudinal synthetic data set to develop a study quantifying the causal effect of a specific exposure on a dimension or dimensions of well-being, as modified by one of the following sociodemographic categories: born_nz, eth_cat, big_doms, or gen_cohort.\n\n\n\nInstructions:\n\nTheoretical interest and research question:\n\nBriefly describe the significance of your chosen exposure and its potential effects on the selected outcome(s) (as modified by the chosen cultural or sociodemographic category).\nExplain the theoretical relevance of understanding these relationships within the field of psychology.\nClearly state your research question.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG representing the relationships between the exposure, outcome(s), chosen sociodemographic category, and potential sources of bias.\nClearly label all variables and arrows in the DAG to ensure easy interpretation.\n\nConfounding control strategy:\n\nDetail your approach to addressing confounding in the analysis.\nJustify your choice of confounders and their relationship to the exposure, outcome(s), and chosen sociodemographic category.\n\nAssumptions and statistical models:\n\nDiscuss the assumptions underlying your approach to causal inference and the statistical model you will employ.\nCritically evaluate the robustness of these assumptions and the potential limitations of your chosen model.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (excluding tables and references)\nAdhere to APA style for citations and references\nIncorporate tables and/or figures as needed to support your analysis\nSubmit as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit any code used for preliminary data analysis in a format appropriate for your chosen software (e.g. your R script, LaTeX)\n\n\n\nEvaluation Criteria:\n\nClarity and coherence of the theoretical framework, research question, and study design\nQuality and appropriateness of the DAG for representing relationships and sources of bias\nrigour and validity of the confounding control strategy\nComprehensive discussion of assumptions and statistical models, including potential limitations\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the introduction and methods part of a psychology manuscript."
  },
  {
    "objectID": "index.html#assessement-5-statistical-assessment-max-4000-words",
    "href": "index.html#assessement-5-statistical-assessment-max-4000-words",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "Assessement 5 Statistical Assessment (max 4,000 words)",
    "text": "Assessement 5 Statistical Assessment (max 4,000 words)\n\nOption 1: Cross-Cultural Psychology: Examining Measurement Invariance and Cultural Bias in Psychological Constructs\n\nObjective:\n\nConduct a statistical analysis to examine configural, metric, and scalar equivalence of your pre-registered study data, focusing on the technical aspects of measurement invariance testing, analysis, reporting, and discussion of findings in the context of cross-cultural psychology.\n\n\n\nInstructions:\n\nConstruct description and sample information:\n\nProvide a minimal description of the construct(s) and the measure used in your study.\nName the samples/countries included in your analysis.\nState the purpose of the analysis and the theoretical question being addressed.\n\nAnalysis procedure:\n\nDescribe the procedure you used to examine configural, metric, and scalar equivalence, including a brief description of each step.\nJustify any decisions you made during the process, explaining their rationale in the context of measurement invariance testing.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nResults reporting:\n\nReport your findings in a clear and concise manner, similar to the results section of an empirical paper.\nPresent factor loadings (optional) and other relevant indices in an appropriate table, following APA style.\n\nDiscussion:\n\nDiscuss your findings, focusing on levels of equivalence, biases identified (if any), biases ruled out, and potential remaining biases.\nProvide a concise discussion on the theoretical implications of your analysis and results, connecting the findings to theories about culture and cultural differences.\n\n\n\n\nRequirements:\n\nMaximum length: 4,000 words (excluding tables, references, and code; provide word count on a separate title page)\nUse APA style for citations, references, tables, and figures\nUse 12-point Times New Roman font or similar\nSubmit your assignment as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit any code used for preliminary data analysis in a format appropriate for your chosen software (e.g. your R script, LaTeX)\n\n\nEvaluation Criteria:\n\nClarity and coherence of the construct description, sample information, and theoretical question\nAppropriateness and rigour of the analysis procedure, including justification for decisions made\nCorrectness and replicability of the submitted code\nAccurate and concise reporting of results, following APA style\nComprehensive and focused discussion of findings, including levels of equivalence, biases, and theoretical implications in the context of cross-cultural psychology\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the results and discussion parts of a psychology manuscript.\n\n\n\n\nOption 2: Causal Inference in Psychology: Developing the Results and Discussion Sections for a Study on the Effect of Exposure on Dimensions of Well-being Modified by Key Sociodemographic Categories\n\nObjective:\n\nBased on your previously developed study using the NZAVS longitudinal synthetic data set, create the Results and Discussion sections of a manuscript, detailing the findings of your causal inference analysis and their implications in the context of psychology.\n\n\n\nInstructions:\n\nData analysis and results presentation:\n\nConduct the data analysis as planned in your previous assignment, using appropriate statistical models and confounding control strategies.\nPresent the results in a clear and concise manner, similar to the results section of an empirical paper.\nInclude tables and/or figures as needed to support your findings, following APA style.\n\nCode submission:\n\nSubmit your replicable and working code as a separate file, preferably in R script format.\n\nDiscussion of findings:\n\nDiscuss the findings in the context of the exposure, outcome(s), and the chosen sociodemographic category.\nAddress the potential limitations of your study, including the robustness of assumptions and potential biases.\nConnect the findings to relevant theories in psychology, discussing their implications for the field.\n\nConclusion and future directions:\n\nSummarise the main findings of your study and their significance in the context of psychology and causal inference.\nSuggest potential future research directions based on your findings and their theoretical implications.\n\n\n\n\nRequirements:\n\nMaximum length: 5,000 words (excluding tables, references, and code; provide word count on a separate title page)\nUse APA style for citations, references, tables, and figures\nUse 12-point Times New Roman font or similar\nSubmit your assignment as a single PDF file\n\n\n\nAdditional Submission:\n\nAs a separate file, submit any code used for preliminary data analysis in a format appropriate for your chosen software (e.g. your R script, LaTeX)\n\n\n\nEvaluation Criteria:\n\nAppropriateness and rigour of the data analysis, including the use of statistical models and confounding control strategies\nClear and concise presentation of results, following APA style\nCorrectness and replicability of the submitted code\nComprehensive and focused discussion of findings, including potential limitations, assumptions, biases, and theoretical implications in the context of psychology\nWell-structured conclusion and suggestions for future research directions\nOverall organisation, presentation, and adherence to formatting guidelines\n\nNote: This assessment is equivalent to the results and discussion parts of a psychology manuscript."
  },
  {
    "objectID": "content/07-content.html#introduction-motivating-example",
    "href": "content/07-content.html#introduction-motivating-example",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Introduction: Motivating Example",
    "text": "Introduction: Motivating Example\nConsider the following cross-cultural question:\n\nDoes bilingualism improve cognitive abilities in children?\n\nThere is evidence that bilingual children perform better oat cognitive tasks, but is learning more than one language a confounding factors?\nIn this lecture, we will discuss the fundamental problem of causal inference, its assumptions, and how we can address this issue in both experimental and observational settings."
  },
  {
    "objectID": "content/07-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/07-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem\nTo understand the fundamental problem of causal inference, let’s first define two potential outcomes for each individual in our study:\n\nY_i^{a = 1}: The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when A = 1.\nY_i^{a = 0}:: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = 0.\n\nThe causal effect of bilingualism on cognitive ability for individual i is then defined as the difference between these potential outcomes:\n\n\\text{Causal Effect}_i = Y^{a=1} - Y^{a=0}\n\nNote that sometimes we will write this contrast as:\n\n\\text{Causal Effect}_i = Y(1) - Y(0)\n\nor\n\n\\text{Causal Effect}_i = Y^{1} - Y^{0}\n\nWe say there is a causal effect if:\n\nY^{a=1} - Y^{a=0}  \\neq 0\n\nThe Philosopher David Hume defines causation in the following way:\n\n“We may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second [definition 1]. Or, in other words, where, if the first object had not been, the second never would have existed [definition 2].”  - Enquiries Concerning Human Understanding, and Concerning the Principles of Morals\n\nThis is a contrast between two states of the world. One in which a child recieves bi-lingual exposure and one in which a child does not.\nHowever, consider that we can only observe one of the potential outcomes for each child. The contrast we require for identifying a causal effect is typically not observed.\nRobert Frost writes,\n\nTwo roads diverged in a yellow wood,\u000b And sorry I could not travel both\u000b And be one traveler, long I stood\u000b And looked down one as far as I could\u000b To where it bent in the undergrowth;\u000b\n\n\nThen took the other, as just as fair,\u000b And having perhaps the better claim,\u000b Because it was grassy and wanted wear;\u000b Though as for that the passing there\u000b Had worn them really about the same,\u000b \u000b And both that morning equally lay\u000b In leaves no step had trodden black.\u000b Oh, I kept the first for another day!\u000b Yet knowing how way leads on to way,\u000b I doubted if I should ever come back.\u000b\n\n\nI shall be telling this with a sigh\u000b Somewhere ages and ages hence:\u000b Two roads diverged in a wood, and I—\u000b I took the one less traveled by,\u000b And that has made all the difference. – The Road Not Taken\n\n“And sorry I could not travel both. And be one traveller\\dots”\nThe fact that causal contrasts are not observed on individuals is called “The fundamental problem of causal inference.”\nWe will discuss how this missing data problem arises in different research designs and explore strategies for addressing it.\nFor now, note that the problem isn’t merely one of statistical analysis on the data. The problem is that the relevant data to identify individual causal effects are missing."
  },
  {
    "objectID": "content/07-content.html#part-2-fundamental-assumptions-of-causal-inference",
    "href": "content/07-content.html#part-2-fundamental-assumptions-of-causal-inference",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 2: Fundamental Assumptions of Causal Inference",
    "text": "Part 2: Fundamental Assumptions of Causal Inference\nAlthough we typically cannot observe individual causal effects, under certain assumptions we can obtain average causal effects.\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}"
  },
  {
    "objectID": "content/07-content.html#part-3-causal-inference-from-randomized-experiments",
    "href": "content/07-content.html#part-3-causal-inference-from-randomized-experiments",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 3: Causal Inference from Randomized Experiments",
    "text": "Part 3: Causal Inference from Randomized Experiments\nRandomized experiments can help us overcome the the fundamental problem of causal inference. By randomly assigning individuals to treatment (bilingual) or control (monolingual) groups, we create comparable groups that allow us to estimate causal effects.\nRandom assignment ensures that all observed and unobserved confounders are balanced between the treatment and control groups on average, satisfying the conditionally exchangeable assumption.\nThe key benefits of randomized experiments are:\n\nUnbiased estimation of average treatment effects.\nControl of both observed and unobserved confounding.\nFacilitated interpretation of causal effects.\n\nNotice that the results of an experiment recover Average Treatment Effects, not individual causal effects. Although a treament might be beneficial for the majority of people, it might not be benefitial for you.\nThis consideration may be extended to subgroups. Although a treatment might be beneficial, on average, for the majority of a population, it might not be benefitial for a sub-population. However, unlike individual causal effects, we may obtain causal effect estimates within sub-populations. We will will return to this topic next week."
  },
  {
    "objectID": "content/07-content.html#part-4-challenges-in-identifying-causal-effects-from-observational-data",
    "href": "content/07-content.html#part-4-challenges-in-identifying-causal-effects-from-observational-data",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 4: Challenges in Identifying Causal Effects from Observational Data",
    "text": "Part 4: Challenges in Identifying Causal Effects from Observational Data\nIn observational studies, we cannot randomly assign individuals to treatment and control groups. As a result, there may be confounding factors that affect both the treatment assignment and the outcome, making it difficult to identify causal effects.\nIn this context, some strategies to address confounding include:\n\nMatching: Pair individuals with similar observed covariates in treatment and control groups. We will discuss various matching techniques, such as exact matching, nearest-neighbor matching, and optimal matching.\nPropensity score methods: Use the propensity score (the probability of treatment given covariates) to balance treatment and control groups on observed covariates. We will cover propensity score matching, weighting, and subclassification, and discuss their advantages and limitations.\nRegression adjustment: include potential confounders as control variables in a regression model. We will discuss the assumptions behind this approach, as well as techniques for model selection and diagnostics.\n\nHowever, these methods rely on the assumption of conditionally exchangeable treatment assignment, which is often unverifiable. They can only account for observed confounding, leaving the possibility of unobserved confounders affecting our causal estimates. For this reason we use sentistive analyses (topic next week. )"
  },
  {
    "objectID": "content/07-content.html#part-5-causal-graphs-and-directed-acyclic-graphs-dags",
    "href": "content/07-content.html#part-5-causal-graphs-and-directed-acyclic-graphs-dags",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 5: Causal Graphs and Directed Acyclic Graphs (DAGs)",
    "text": "Part 5: Causal Graphs and Directed Acyclic Graphs (DAGs)\nCausal graphs, also known as directed acyclic graphs (DAGs), provide a powerful way to represent causal relationships between variables. In a DAG, nodes represent variables, and directed edges represent causal relationships between them.\n\nDAG basics: How to construct and interpret DAGs, including the concepts of parents, children, ancestors, and descendants.\nPsychology begins with a question. Ours: “Does A cause Y?”\n\nD-separation and conditional independence: Understanding when variables are conditionally independent given a set of conditioning variables in a DAG, and how this relates to the concept of d-separation.\nIdentifying confounders: Using DAGs to identify confounders that must be controlled for in order to estimate causal effects.\nCollider bias and conditioning: Understanding how conditioning on colliders can induce bias in causal estimates, and how to avoid this issue.\nMediation and direct/indirect effects: Using DAGs to assess mediation and decompose total effects into direct and indirect effects."
  },
  {
    "objectID": "slides/07-slides.html#goals",
    "href": "slides/07-slides.html#goals",
    "title": "Introduction to Causal Inference",
    "section": "Goals",
    "text": "Goals\nBy the end of this seminaryou will be able to:\n\nUnderstand the definition of “causality” as it is used in the human and health sciences.\nUnderstand the assumptions required for consistently estimating causal effects.\nUnderstand how to use causal diagrammes to assess these assumptions.\n\nKey Concepts\n\nCause/Effect\nConfounder\nCollider"
  },
  {
    "objectID": "slides/07-slides.html#recall-where-does-psychology-start",
    "href": "slides/07-slides.html#recall-where-does-psychology-start",
    "title": "Introduction to Causal Inference",
    "section": "Recall: Where does psychology start?",
    "text": "Recall: Where does psychology start?\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/07-slides.html#group-discussion",
    "href": "slides/07-slides.html#group-discussion",
    "title": "Introduction to Causal Inference",
    "section": "Group discussion",
    "text": "Group discussion\n\nWe know that bilingual children tend to perform better on various cognitive tasks. Why might this be the case?\nHow can we know whether it is bilingualism that causes better performance on various cognitive tasks?"
  },
  {
    "objectID": "slides/07-slides.html#definition-of-a-causality",
    "href": "slides/07-slides.html#definition-of-a-causality",
    "title": "Introduction to Causal Inference",
    "section": "Definition of a causality",
    "text": "Definition of a causality\nThe Philosopher David Hume defines causation in the following way:\n\n“we may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second [definition 1]. Or, in other words, where, if the first object had not been, the second never would have existed [definition 2].” - David Hume, Enquiries Concerning Human Understanding, and Concerning the Principles of Morals, Section VII"
  },
  {
    "objectID": "slides/07-slides.html#individual-causal-effects-are-generally-not-identified",
    "href": "slides/07-slides.html#individual-causal-effects-are-generally-not-identified",
    "title": "Introduction to Causal Inference",
    "section": "Individual causal effects are generally not identified",
    "text": "Individual causal effects are generally not identified\n\nWe say there is a causal effect for individual i if:\n\n\nY_i^{a=1} - Y_i^{a=0}  \\neq 0"
  },
  {
    "objectID": "slides/07-slides.html#conduct-a-literature-review-of-previous-research",
    "href": "slides/07-slides.html#conduct-a-literature-review-of-previous-research",
    "title": "Introduction to Causal Inference",
    "section": "Conduct a literature review of previous research",
    "text": "Conduct a literature review of previous research\n\nWhat would this tell us?\n\nwhat other researchers have found.\n\nWhat would this not tell us?\n\nwhat other researchers have not found.\nwhat other researchers got wrong."
  },
  {
    "objectID": "slides/07-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "href": "slides/07-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "title": "Introduction to Causal Inference",
    "section": "Conduct a survey of a large and diverse sample of individuals",
    "text": "Conduct a survey of a large and diverse sample of individuals\n\nWhere to begin?"
  },
  {
    "objectID": "slides/07-slides.html#measurement",
    "href": "slides/07-slides.html#measurement",
    "title": "Introduction to Causal Inference",
    "section": "Measurement",
    "text": "Measurement"
  },
  {
    "objectID": "slides/07-slides.html#what-else-might-go-wrong",
    "href": "slides/07-slides.html#what-else-might-go-wrong",
    "title": "Introduction to Causal Inference",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nSuppose we are confident in measurement, administer survey, and find an relationship.\n\nDoes marriage cause happiness cross culturally?"
  },
  {
    "objectID": "slides/07-slides.html#what-might-go-wrong",
    "href": "slides/07-slides.html#what-might-go-wrong",
    "title": "Introduction to Causal Inference",
    "section": "What might go wrong?",
    "text": "What might go wrong?"
  },
  {
    "objectID": "slides/07-slides.html#happiness-might-cause-marriage",
    "href": "slides/07-slides.html#happiness-might-cause-marriage",
    "title": "Introduction to Causal Inference",
    "section": "Happiness might cause marriage",
    "text": "Happiness might cause marriage"
  },
  {
    "objectID": "slides/07-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "href": "slides/07-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "title": "Introduction to Causal Inference",
    "section": "Age might be a confounder of both Marriage and Happiness",
    "text": "Age might be a confounder of both Marriage and Happiness"
  },
  {
    "objectID": "slides/07-slides.html#many-psychologists-will-simply-control-for-age",
    "href": "slides/07-slides.html#many-psychologists-will-simply-control-for-age",
    "title": "Introduction to Causal Inference",
    "section": "Many psychologists will simply “control for age”",
    "text": "Many psychologists will simply “control for age”\n\nThis doesn’t work if there is reverse causation: Regression coefficient for Marriage (M) is biased.\n\n\n\nCode\n## Simulate data \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 1000\n  A &lt;- rnorm(n, 1) # simulates age,\n  H &lt;- rnorm(n , A) #  simulates happy as function of age\n  M &lt;- rnorm(n , A) + .2 * H # simulate marriage as a function of age + happiness\n\n  \n# Simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  A = A, \n  H = H,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + A, data = simdat_B)\nsim_B  # returns output\n}\n\n# Replication 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results\nparameters::pool_parameters(r_lm_B)\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | Statistic |     df |      p\n------------------------------------------------------------------------------\n(Intercept) |    4.12e-03 | 0.06 | [-0.12, 0.13] |      0.07 | 220.99 | 0.947 \nM           |        0.19 | 0.04 | [ 0.10, 0.27] |      4.42 | 234.19 | &lt; .001\nA           |        0.77 | 0.07 | [ 0.64, 0.91] |     11.17 | 211.13 | &lt; .001"
  },
  {
    "objectID": "slides/07-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "href": "slides/07-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "title": "Introduction to Causal Inference",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\nWeek 7: Causal inference: Causal Graphs(Joseph Bulbulia). How to diagnosis sources of confounding and what to do about it.\nWeek 8: Causal inference: Target Trial Emulation (Joseph Bulbulia). How to emulate an experiment with observational data and how to identify the modification of effects by cultural groups.\nWeek 9: Causal inference: Reconsidering measurement and selection biases (Joseph Bulbulia). A discussion of problems in standard approaches to measurement.\nWeek 10 Causal inference: Cross-Cultural Experiments (Joseph Bulbulia & Guests). For whom to our results generalise?"
  },
  {
    "objectID": "slides/07-slides.html#course-conclusions",
    "href": "slides/07-slides.html#course-conclusions",
    "title": "Introduction to Causal Inference",
    "section": "Course conclusions",
    "text": "Course conclusions\nWeek 11 Ethics and Ownership (Joseph Bulbulia & Guests). Can causal inference help us to improve ethical reasoning? Why is data protection important? How do the ethics of data protection relate to the ethics of open science?\nWeek 12 Future Horizons (Joseph Bulbulia, Torven, Inkuk):"
  },
  {
    "objectID": "slides/07-slides.html#what-have-we-learned",
    "href": "slides/07-slides.html#what-have-we-learned",
    "title": "Introduction to Causal Inference",
    "section": "What have we learned?",
    "text": "What have we learned?\n\nCause/Effect: a contrast between the world under different interventions, at most one of which is realised.\nConfounding: failure to condition on a common cause.\nCollider bias: conditioning on a common effect\n\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/07-slides.html#references",
    "href": "slides/07-slides.html#references",
    "title": "Introduction to Causal Inference",
    "section": "References",
    "text": "References\nTBA\nBibliography\nBibliography\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/07-slides.html#what-else-might-go-wrong-1",
    "href": "slides/07-slides.html#what-else-might-go-wrong-1",
    "title": "Introduction to Causal Inference",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nSuppose we are confident in measurement, administer survey, and find an relationship.\n\nDoes marriage cause happiness cross culturally?"
  },
  {
    "objectID": "slides/07-slides.html#what-might-go-wrong-1",
    "href": "slides/07-slides.html#what-might-go-wrong-1",
    "title": "Introduction to Causal Inference",
    "section": "What might go wrong?",
    "text": "What might go wrong?"
  },
  {
    "objectID": "slides/07-slides.html#happiness-might-cause-marriage-1",
    "href": "slides/07-slides.html#happiness-might-cause-marriage-1",
    "title": "Introduction to Causal Inference",
    "section": "Happiness might cause marriage",
    "text": "Happiness might cause marriage"
  },
  {
    "objectID": "slides/07-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness-1",
    "href": "slides/07-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness-1",
    "title": "Introduction to Causal Inference",
    "section": "Age might be a confounder of both Marriage and Happiness",
    "text": "Age might be a confounder of both Marriage and Happiness"
  },
  {
    "objectID": "slides/07-slides.html#many-psychologists-will-simply-control-for-age-1",
    "href": "slides/07-slides.html#many-psychologists-will-simply-control-for-age-1",
    "title": "Introduction to Causal Inference",
    "section": "Many psychologists will simply “control for age”",
    "text": "Many psychologists will simply “control for age”\n\nThis doesn’t work if there is reverse causation: Regression coefficient for Marriage (M) is biased.\n\n\n\nCode\n## Simulate data \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 1000\n  A &lt;- rnorm(n, 1) # simulates age,\n  H &lt;- rnorm(n , A) #  simulates happy as function of age\n  M &lt;- rnorm(n , A) + .2 * H # simulate marriage as a function of age + happiness\n\n  \n# Simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  A = A, \n  H = H,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + A, data = simdat_B)\nsim_B  # returns output\n}\n\n# Replication 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results\nparameters::pool_parameters(r_lm_B)\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | Statistic |     df |      p\n------------------------------------------------------------------------------\n(Intercept) |    4.12e-03 | 0.06 | [-0.12, 0.13] |      0.07 | 220.99 | 0.947 \nM           |        0.19 | 0.04 | [ 0.10, 0.27] |      4.42 | 234.19 | &lt; .001\nA           |        0.77 | 0.07 | [ 0.64, 0.91] |     11.17 | 211.13 | &lt; .001"
  },
  {
    "objectID": "slides/07-slides.html#the-problem-is-confounding-what-can-we-do-about-it-1",
    "href": "slides/07-slides.html#the-problem-is-confounding-what-can-we-do-about-it-1",
    "title": "Introduction to Causal Inference",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\nWeek 7: Causal inference: Causal Graphs(Joseph Bulbulia). How to diagnosis sources of confounding and what to do about it.\nWeek 8: Causal inference: Target Trial Emulation (Joseph Bulbulia). How to emulate an experiment with observational data and how to identify the modification of effects by cultural groups.\nWeek 9: Causal inference: Reconsidering measurement and selection biases (Joseph Bulbulia). A discussion of problems in standard approaches to measurement.\nWeek 10 Causal inference: Cross-Cultural Experiments (Joseph Bulbulia & Guests). For whom to our results generalise?"
  },
  {
    "objectID": "slides/07-slides.html#course-conclusions-1",
    "href": "slides/07-slides.html#course-conclusions-1",
    "title": "Introduction to Causal Inference",
    "section": "Course conclusions",
    "text": "Course conclusions\nWeek 11 Ethics and Ownership (Joseph Bulbulia & Guests). Can causal inference help us to improve ethical reasoning? Why is data protection important? How do the ethics of data protection relate to the ethics of open science?\nWeek 12 Future Horizons (Joseph Bulbulia, Torven, Inkuk):"
  },
  {
    "objectID": "slides/07-slides.html#what-have-we-learned-1",
    "href": "slides/07-slides.html#what-have-we-learned-1",
    "title": "Introduction to Causal Inference",
    "section": "What have we learned?",
    "text": "What have we learned?\nKey Concepts ::: {.fragment .highlight-red} 1. Measurement = quantification 2. Validity = accuracy 3. Confounder = interference :::"
  },
  {
    "objectID": "slides/07-slides.html#references-1",
    "href": "slides/07-slides.html#references-1",
    "title": "Introduction to Causal Inference",
    "section": "References",
    "text": "References\nFor an account of the history of measurement in psychological research, see: (Briggs 2021)\nFor an account of key concepts and current debates in psychometrics, see: (Bandalos 2018)\nFor an account of special measurement problems that arise in cross-cultural research, see: (Van de Vijver and Leung 2021)\nFor an accessible introduction to causal inference and its history see: (Pearl and Mackenzie 2018)\nBibliography\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups.\n\n\n\n\n\n\n\nBandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. Guilford Publications.\n\n\nBriggs, Derek C. 2021. Historical and Conceptual Foundations of Measurement in the Human Sciences: Credos and Controversies. Routledge.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nVan de Vijver, Fons JR, and Kwok Leung. 2021. Methods and Data Analysis for Cross-Cultural Research. Vol. 116. Cambridge University Press."
  },
  {
    "objectID": "slides/07-slides.html#where-does-psychology-start",
    "href": "slides/07-slides.html#where-does-psychology-start",
    "title": "Introduction to Causal Inference",
    "section": "0Where does psychology start?",
    "text": "0Where does psychology start?\n\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/07-slides.html#hummes-definitions-of-a-causality",
    "href": "slides/07-slides.html#hummes-definitions-of-a-causality",
    "title": "Introduction to Causal Inference",
    "section": "Humme’s definitions of a causality",
    "text": "Humme’s definitions of a causality\n\n“we may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second [definition 1]. Or, in other words, where, if the first object had not been, the second never would have existed [definition 2].” - David Hume, Enquiries Concerning Human Understanding, and Concerning the Principles of Morals, Section VII"
  },
  {
    "objectID": "slides/07-slides.html#humes-definitions-of-a-causality",
    "href": "slides/07-slides.html#humes-definitions-of-a-causality",
    "title": "Introduction to Causal Inference",
    "section": "Hume’s definitions of a causality",
    "text": "Hume’s definitions of a causality\n\n“we may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second [definition 1]. Or, in other words, where, if the first object had not been, the second never would have existed [definition 2].” - David Hume, Enquiries Concerning Human Understanding, and Concerning the Principles of Morals, Section VII"
  },
  {
    "objectID": "slides/07-slides.html#individual-causal-effects",
    "href": "slides/07-slides.html#individual-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "Individual causal effects",
    "text": "Individual causal effects\nY_i^{a = 1}: The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when A = 1.\nY_i^{a = 0}: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = 0.\n\n\\text{Causal Effect}_i = Y^{a = 1} - Y^{a = 0}"
  },
  {
    "objectID": "slides/07-slides.html#individual-causal-quantities",
    "href": "slides/07-slides.html#individual-causal-quantities",
    "title": "Introduction to Causal Inference",
    "section": "Individual causal quantities",
    "text": "Individual causal quantities\nWe say there is a causal effect for individual i if:\n\nY_i^{a=1} - Y_i^{a=0}  \\neq 0"
  },
  {
    "objectID": "slides/07-slides.html#two-roads-diverge-in-a-yellow-wood",
    "href": "slides/07-slides.html#two-roads-diverge-in-a-yellow-wood",
    "title": "Introduction to Causal Inference",
    "section": "Two Roads Diverge in a Yellow Wood",
    "text": "Two Roads Diverge in a Yellow Wood\nRobert Frost writes,\n\nTwo roads diverged in a yellow wood,\n\n\nAnd sorry I could not travel both\u000b\n\n\nAnd be one traveler, long I stood\u000b\n\n\nAnd looked down one as far as I could\u000b\n\n\nTo where it bent in the undergrowth;\u000b\n\n\nThen took the other, as just as fair,\u000b\n\n\nAnd having perhaps the better claim,\u000b\n\n\nBecause it was grassy and wanted wear;\u000b\n\n\nThough as for that the passing there\u000b\n\n\nHad worn them really about the same,\u000b \u000b\n\n\nAnd both that morning equally lay\u000b\n\n\nIn leaves no step had trodden black.\u000b\n\n\nOh, I kept the first for another day!\u000b\n\n\nYet knowing how way leads on to way,\u000b\n\n\nI doubted if I should ever come back.\u000b\n\n\nI shall be telling this with a sigh\u000b\n\n\nSomewhere ages and ages hence:\u000b\n\n\nTwo roads diverged in a wood, and I—\u000b\n\n\nI took the one less traveled by,\u000b\n\n\nAnd that has made all the difference.\n\n\nRobert Frost, The Road Not Taken"
  },
  {
    "objectID": "slides/07-slides.html#how-can-we-identify-causal-effects",
    "href": "slides/07-slides.html#how-can-we-identify-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "How can we identify causal effects?",
    "text": "How can we identify causal effects?\n\nRecall the answers you proposed to bilingual causal question\n\ne.g. experiment: random assigment to bilingual training.\n\nHow does this work?\n\nConfounders equally distributed\nCount up results\nTake the average of the differences in the two groups."
  },
  {
    "objectID": "slides/07-slides.html#average-treatement-effect-in-a-randomised-experiment",
    "href": "slides/07-slides.html#average-treatement-effect-in-a-randomised-experiment",
    "title": "Introduction to Causal Inference",
    "section": "Average Treatement Effect in a Randomised Experiment",
    "text": "Average Treatement Effect in a Randomised Experiment\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\n\n\nE(\\delta) is the “estimand” or causal quantity of interest (the expected difference between the means of two randomised groups, or equivalentally, the mean of the differences)"
  },
  {
    "objectID": "slides/07-slides.html#what-is-the-problem",
    "href": "slides/07-slides.html#what-is-the-problem",
    "title": "Introduction to Causal Inference",
    "section": "What is the problem?",
    "text": "What is the problem?\n\nThese data required to compute this quantity is generally not available."
  },
  {
    "objectID": "content/07-content.html#conclusion-10-minutes",
    "href": "content/07-content.html#conclusion-10-minutes",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Conclusion (10 minutes)",
    "text": "Conclusion (10 minutes)\nIn this lecture, we discussed the fundamental problem of causal inference, its assumptions, the role of randomized experiments in estimating causal effects, the challenges in identifying causal effects from observational data, and the use of causal graphs to represent and analyze causal relationships. We focused on the example of bilingualism and cognitive abilities in children, but these concepts and methods can be applied to a wide range of research questions in cross-cultural psychology. As researchers, it is essential to be aware of these issues and to use appropriate methods to make valid causal inferences from our data. In later lectures, we will develop more cross-cultural research examples to illustrate the application of these methods in various contexts."
  },
  {
    "objectID": "slides/07-slides.html#assumptions-required-for-estimating-causal-effects-from-data",
    "href": "slides/07-slides.html#assumptions-required-for-estimating-causal-effects-from-data",
    "title": "Introduction to Causal Inference",
    "section": "Assumptions Required for Estimating Causal Effects From Data",
    "text": "Assumptions Required for Estimating Causal Effects From Data\n\nCausal Consistency: The values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.(see: Chatton, Hernan & Robbins)\nPositivity: The probability of receiving every value of the exposure within all strata of co-variates is greater than zero\nExchangeablility:The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates (see: Chatton, Hernan & Robbins)"
  },
  {
    "objectID": "slides/07-slides.html#causal-consistency-assumption",
    "href": "slides/07-slides.html#causal-consistency-assumption",
    "title": "Introduction to Causal Inference",
    "section": "Causal Consistency assumption",
    "text": "Causal Consistency assumption\n\nThe values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.\n\n\\begin{equation}\nY^{obs} = AY^{a=1} + (1-A)Y^{a=0}\n\\end{equation}"
  },
  {
    "objectID": "slides/07-slides.html#causal-consistency-implications",
    "href": "slides/07-slides.html#causal-consistency-implications",
    "title": "Introduction to Causal Inference",
    "section": "Causal Consistency: implications",
    "text": "Causal Consistency: implications\nFor individuals with exposure level A = 1:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 1) &= 1 \\times A  \\times Y^{a=1} + (1-1) \\times Y^{a=0}\\\\\n& = 1 \\times Y^{a=1} + 0 \\times  Y^{a=0} \\\\\n& =  Y^{a=1}\n\\end{split}\n\\end{equation}\nFor individuals with exposure level A = 0:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 0) &= 0 \\times A  \\times Y^{a=1} + (1-0) \\times Y^{a=0}\\\\\n& = 0 \\times Y^{a=1} + 1 \\times  Y^{a=0} \\\\\n& =  Y^{a=0}\n\\end{split}\n\\end{equation}\nWhich implies:\n\\begin{equation}\n\\begin{split}\nY_i &= Y_i^{a=1}~~~\\text{if}~ A_i = 1\\\\\nY_i &= Y_i^{a=0}~~~ \\text{if}~  A_i = 0\n\\end{split}\n\\end{equation}"
  },
  {
    "objectID": "slides/07-slides.html#causal-consistency-gets-from-observations-to-counterfactuals",
    "href": "slides/07-slides.html#causal-consistency-gets-from-observations-to-counterfactuals",
    "title": "Introduction to Causal Inference",
    "section": "Causal Consistency gets from observations to counterfactuals",
    "text": "Causal Consistency gets from observations to counterfactuals\nFor individuals with exposure level A = 1:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 1) &= 1 \\times A  \\times Y^{a=1} + (1-1) \\times Y^{a=0}\\\\\n& = 1 \\times Y^{a=1} + 0 \\times  Y^{a=0} \\\\\n& =  Y^{a=1}\n\\end{split}\n\\end{equation}\nFor individuals with exposure level A = 0:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 0) &= 0 \\times A  \\times Y^{a=1} + (1-0) \\times Y^{a=0}\\\\\n& = 0 \\times Y^{a=1} + 1 \\times  Y^{a=0} \\\\\n& =  Y^{a=0}\n\\end{split}\n\\end{equation}\nWhich implies:\n\\begin{equation}\n\\begin{split}\nY_i &= Y_i^{a=1}~~~\\text{if}~ A_i = 1\\\\\nY_i &= Y_i^{a=0}~~~ \\text{if}~  A_i = 0\n\\end{split}\n\\end{equation}"
  },
  {
    "objectID": "slides/07-slides.html#positivity",
    "href": "slides/07-slides.html#positivity",
    "title": "Introduction to Causal Inference",
    "section": "Positivity",
    "text": "Positivity\n\nThe probability of receiving every value of the exposure within all strata of co-variates is greater than zero\n\n\\begin{equation}\n0 &lt; \\Pr(A=a|L)&lt;1, ~ \\forall a \\in A, ~ \\forall a \\in L\n\\end{equation}\n\nTwo types of positivity violations\n\nRandom non-positivity: the casual effect of aging with observations missing at ages 40-41 (we use parametric models as a work around.)\nDeterministic non-positivity: the causal effect of hysterectomy in biological males (assumption violated)."
  },
  {
    "objectID": "slides/07-slides.html#conditional-exchangeability",
    "href": "slides/07-slides.html#conditional-exchangeability",
    "title": "Introduction to Causal Inference",
    "section": "Conditional Exchangeability",
    "text": "Conditional Exchangeability\n\nThe conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates\n\ndef: \\coprod means “independent of”, a|b translates to “a conditional on b”\n\\begin{equation}\nY^{a=1},Y^{a=0}\\coprod A|L\n\\end{equation}\nor equivalently\n\\begin{equation}\nA \\coprod Y^{a=1},Y^{a=0}|L\n\\end{equation}\nWhere L is the set of co-variates sufficient to ensure the independence of the counterfactual outcomes and the exposure."
  },
  {
    "objectID": "slides/07-slides.html#test",
    "href": "slides/07-slides.html#test",
    "title": "Introduction to Causal Inference",
    "section": "TEST",
    "text": "TEST\n\n\nCode\n## Simulate data \nset.seed(123)"
  },
  {
    "objectID": "slides/07-slides.html#causal-graphs",
    "href": "slides/07-slides.html#causal-graphs",
    "title": "Introduction to Causal Inference",
    "section": "Causal Graphs",
    "text": "Causal Graphs\nPsychology starts with a question. “Does A cause Y?”\n\ncg-1"
  },
  {
    "objectID": "slides/07-slides.html#problem-y---a-might-cause-marriage",
    "href": "slides/07-slides.html#problem-y---a-might-cause-marriage",
    "title": "Introduction to Causal Inference",
    "section": "Problem: Y -> A might cause marriage",
    "text": "Problem: Y -&gt; A might cause marriage"
  },
  {
    "objectID": "slides/07-slides.html#problem",
    "href": "slides/07-slides.html#problem",
    "title": "Introduction to Causal Inference",
    "section": "Problem:",
    "text": "Problem:\n\n\n\n\n\n\n##. How might we address this problem of temporal order?\n\nAnswer: collect time series data."
  },
  {
    "objectID": "content/07-content.html#conclusion",
    "href": "content/07-content.html#conclusion",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this lecture, we discussed the fundamental problem of causal inference, its assumptions, the role of randomized experiments in estimating causal effects, the challenges in identifying causal effects from observational data, and the use of causal graphs to represent and analyze causal relationships. You have learned how to use causal graphs to identify sources of bias. In the weeks ahead, we will apply this knowledge to address questions in cross-cultural psychology."
  },
  {
    "objectID": "slides/07-slides.html#question",
    "href": "slides/07-slides.html#question",
    "title": "Introduction to Causal Inference",
    "section": "Question:",
    "text": "Question:\n\ncg-2"
  },
  {
    "objectID": "slides/07-slides.html#one-problem",
    "href": "slides/07-slides.html#one-problem",
    "title": "Introduction to Causal Inference",
    "section": "One Problem",
    "text": "One Problem\n\ncg-3"
  },
  {
    "objectID": "slides/07-slides.html#solution-collect-time-series-data",
    "href": "slides/07-slides.html#solution-collect-time-series-data",
    "title": "Introduction to Causal Inference",
    "section": "Solution: collect time-series data",
    "text": "Solution: collect time-series data\n\ncg-4"
  },
  {
    "objectID": "slides/07-slides.html#why-time-series-data-are-critical",
    "href": "slides/07-slides.html#why-time-series-data-are-critical",
    "title": "Introduction to Causal Inference",
    "section": "Why time series data are critical",
    "text": "Why time series data are critical\n\ncg-5"
  },
  {
    "objectID": "slides/07-slides.html#solution-to-problem-of-common-causes",
    "href": "slides/07-slides.html#solution-to-problem-of-common-causes",
    "title": "Introduction to Causal Inference",
    "section": "Solution: to problem of common causes",
    "text": "Solution: to problem of common causes\n\ncg-8"
  },
  {
    "objectID": "slides/07-slides.html#another-problem",
    "href": "slides/07-slides.html#another-problem",
    "title": "Introduction to Causal Inference",
    "section": "Another problem",
    "text": "Another problem\n\ncg-7"
  },
  {
    "objectID": "slides/07-slides.html#conditioning-on-a-mediator-is-a-common-problem",
    "href": "slides/07-slides.html#conditioning-on-a-mediator-is-a-common-problem",
    "title": "Introduction to Causal Inference",
    "section": "Conditioning on a mediator is a common problem",
    "text": "Conditioning on a mediator is a common problem\n\ncg-9"
  },
  {
    "objectID": "slides/07-slides.html#aside-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "href": "slides/07-slides.html#aside-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "Aside: in the weeks ahead we will discuss inferring mediated causal effects",
    "text": "Aside: in the weeks ahead we will discuss inferring mediated causal effects"
  },
  {
    "objectID": "slides/07-slides.html#forshadowing-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "href": "slides/07-slides.html#forshadowing-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "Forshadowing: in the weeks ahead we will discuss inferring mediated causal effects",
    "text": "Forshadowing: in the weeks ahead we will discuss inferring mediated causal effects"
  },
  {
    "objectID": "slides/07-slides.html#collider-bias",
    "href": "slides/07-slides.html#collider-bias",
    "title": "Introduction to Causal Inference",
    "section": "Collider bias",
    "text": "Collider bias\n\ncg-13"
  },
  {
    "objectID": "slides/07-slides.html#collider-bias-1",
    "href": "slides/07-slides.html#collider-bias-1",
    "title": "Introduction to Causal Inference",
    "section": "Collider bias",
    "text": "Collider bias\n\ncg-13"
  },
  {
    "objectID": "slides/07-slides.html#conditioning-on-descendants",
    "href": "slides/07-slides.html#conditioning-on-descendants",
    "title": "Introduction to Causal Inference",
    "section": "Conditioning on descendants",
    "text": "Conditioning on descendants\n\ncg-14"
  },
  {
    "objectID": "slides/07-slides.html#summary",
    "href": "slides/07-slides.html#summary",
    "title": "Introduction to Causal Inference",
    "section": "Summary",
    "text": "Summary\n\ncg-15"
  },
  {
    "objectID": "slides/07-slides.html#advice-some-unconventional",
    "href": "slides/07-slides.html#advice-some-unconventional",
    "title": "Introduction to Causal Inference",
    "section": "Advice (some unconventional)",
    "text": "Advice (some unconventional)\n\nCauses must precede effects. Index your variables by time.\nArrange your variables in temporal order.\nDeclutter: remove any nodes that are not required to assess whether the association between an exposure and an outcome is biased.\nRemember a DAG is a qualitative tools. They do not represent non-linear associations or interactions. Do not attempt to draw an interaction by showing and edge (arrow) intersect and edge (arrow).\nDAGs differ from the graphs used for structural equation modelling. Unfortunately much of the literature in SEM has ignored the assumptions required for causal inference."
  },
  {
    "objectID": "slides/07-slides.html#summary-on-causal-diagrammes-dags",
    "href": "slides/07-slides.html#summary-on-causal-diagrammes-dags",
    "title": "Introduction to Causal Inference",
    "section": "Summary on Causal Diagrammes (DAGs)",
    "text": "Summary on Causal Diagrammes (DAGs)\n\nDAGs graphically reveal sources of bias.\nThere are five sources of bias\n\nTemporal order: it is unclear whether effects are measured before causes. Solution: collect time series data or when these are not available, make assumptions clear.\nCommon causes of the exposure and outcome. Solution: condition on common causes by including them in your statistical model (this weeks excercises focus on regression)\nConditioning on a mediator. Solution: do not condition on a mediator (unless mediation is of interest).\nConditioning on a collider. Solution: do not condition on a collider.\nInducing bias by conditioning on the descendent of a confounder. Solution: Draw your DAG and follow instructions for 1-4.\n\nCRITICAL NOTE: in observational studies we cannot be certain we have controlled for confounding. We should therefore always perform sensitivity analyses. Next week we will show you how to perform sensitivity analyses.\nCRITICAL NOTE 2: We have not yet described how to compute causal effects for comparisions between groups. That comes next week."
  },
  {
    "objectID": "slides/07-slides.html#another-problem-common-causes-of-a-an-y",
    "href": "slides/07-slides.html#another-problem-common-causes-of-a-an-y",
    "title": "Introduction to Causal Inference",
    "section": "Another problem: common causes of A an Y",
    "text": "Another problem: common causes of A an Y\n\ncg-6"
  },
  {
    "objectID": "content/07-content.html#part-4-causal-graphs-and-directed-acyclic-graphs-dags",
    "href": "content/07-content.html#part-4-causal-graphs-and-directed-acyclic-graphs-dags",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Part 4: Causal Graphs and Directed Acyclic Graphs (DAGs)",
    "text": "Part 4: Causal Graphs and Directed Acyclic Graphs (DAGs)\nCausal graphs, also known as directed acyclic graphs (DAGs), provide a powerful way to represent causal relationships between variables. In a DAG, nodes represent variables, and directed edges represent causal relationships between them.\nSee this week’s lectures slides where we cover:\n\nDAG basics: How to construct and interpret DAGs, including the concepts of parents, children, ancestors, and descendants.\nD-separation and conditional independence: Understanding when variables are conditionally independent given a set of conditioning variables in a DAG, and how this relates to the concept of d-separation.\nIdentifying confounders: Using DAGs to identify confounders that must be controlled for in order to estimate causal effects.\nCollider bias and conditioning: Understanding how conditioning on colliders can induce bias in causal estimates, and how to avoid this issue.\nMediation and direct/indirect effects: if we are interested in total effects, we should not condition on a mediator."
  },
  {
    "objectID": "slides/07-slides.html#average-treatement-effect-in-observational-studies",
    "href": "slides/07-slides.html#average-treatement-effect-in-observational-studies",
    "title": "Introduction to Causal Inference",
    "section": "Average Treatement Effect in Observational Studies",
    "text": "Average Treatement Effect in Observational Studies\nWhere L is observed:\n\n\\begin{aligned}\nATE = E[Y^{a=1}|L = l] - E[Y^{a=0}|L = l] ~ \\text{for any value}~l\n\\end{aligned}"
  },
  {
    "objectID": "slides/07-slides.html#another-problem-common-causes-of-a-an-y-d-separation",
    "href": "slides/07-slides.html#another-problem-common-causes-of-a-an-y-d-separation",
    "title": "Introduction to Causal Inference",
    "section": "Another problem: common causes of A an Y (D-separation)",
    "text": "Another problem: common causes of A an Y (D-separation)\n\ncg-6"
  },
  {
    "objectID": "slides/07-slides.html#solution-condition-on-common-causes-to-ensure-d-separation",
    "href": "slides/07-slides.html#solution-condition-on-common-causes-to-ensure-d-separation",
    "title": "Introduction to Causal Inference",
    "section": "Solution: condition on common causes to ensure d-separation",
    "text": "Solution: condition on common causes to ensure d-separation\n\ncg-8"
  },
  {
    "objectID": "slides/07-slides.html#another-problem-conditioning-on-a-mediator",
    "href": "slides/07-slides.html#another-problem-conditioning-on-a-mediator",
    "title": "Introduction to Causal Inference",
    "section": "Another problem: conditioning on a mediator",
    "text": "Another problem: conditioning on a mediator\n\ncg-7"
  },
  {
    "objectID": "slides/07-slides.html#collider-bias-solution-do-not-condition-on-a-collider",
    "href": "slides/07-slides.html#collider-bias-solution-do-not-condition-on-a-collider",
    "title": "Introduction to Causal Inference",
    "section": "Collider bias: solution do not condition on a collider",
    "text": "Collider bias: solution do not condition on a collider\n\ncg-13"
  },
  {
    "objectID": "slides/07-slides.html#clear-advice-for-drawing-causal-graphs",
    "href": "slides/07-slides.html#clear-advice-for-drawing-causal-graphs",
    "title": "Introduction to Causal Inference",
    "section": "Clear Advice for Drawing Causal Graphs",
    "text": "Clear Advice for Drawing Causal Graphs\n\nEnsure that causes come before effects. Assign time indices to your variables.\nOrganize your variables chronologically.\nSimplify your graph by removing unnecessary nodes that don’t impact the assessment of bias between an exposure and an outcome.\nKeep in mind that Directed Acyclic Graphs (DAGs) are qualitative tools. They don’t represent non-linear associations or interactions.\nAvoid depicting interactions by crossing arrows.\nRemember that DAGs are distinct from graphs used in Structural Equation Modeling (SEM). Be cautious of SEM literature, as it often overlooks the assumptions needed for causal inference."
  },
  {
    "objectID": "slides/07-slides.html#summary-drawing-causal-graphs-dags",
    "href": "slides/07-slides.html#summary-drawing-causal-graphs-dags",
    "title": "Introduction to Causal Inference",
    "section": "Summary: Drawing Causal Graphs (DAGs)",
    "text": "Summary: Drawing Causal Graphs (DAGs)\n\nDirected Acyclic Graphs (DAGs) help visualize sources of bias.\nThere are five main sources of bias:\n\nTemporal order ambiguity: Uncertainty about whether causes precede effects. Solution: Collect time series data or clarify assumptions when unavailable.\nCommon causes of exposure and outcome: Address this by including common causes in your statistical model (e.g., using regression).\nConditioning on a mediator: Avoid this unless mediation is of interest.\nConditioning on a collider: Refrain from doing this.\nBias induced by conditioning on a confounder’s descendant: Draw your DAG and follow guidelines for points 1-4.\n\nImportant Note 1: In observational studies, it’s impossible to guarantee complete control for confounding. Always conduct sensitivity analyses. Techniques for sensitivity analyses will be discussed next week.\nImportant Note 2: Methods for computing causal effects for group comparisons will be covered in the following week’s lecture."
  },
  {
    "objectID": "content/07-content.html#clear-advice-for-drawing-causal-graphs",
    "href": "content/07-content.html#clear-advice-for-drawing-causal-graphs",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Clear Advice for Drawing Causal Graphs",
    "text": "Clear Advice for Drawing Causal Graphs\n\nEnsure that causes come before effects. Assign time indices to your variables.\nOrganize your variables chronologically.\nSimplify your graph by removing unnecessary nodes that don’t impact the assessment of bias between an exposure and an outcome.\nKeep in mind that Directed Acyclic Graphs (DAGs) are qualitative tools. They don’t represent non-linear associations or interactions.\nAvoid depicting interactions by crossing arrows.\nRemember that DAGs are distinct from graphs used in Structural Equation Modeling (SEM). Be cautious of SEM literature, as it often overlooks the assumptions needed for causal inference."
  },
  {
    "objectID": "content/07-content.html#summary-drawing-causal-graphs-dags",
    "href": "content/07-content.html#summary-drawing-causal-graphs-dags",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Summary: Drawing Causal Graphs (DAGs)",
    "text": "Summary: Drawing Causal Graphs (DAGs)\n\nDirected Acyclic Graphs (DAGs) help visualize sources of bias.\nThere are five main sources of bias:\n\nTemporal order ambiguity: Uncertainty about whether causes precede effects. Solution: Collect time series data or clarify assumptions when unavailable.\nCommon causes of exposure and outcome: Address this by including common causes in your statistical model (e.g., using regression).\nConditioning on a mediator: Avoid this unless mediation is of interest.\nConditioning on a collider: Refrain from doing this.\nBias induced by conditioning on a confounder’s descendant: Draw your DAG and follow guidelines for points 1-4.\n\nImportant Note 1: In observational studies, it’s impossible to guarantee complete control for confounding. Always conduct sensitivity analyses. Techniques for sensitivity analyses will be discussed next week.\nImportant Note 2: Methods for computing causal effects for group comparisons will be covered in the following week’s lecture."
  },
  {
    "objectID": "content/07-content.html#assumptions-required-for-estimating-causal-effects-from-data",
    "href": "content/07-content.html#assumptions-required-for-estimating-causal-effects-from-data",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Assumptions Required for Estimating Causal Effects From Data",
    "text": "Assumptions Required for Estimating Causal Effects From Data\n\nCausal Consistency: The values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.(see: Chatton, Hernan & Robbins)\nPositivity: The probability of receiving every value of the exposure within all strata of co-variates is greater than zero\nExchangeablility: The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates (see: Chatton, Hernan & Robbins)\n\n\nCausal Consistency\nThe fundamental problem of causal inference is that an individual cannot receive two different exposures at the same time. For a binary exposure, an observed outcome under an exposure can be can be expressed:\nY^{observed} = AY^{a=1} + (1-A)Y^{a=0}\nTable Table 1 expresses the relationship between observable outcomes and counterfactual outcomes as a contingency table.\n\n\n\n\nTable 1: Causal Consistency\n\n\nGroup\nExposure_A_1\nNo_Exposure_A_0\n\n\n\n\nY(1)\nObservable\nCounterfactual\n\n\nY(0)\nCounterfactual\nObservable\n\n\n\n\n\n\n\n\nWhen the counterfactual consistency theorem holds, and individuals observed outcome under an exposure is equal to their counterfactual outcome under that exposure:\n\nY_i = Y_i^1 ~\\text{if}~ A_i = 1\n\nand\n\nY_i = Y_i^0 ~\\text{if}~  A_i = 0\n\n\n\nPositivity\nThe probability of receiving every value of the exposure within all strata of co-variates is greater than zero\n\\begin{equation}\n0 &lt; \\Pr(A=a|L)&lt;1, ~ \\forall a \\in A, ~ \\forall a \\in L\n\\end{equation}\nThere are two types of positivity violations - Random non-positivity: the casual effect of aging with observations missing at ages 40-41 (we use parametric models as a work around.) - Deterministic non-positivity: the causal effect of hysterectomy in biological males (assumption violated).\n\n\nConditional exchangeablity\nGiven the observed covariates, the treatment assignment is independent of the potential outcomes. Mathematically, this can be expressed as.\nY^a\\coprod A \\text{ for all }a\nor with strata of confounding covarates:\nY^a\\coprod  A \\text{ for all }a|L\nWhen conditional exchangability holds:\n\n\\begin{aligned}\nATE = E[Y^{a=1}|L = l] - E[Y^{a=0}|L = l] ~ \\text{for any value}~l\n\\end{aligned}\n\n\n“We say that a set L of measured non-descendants of L is a sufficient set for confounding adjustment when conditioning on L blocks all backdoor paths–that is, the treated and the untreated are exchangeable within levels of L” (Hernan & Robins, What IF p. 86)\n\n\n\nPositivity\nThere is a non-zero probability of receiving each treatment level for all strata of the observed covariates. This means that every individual has some chance of being in either treatment or control group."
  },
  {
    "objectID": "slides/07-exercise.html#load-data",
    "href": "slides/07-exercise.html#load-data",
    "title": "Introduction to Causal Inference",
    "section": "load Data",
    "text": "load Data\n\n\nCode\n#|label: source\nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")"
  },
  {
    "objectID": "slides/07-exercise.html#load-data-1",
    "href": "slides/07-exercise.html#load-data-1",
    "title": "Introduction to Causal Inference",
    "section": "Load Data",
    "text": "Load Data\n\n\nCode\n#|label: load-data\n#|eval: false\n######### PART 1: DATA EXCERCISE ##############\n\n# Create a folder called \"data\", in your Rstudio project. Download this file, add it to your the folder called \"data\" in your Rstudio project.\n# \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))"
  },
  {
    "objectID": "slides/07-exercise.html#inspect",
    "href": "slides/07-exercise.html#inspect",
    "title": "Introduction to Causal Inference",
    "section": "Inspect",
    "text": "Inspect\n\n\nCode\n## inspect colnames\ncolnames(nzavs_synth)\n\n## inspect data properties\nstr(nzavs_synth)"
  },
  {
    "objectID": "slides/07-exercise.html#prepare-data",
    "href": "slides/07-exercise.html#prepare-data",
    "title": "Introduction to Causal Inference",
    "section": "Prepare data",
    "text": "Prepare data\n\n\nCode\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\",\n  \"nzsei13\",\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n  \"rural_gch2018\",\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\n\nexposure_var = c(\"perfectionism\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"meaning_purpose\",\n                            \"meaning_sense\")\n\n\n# optional: select exclusion variables (this will not be necessary most of the time)\nexclude_vars = c(\"year_measured\")\n\n\n# the function \"create_wide_data\" should be in your environment. If not, make sure to run the first line of code in this script once more.  You may ignore the warnings.\n\nprep_reflective &lt;-\n  create_wide_data(\n    dat_long = nzavs_synth,\n    #nzavs_synth,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\n\n\n# check. Note that any column that is the exposure or an outcome is added to \"t0_\".  This ensures the strongest possible confounding control, as described by VanderWeele:\n# https://cdn1.sph.harvard.edu/wp-content/uploads/sites/603/2020/09/OutcomeWide_StatisticalScience.pdf\n\ncolnames(prep_reflective)\n\n\n [1] \"t0_edu\"                           \"t0_male\"                         \n [3] \"t0_eth_cat\"                       \"t0_employed\"                     \n [5] \"t0_gen_cohort\"                    \"t0_nz_dep2018\"                   \n [7] \"t0_nzsei13\"                       \"t0_partner\"                      \n [9] \"t0_parent\"                        \"t0_pol_orient\"                   \n[11] \"t0_rural_gch2018\"                 \"t0_agreeableness\"                \n[13] \"t0_conscientiousness\"             \"t0_extraversion\"                 \n[15] \"t0_honesty_humility\"              \"t0_openness\"                     \n[17] \"t0_neuroticism\"                   \"t0_modesty\"                      \n[19] \"t0_religion_identification_level\" \"t0_perfectionism\"                \n[21] \"t0_meaning_purpose\"               \"t0_meaning_sense\"                \n[23] \"t1_perfectionism\"                 \"t2_meaning_purpose\"              \n[25] \"t2_meaning_sense\"                \n\n\nCode\n# if the data is not working, you much run the code below to make the object in an object of the class dataframe.\n# prep_reflective &lt;- as.data.frame(prep_reflective)"
  },
  {
    "objectID": "slides/07-exercise.html#create-composite-scores-for-constructs",
    "href": "slides/07-exercise.html#create-composite-scores-for-constructs",
    "title": "Introduction to Causal Inference",
    "section": "create composite scores for constructs",
    "text": "create composite scores for constructs\n\n\nCode\nlibrary(tidyverse) # should be loaded\n dt_ref &lt;- prep_reflective |&gt;\n  mutate(id = factor(1:nrow(prep_reflective))) |&gt;\n  mutate(t1_perfectionism = round(t1_perfectionism)) |&gt; # we create a three-level exposure to enable clear causal contrasts. We could also use a continous variable.\n  mutate(\n    t1_perfectionism_coarsen = cut(\n      t1_perfectionism,\n      breaks = c(1, 4, 5, 7),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  mutate(\n    t0_eth_cat = as.factor(t0_eth_cat),\n    t0_rural_gch2018 = as.factor(t0_rural_gch2018),\n    t0_gen_cohort = as.factor(t0_gen_cohort)\n  ) |&gt;\n  group_by(id) |&gt;\n  dplyr::mutate(t2_meaning = mean(c(t2_meaning_purpose,\n                                    t2_meaning_sense),\n                                  na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id,\n         where(is.factor),\n         t1_perfectionism, # for comparison\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\"))\n\n\n# inspect\nlevels(dt_ref$t1_perfectionism_coarsen)\n\n\n[1] \"[1,4)\" \"[4,5)\" \"[5,7]\"\n\n\nCode\n# rename levels\ndt_ref$t1_perfectionism_coarsen &lt;-\n  factor(\n    dt_ref$t1_perfectionism_coarsen,\n    levels = c(\"[1,4)\", \"[4,5)\", \"[5,7]\"),\n    labels = c(\"low\", \"medium\", \"high\"),\n    ordered = TRUE\n  )"
  },
  {
    "objectID": "slides/07-exercise.html#save-your-dataframe-for-future-use",
    "href": "slides/07-exercise.html#save-your-dataframe-for-future-use",
    "title": "Introduction to Causal Inference",
    "section": "save your dataframe for future use",
    "text": "save your dataframe for future use\n\n\nCode\n#|eval: false\n# make dataframe\ndt_ref = as.data.frame(dt_ref)\n\n# save data\nsaveRDS(dt_ref, here::here(\"data\", \"dt_ref\"))\n\n# read -- you may start here if you need to repeat the analysis\n\ndt_ref &lt;- readRDS(here::here(\"data\", \"dt_ref\"))"
  },
  {
    "objectID": "slides/07-exercise.html#prepare-models",
    "href": "slides/07-exercise.html#prepare-models",
    "title": "Introduction to Causal Inference",
    "section": "prepare models",
    "text": "prepare models\n\n\nCode\ndt_ref &lt;- readRDS(here::here(\"data\", \"dt_ref\"))\n\n# needs to be data frame\ndt_ref = data.frame(dt_ref)\n\n# simplicity\ndf = dt_ref \n\n# this is the continuous exposure\nX = \"t1_perfectionism_z\"\n\n# this is a categorical exposure\nX_pc &lt;- \"t1_perfectionism_coarsen\"\n\n\n# set our outcome variable:\nY = \"t2_meaning_z\" #note that we have created all numeric values into z-scores.  This will facilitate estimation and also interpretation. The outcome is expressed in standard deviation units\n\n\n# Get baseline names\nbaseline_vars_reflective_cont = dt_ref |&gt;\n  dplyr::select(starts_with(\"t0\")) |&gt; colnames()\n\n# See what we have created:  These are all the \"t0_\" variables.\n# baseline_vars_reflective_cont\n\n\n# to run these models our data need to be a dataframe (not a tibble or another kind of obect)\n# above we've made the data a dataframe, but lets repeat in case you skipped that steip\n\ndt_ref = as.data.frame(dt_ref)\n\n\n# create our formula string, this time for the categorical variable.\nformula_str_X &lt;-\n  paste(Y,\n        \"~\",\n        X ,\n        \"*\",\n        \"(\",\n        paste(baseline_vars_reflective_cont, collapse = \"+\"),\n        \")\")\nformula_str_X\n\n\n[1] \"t2_meaning_z ~ t1_perfectionism_z * ( t0_male+t0_eth_cat+t0_gen_cohort+t0_rural_gch2018+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_perfectionism_z+t0_meaning_purpose_z+t0_meaning_sense_z )\"\n\n\nCode\n## regression based control\n\n\n# fit model\nm1 &lt;- glm(as.formula(formula_str_X),\n          # shortcut\n          #  weights = weights, # will use weights with propensity score models\n          family = \"gaussian\",\n          data = df)\n\n# we can look at the coefficients of this model, but again, it would be a mistake to interpret them\nsummary(m1)\n\n\n\nCall:\nglm(formula = as.formula(formula_str_X), family = \"gaussian\", \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.3374  -0.3647   0.0675   0.4199   3.2766  \n\nCoefficients:\n                                                                Estimate\n(Intercept)                                                   -0.0458559\nt1_perfectionism_z                                            -0.1347193\nt0_maleNot_male                                                0.0632191\nt0_eth_catmāori                                                0.0341978\nt0_eth_catpacific                                             -0.0420690\nt0_eth_catasian                                                0.0224974\nt0_gen_cohort.L                                               -0.0203971\nt0_gen_cohort.Q                                                0.0224781\nt0_gen_cohort.C                                               -0.0005157\nt0_gen_cohort^4                                               -0.0234273\nt0_rural_gch2018medium_urban_accessibility                     0.0277126\nt0_rural_gch2018low_urban_accessibility                        0.0196446\nt0_rural_gch2018remote                                         0.0314218\nt0_rural_gch2018very_remote                                    0.0184033\nt0_edu_z                                                       0.0354573\nt0_employed_z                                                  0.0104852\nt0_nz_dep2018_z                                                0.0038250\nt0_nzsei13_z                                                   0.0186043\nt0_partner_z                                                   0.0186944\nt0_parent_z                                                    0.0304512\nt0_pol_orient_z                                                0.0268793\nt0_agreeableness_z                                             0.0251108\nt0_conscientiousness_z                                         0.0160022\nt0_extraversion_z                                              0.0443726\nt0_honesty_humility_z                                          0.0189023\nt0_openness_z                                                  0.0116959\nt0_neuroticism_z                                              -0.0440279\nt0_modesty_z                                                   0.0008530\nt0_religion_identification_level_z                             0.0601431\nt0_perfectionism_z                                             0.0136652\nt0_meaning_purpose_z                                           0.3689498\nt0_meaning_sense_z                                             0.2410291\nt1_perfectionism_z:t0_maleNot_male                             0.0331692\nt1_perfectionism_z:t0_eth_catmāori                             0.0094140\nt1_perfectionism_z:t0_eth_catpacific                          -0.0679774\nt1_perfectionism_z:t0_eth_catasian                            -0.0852596\nt1_perfectionism_z:t0_gen_cohort.L                            -0.0211112\nt1_perfectionism_z:t0_gen_cohort.Q                             0.1133846\nt1_perfectionism_z:t0_gen_cohort.C                            -0.0321318\nt1_perfectionism_z:t0_gen_cohort^4                             0.0033751\nt1_perfectionism_z:t0_rural_gch2018medium_urban_accessibility -0.0007409\nt1_perfectionism_z:t0_rural_gch2018low_urban_accessibility    -0.0087372\nt1_perfectionism_z:t0_rural_gch2018remote                     -0.0051989\nt1_perfectionism_z:t0_rural_gch2018very_remote                 0.0406504\nt1_perfectionism_z:t0_edu_z                                    0.0180683\nt1_perfectionism_z:t0_employed_z                               0.0110596\nt1_perfectionism_z:t0_nz_dep2018_z                            -0.0062207\nt1_perfectionism_z:t0_nzsei13_z                               -0.0011272\nt1_perfectionism_z:t0_partner_z                                0.0053248\nt1_perfectionism_z:t0_parent_z                                 0.0111114\nt1_perfectionism_z:t0_pol_orient_z                            -0.0003759\nt1_perfectionism_z:t0_agreeableness_z                         -0.0052810\nt1_perfectionism_z:t0_conscientiousness_z                     -0.0003144\nt1_perfectionism_z:t0_extraversion_z                          -0.0041967\nt1_perfectionism_z:t0_honesty_humility_z                       0.0026688\nt1_perfectionism_z:t0_openness_z                               0.0086363\nt1_perfectionism_z:t0_neuroticism_z                           -0.0145938\nt1_perfectionism_z:t0_modesty_z                               -0.0026194\nt1_perfectionism_z:t0_religion_identification_level_z          0.0086293\nt1_perfectionism_z:t0_perfectionism_z                          0.0212526\nt1_perfectionism_z:t0_meaning_purpose_z                        0.0231580\nt1_perfectionism_z:t0_meaning_sense_z                         -0.0017437\n                                                              Std. Error\n(Intercept)                                                    0.0206239\nt1_perfectionism_z                                             0.0217018\nt0_maleNot_male                                                0.0158872\nt0_eth_catmāori                                                0.0257156\nt0_eth_catpacific                                              0.0513832\nt0_eth_catasian                                                0.0401502\nt0_gen_cohort.L                                                0.0537594\nt0_gen_cohort.Q                                                0.0452441\nt0_gen_cohort.C                                                0.0287586\nt0_gen_cohort^4                                                0.0160948\nt0_rural_gch2018medium_urban_accessibility                     0.0185188\nt0_rural_gch2018low_urban_accessibility                        0.0226516\nt0_rural_gch2018remote                                         0.0338997\nt0_rural_gch2018very_remote                                    0.0697321\nt0_edu_z                                                       0.0085782\nt0_employed_z                                                  0.0073289\nt0_nz_dep2018_z                                                0.0072973\nt0_nzsei13_z                                                   0.0083108\nt0_partner_z                                                   0.0075420\nt0_parent_z                                                    0.0082115\nt0_pol_orient_z                                                0.0079719\nt0_agreeableness_z                                             0.0079531\nt0_conscientiousness_z                                         0.0075374\nt0_extraversion_z                                              0.0075663\nt0_honesty_humility_z                                          0.0080126\nt0_openness_z                                                  0.0076666\nt0_neuroticism_z                                               0.0084511\nt0_modesty_z                                                   0.0079318\nt0_religion_identification_level_z                             0.0075053\nt0_perfectionism_z                                             0.0106384\nt0_meaning_purpose_z                                           0.0097171\nt0_meaning_sense_z                                             0.0098484\nt1_perfectionism_z:t0_maleNot_male                             0.0159842\nt1_perfectionism_z:t0_eth_catmāori                             0.0248536\nt1_perfectionism_z:t0_eth_catpacific                           0.0513900\nt1_perfectionism_z:t0_eth_catasian                             0.0395309\nt1_perfectionism_z:t0_gen_cohort.L                             0.0531681\nt1_perfectionism_z:t0_gen_cohort.Q                             0.0443921\nt1_perfectionism_z:t0_gen_cohort.C                             0.0280822\nt1_perfectionism_z:t0_gen_cohort^4                             0.0158307\nt1_perfectionism_z:t0_rural_gch2018medium_urban_accessibility  0.0181669\nt1_perfectionism_z:t0_rural_gch2018low_urban_accessibility     0.0231386\nt1_perfectionism_z:t0_rural_gch2018remote                      0.0363971\nt1_perfectionism_z:t0_rural_gch2018very_remote                 0.0788117\nt1_perfectionism_z:t0_edu_z                                    0.0086251\nt1_perfectionism_z:t0_employed_z                               0.0070229\nt1_perfectionism_z:t0_nz_dep2018_z                             0.0072208\nt1_perfectionism_z:t0_nzsei13_z                                0.0082785\nt1_perfectionism_z:t0_partner_z                                0.0072999\nt1_perfectionism_z:t0_parent_z                                 0.0082109\nt1_perfectionism_z:t0_pol_orient_z                             0.0078204\nt1_perfectionism_z:t0_agreeableness_z                          0.0075244\nt1_perfectionism_z:t0_conscientiousness_z                      0.0071301\nt1_perfectionism_z:t0_extraversion_z                           0.0073804\nt1_perfectionism_z:t0_honesty_humility_z                       0.0078333\nt1_perfectionism_z:t0_openness_z                               0.0074053\nt1_perfectionism_z:t0_neuroticism_z                            0.0082842\nt1_perfectionism_z:t0_modesty_z                                0.0079651\nt1_perfectionism_z:t0_religion_identification_level_z          0.0074253\nt1_perfectionism_z:t0_perfectionism_z                          0.0081526\nt1_perfectionism_z:t0_meaning_purpose_z                        0.0090046\nt1_perfectionism_z:t0_meaning_sense_z                          0.0086903\n                                                              t value Pr(&gt;|t|)\n(Intercept)                                                    -2.223  0.02621\nt1_perfectionism_z                                             -6.208 5.59e-10\nt0_maleNot_male                                                 3.979 6.96e-05\nt0_eth_catmāori                                                 1.330  0.18360\nt0_eth_catpacific                                              -0.819  0.41296\nt0_eth_catasian                                                 0.560  0.57527\nt0_gen_cohort.L                                                -0.379  0.70439\nt0_gen_cohort.Q                                                 0.497  0.61933\nt0_gen_cohort.C                                                -0.018  0.98569\nt0_gen_cohort^4                                                -1.456  0.14554\nt0_rural_gch2018medium_urban_accessibility                      1.496  0.13457\nt0_rural_gch2018low_urban_accessibility                         0.867  0.38583\nt0_rural_gch2018remote                                          0.927  0.35400\nt0_rural_gch2018very_remote                                     0.264  0.79185\nt0_edu_z                                                        4.133 3.60e-05\nt0_employed_z                                                   1.431  0.15256\nt0_nz_dep2018_z                                                 0.524  0.60018\nt0_nzsei13_z                                                    2.239  0.02521\nt0_partner_z                                                    2.479  0.01320\nt0_parent_z                                                     3.708  0.00021\nt0_pol_orient_z                                                 3.372  0.00075\nt0_agreeableness_z                                              3.157  0.00160\nt0_conscientiousness_z                                          2.123  0.03378\nt0_extraversion_z                                               5.865 4.65e-09\nt0_honesty_humility_z                                           2.359  0.01834\nt0_openness_z                                                   1.526  0.12715\nt0_neuroticism_z                                               -5.210 1.93e-07\nt0_modesty_z                                                    0.108  0.91436\nt0_religion_identification_level_z                              8.013 1.24e-15\nt0_perfectionism_z                                              1.285  0.19899\nt0_meaning_purpose_z                                           37.969  &lt; 2e-16\nt0_meaning_sense_z                                             24.474  &lt; 2e-16\nt1_perfectionism_z:t0_maleNot_male                              2.075  0.03800\nt1_perfectionism_z:t0_eth_catmāori                              0.379  0.70486\nt1_perfectionism_z:t0_eth_catpacific                           -1.323  0.18594\nt1_perfectionism_z:t0_eth_catasian                             -2.157  0.03105\nt1_perfectionism_z:t0_gen_cohort.L                             -0.397  0.69133\nt1_perfectionism_z:t0_gen_cohort.Q                              2.554  0.01066\nt1_perfectionism_z:t0_gen_cohort.C                             -1.144  0.25257\nt1_perfectionism_z:t0_gen_cohort^4                              0.213  0.83117\nt1_perfectionism_z:t0_rural_gch2018medium_urban_accessibility  -0.041  0.96747\nt1_perfectionism_z:t0_rural_gch2018low_urban_accessibility     -0.378  0.70573\nt1_perfectionism_z:t0_rural_gch2018remote                      -0.143  0.88642\nt1_perfectionism_z:t0_rural_gch2018very_remote                  0.516  0.60601\nt1_perfectionism_z:t0_edu_z                                     2.095  0.03621\nt1_perfectionism_z:t0_employed_z                                1.575  0.11533\nt1_perfectionism_z:t0_nz_dep2018_z                             -0.861  0.38899\nt1_perfectionism_z:t0_nzsei13_z                                -0.136  0.89169\nt1_perfectionism_z:t0_partner_z                                 0.729  0.46575\nt1_perfectionism_z:t0_parent_z                                  1.353  0.17601\nt1_perfectionism_z:t0_pol_orient_z                             -0.048  0.96167\nt1_perfectionism_z:t0_agreeableness_z                          -0.702  0.48279\nt1_perfectionism_z:t0_conscientiousness_z                      -0.044  0.96482\nt1_perfectionism_z:t0_extraversion_z                           -0.569  0.56963\nt1_perfectionism_z:t0_honesty_humility_z                        0.341  0.73334\nt1_perfectionism_z:t0_openness_z                                1.166  0.24355\nt1_perfectionism_z:t0_neuroticism_z                            -1.762  0.07816\nt1_perfectionism_z:t0_modesty_z                                -0.329  0.74226\nt1_perfectionism_z:t0_religion_identification_level_z           1.162  0.24520\nt1_perfectionism_z:t0_perfectionism_z                           2.607  0.00915\nt1_perfectionism_z:t0_meaning_purpose_z                         2.572  0.01013\nt1_perfectionism_z:t0_meaning_sense_z                          -0.201  0.84098\n                                                                 \n(Intercept)                                                   *  \nt1_perfectionism_z                                            ***\nt0_maleNot_male                                               ***\nt0_eth_catmāori                                                  \nt0_eth_catpacific                                                \nt0_eth_catasian                                                  \nt0_gen_cohort.L                                                  \nt0_gen_cohort.Q                                                  \nt0_gen_cohort.C                                                  \nt0_gen_cohort^4                                                  \nt0_rural_gch2018medium_urban_accessibility                       \nt0_rural_gch2018low_urban_accessibility                          \nt0_rural_gch2018remote                                           \nt0_rural_gch2018very_remote                                      \nt0_edu_z                                                      ***\nt0_employed_z                                                    \nt0_nz_dep2018_z                                                  \nt0_nzsei13_z                                                  *  \nt0_partner_z                                                  *  \nt0_parent_z                                                   ***\nt0_pol_orient_z                                               ***\nt0_agreeableness_z                                            ** \nt0_conscientiousness_z                                        *  \nt0_extraversion_z                                             ***\nt0_honesty_humility_z                                         *  \nt0_openness_z                                                    \nt0_neuroticism_z                                              ***\nt0_modesty_z                                                     \nt0_religion_identification_level_z                            ***\nt0_perfectionism_z                                               \nt0_meaning_purpose_z                                          ***\nt0_meaning_sense_z                                            ***\nt1_perfectionism_z:t0_maleNot_male                            *  \nt1_perfectionism_z:t0_eth_catmāori                               \nt1_perfectionism_z:t0_eth_catpacific                             \nt1_perfectionism_z:t0_eth_catasian                            *  \nt1_perfectionism_z:t0_gen_cohort.L                               \nt1_perfectionism_z:t0_gen_cohort.Q                            *  \nt1_perfectionism_z:t0_gen_cohort.C                               \nt1_perfectionism_z:t0_gen_cohort^4                               \nt1_perfectionism_z:t0_rural_gch2018medium_urban_accessibility    \nt1_perfectionism_z:t0_rural_gch2018low_urban_accessibility       \nt1_perfectionism_z:t0_rural_gch2018remote                        \nt1_perfectionism_z:t0_rural_gch2018very_remote                   \nt1_perfectionism_z:t0_edu_z                                   *  \nt1_perfectionism_z:t0_employed_z                                 \nt1_perfectionism_z:t0_nz_dep2018_z                               \nt1_perfectionism_z:t0_nzsei13_z                                  \nt1_perfectionism_z:t0_partner_z                                  \nt1_perfectionism_z:t0_parent_z                                   \nt1_perfectionism_z:t0_pol_orient_z                               \nt1_perfectionism_z:t0_agreeableness_z                            \nt1_perfectionism_z:t0_conscientiousness_z                        \nt1_perfectionism_z:t0_extraversion_z                             \nt1_perfectionism_z:t0_honesty_humility_z                         \nt1_perfectionism_z:t0_openness_z                                 \nt1_perfectionism_z:t0_neuroticism_z                           .  \nt1_perfectionism_z:t0_modesty_z                                  \nt1_perfectionism_z:t0_religion_identification_level_z            \nt1_perfectionism_z:t0_perfectionism_z                         ** \nt1_perfectionism_z:t0_meaning_purpose_z                       *  \nt1_perfectionism_z:t0_meaning_sense_z                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.479121)\n\n    Null deviance: 9999.0  on 9999  degrees of freedom\nResidual deviance: 4761.5  on 9938  degrees of freedom\nAIC: 21085\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "slides/07-exercise.html#coarsened-variable.",
    "href": "slides/07-exercise.html#coarsened-variable.",
    "title": "Introduction to Causal Inference",
    "section": "Coarsened variable.",
    "text": "Coarsened variable.\n\n\nCode\n# create our formula string:\nformula_str_X_pc &lt;-\n  paste(Y,\n        \"~\",\n        X_pc ,\n        \"*\",\n        \"(\",\n        paste(baseline_vars_reflective_cont, collapse = \"+\"),\n        \")\")\n\nformula_str_X_pc\n\n\n[1] \"t2_meaning_z ~ t1_perfectionism_coarsen * ( t0_male+t0_eth_cat+t0_gen_cohort+t0_rural_gch2018+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_perfectionism_z+t0_meaning_purpose_z+t0_meaning_sense_z )\"\n\n\nCode\n# fit model\nm2 &lt;- glm(as.formula(formula_str_X_pc),\n          # shortcut\n          #  weights = weights, # will use weights with propensity score models\n          family = \"gaussian\",\n          data = df)\n\n# we can look at the coefficients of this model, but again it would be a mistake to interpret them\n\nsummary(m2)\n\n\n\nCall:\nglm(formula = as.formula(formula_str_X_pc), family = \"gaussian\", \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.1848  -0.3670   0.0660   0.4251   3.2601  \n\nCoefficients:\n                                                                        Estimate\n(Intercept)                                                           -0.1226132\nt1_perfectionism_coarsen.L                                            -0.1470404\nt1_perfectionism_coarsen.Q                                             0.0968493\nt0_maleNot_male                                                        0.0953799\nt0_eth_catmāori                                                        0.0330656\nt0_eth_catpacific                                                     -0.0797194\nt0_eth_catasian                                                       -0.0450017\nt0_gen_cohort.L                                                       -0.0638088\nt0_gen_cohort.Q                                                        0.0783060\nt0_gen_cohort.C                                                       -0.0171482\nt0_gen_cohort^4                                                       -0.0209400\nt0_rural_gch2018medium_urban_accessibility                             0.0356611\nt0_rural_gch2018low_urban_accessibility                                0.0112271\nt0_rural_gch2018remote                                                 0.0206451\nt0_rural_gch2018very_remote                                            0.0708426\nt0_edu_z                                                               0.0436332\nt0_employed_z                                                          0.0170529\nt0_nz_dep2018_z                                                       -0.0018689\nt0_nzsei13_z                                                           0.0204286\nt0_partner_z                                                           0.0165503\nt0_parent_z                                                            0.0377576\nt0_pol_orient_z                                                        0.0228034\nt0_agreeableness_z                                                     0.0180559\nt0_conscientiousness_z                                                 0.0192677\nt0_extraversion_z                                                      0.0450764\nt0_honesty_humility_z                                                  0.0222787\nt0_openness_z                                                          0.0159350\nt0_neuroticism_z                                                      -0.0602859\nt0_modesty_z                                                          -0.0035097\nt0_religion_identification_level_z                                     0.0604881\nt0_perfectionism_z                                                     0.0008418\nt0_meaning_purpose_z                                                   0.3868685\nt0_meaning_sense_z                                                     0.2434801\nt1_perfectionism_coarsen.L:t0_maleNot_male                             0.0620947\nt1_perfectionism_coarsen.Q:t0_maleNot_male                            -0.0431561\nt1_perfectionism_coarsen.L:t0_eth_catmāori                             0.0669463\nt1_perfectionism_coarsen.Q:t0_eth_catmāori                             0.1235800\nt1_perfectionism_coarsen.L:t0_eth_catpacific                          -0.0718211\nt1_perfectionism_coarsen.Q:t0_eth_catpacific                           0.0856952\nt1_perfectionism_coarsen.L:t0_eth_catasian                            -0.1551102\nt1_perfectionism_coarsen.Q:t0_eth_catasian                             0.0883442\nt1_perfectionism_coarsen.L:t0_gen_cohort.L                            -0.0306497\nt1_perfectionism_coarsen.Q:t0_gen_cohort.L                             0.1516702\nt1_perfectionism_coarsen.L:t0_gen_cohort.Q                             0.1958547\nt1_perfectionism_coarsen.Q:t0_gen_cohort.Q                             0.0454657\nt1_perfectionism_coarsen.L:t0_gen_cohort.C                            -0.0322455\nt1_perfectionism_coarsen.Q:t0_gen_cohort.C                             0.0440922\nt1_perfectionism_coarsen.L:t0_gen_cohort^4                             0.0173854\nt1_perfectionism_coarsen.Q:t0_gen_cohort^4                             0.0131898\nt1_perfectionism_coarsen.L:t0_rural_gch2018medium_urban_accessibility  0.0090066\nt1_perfectionism_coarsen.Q:t0_rural_gch2018medium_urban_accessibility  0.0005206\nt1_perfectionism_coarsen.L:t0_rural_gch2018low_urban_accessibility    -0.0464747\nt1_perfectionism_coarsen.Q:t0_rural_gch2018low_urban_accessibility    -0.0419324\nt1_perfectionism_coarsen.L:t0_rural_gch2018remote                     -0.0830230\nt1_perfectionism_coarsen.Q:t0_rural_gch2018remote                     -0.1385026\nt1_perfectionism_coarsen.L:t0_rural_gch2018very_remote                 0.0971420\nt1_perfectionism_coarsen.Q:t0_rural_gch2018very_remote                -0.0499774\nt1_perfectionism_coarsen.L:t0_edu_z                                    0.0286493\nt1_perfectionism_coarsen.Q:t0_edu_z                                   -0.0028539\nt1_perfectionism_coarsen.L:t0_employed_z                               0.0252432\nt1_perfectionism_coarsen.Q:t0_employed_z                               0.0091128\nt1_perfectionism_coarsen.L:t0_nz_dep2018_z                            -0.0093324\nt1_perfectionism_coarsen.Q:t0_nz_dep2018_z                             0.0005713\nt1_perfectionism_coarsen.L:t0_nzsei13_z                                0.0009809\nt1_perfectionism_coarsen.Q:t0_nzsei13_z                                0.0032192\nt1_perfectionism_coarsen.L:t0_partner_z                                0.0012847\nt1_perfectionism_coarsen.Q:t0_partner_z                                0.0115548\nt1_perfectionism_coarsen.L:t0_parent_z                                 0.0270801\nt1_perfectionism_coarsen.Q:t0_parent_z                                 0.0090702\nt1_perfectionism_coarsen.L:t0_pol_orient_z                            -0.0006814\nt1_perfectionism_coarsen.Q:t0_pol_orient_z                             0.0172947\nt1_perfectionism_coarsen.L:t0_agreeableness_z                         -0.0189019\nt1_perfectionism_coarsen.Q:t0_agreeableness_z                          0.0024354\nt1_perfectionism_coarsen.L:t0_conscientiousness_z                     -0.0126309\nt1_perfectionism_coarsen.Q:t0_conscientiousness_z                     -0.0194902\nt1_perfectionism_coarsen.L:t0_extraversion_z                          -0.0002204\nt1_perfectionism_coarsen.Q:t0_extraversion_z                           0.0165607\nt1_perfectionism_coarsen.L:t0_honesty_humility_z                      -0.0027931\nt1_perfectionism_coarsen.Q:t0_honesty_humility_z                      -0.0140992\nt1_perfectionism_coarsen.L:t0_openness_z                               0.0065241\nt1_perfectionism_coarsen.Q:t0_openness_z                              -0.0126349\nt1_perfectionism_coarsen.L:t0_neuroticism_z                           -0.0234255\nt1_perfectionism_coarsen.Q:t0_neuroticism_z                            0.0062206\nt1_perfectionism_coarsen.L:t0_modesty_z                               -0.0108579\nt1_perfectionism_coarsen.Q:t0_modesty_z                                0.0147372\nt1_perfectionism_coarsen.L:t0_religion_identification_level_z          0.0227283\nt1_perfectionism_coarsen.Q:t0_religion_identification_level_z          0.0279009\nt1_perfectionism_coarsen.L:t0_perfectionism_z                          0.0320759\nt1_perfectionism_coarsen.Q:t0_perfectionism_z                         -0.0455083\nt1_perfectionism_coarsen.L:t0_meaning_purpose_z                        0.0411158\nt1_perfectionism_coarsen.Q:t0_meaning_purpose_z                        0.0056432\nt1_perfectionism_coarsen.L:t0_meaning_sense_z                         -0.0091829\nt1_perfectionism_coarsen.Q:t0_meaning_sense_z                         -0.0111806\n                                                                      Std. Error\n(Intercept)                                                            0.0254701\nt1_perfectionism_coarsen.L                                             0.0431872\nt1_perfectionism_coarsen.Q                                             0.0450248\nt0_maleNot_male                                                        0.0192675\nt0_eth_catmāori                                                        0.0299149\nt0_eth_catpacific                                                      0.0608525\nt0_eth_catasian                                                        0.0426782\nt0_gen_cohort.L                                                        0.0603945\nt0_gen_cohort.Q                                                        0.0506044\nt0_gen_cohort.C                                                        0.0321417\nt0_gen_cohort^4                                                        0.0184486\nt0_rural_gch2018medium_urban_accessibility                             0.0227806\nt0_rural_gch2018low_urban_accessibility                                0.0280264\nt0_rural_gch2018remote                                                 0.0427166\nt0_rural_gch2018very_remote                                            0.0932044\nt0_edu_z                                                               0.0105185\nt0_employed_z                                                          0.0086131\nt0_nz_dep2018_z                                                        0.0086805\nt0_nzsei13_z                                                           0.0100606\nt0_partner_z                                                           0.0085931\nt0_parent_z                                                            0.0097982\nt0_pol_orient_z                                                        0.0097893\nt0_agreeableness_z                                                     0.0092234\nt0_conscientiousness_z                                                 0.0085661\nt0_extraversion_z                                                      0.0091048\nt0_honesty_humility_z                                                  0.0096794\nt0_openness_z                                                          0.0092394\nt0_neuroticism_z                                                       0.0099422\nt0_modesty_z                                                           0.0094387\nt0_religion_identification_level_z                                     0.0092116\nt0_perfectionism_z                                                     0.0113879\nt0_meaning_purpose_z                                                   0.0105537\nt0_meaning_sense_z                                                     0.0099679\nt1_perfectionism_coarsen.L:t0_maleNot_male                             0.0314355\nt1_perfectionism_coarsen.Q:t0_maleNot_male                             0.0352027\nt1_perfectionism_coarsen.L:t0_eth_catmāori                             0.0484912\nt1_perfectionism_coarsen.Q:t0_eth_catmāori                             0.0549365\nt1_perfectionism_coarsen.L:t0_eth_catpacific                           0.1061646\nt1_perfectionism_coarsen.Q:t0_eth_catpacific                           0.1046290\nt1_perfectionism_coarsen.L:t0_eth_catasian                             0.0698406\nt1_perfectionism_coarsen.Q:t0_eth_catasian                             0.0777872\nt1_perfectionism_coarsen.L:t0_gen_cohort.L                             0.1029227\nt1_perfectionism_coarsen.Q:t0_gen_cohort.L                             0.1062634\nt1_perfectionism_coarsen.L:t0_gen_cohort.Q                             0.0861450\nt1_perfectionism_coarsen.Q:t0_gen_cohort.Q                             0.0891283\nt1_perfectionism_coarsen.L:t0_gen_cohort.C                             0.0545987\nt1_perfectionism_coarsen.Q:t0_gen_cohort.C                             0.0567232\nt1_perfectionism_coarsen.L:t0_gen_cohort^4                             0.0306661\nt1_perfectionism_coarsen.Q:t0_gen_cohort^4                             0.0331920\nt1_perfectionism_coarsen.L:t0_rural_gch2018medium_urban_accessibility  0.0367803\nt1_perfectionism_coarsen.Q:t0_rural_gch2018medium_urban_accessibility  0.0419635\nt1_perfectionism_coarsen.L:t0_rural_gch2018low_urban_accessibility     0.0461386\nt1_perfectionism_coarsen.Q:t0_rural_gch2018low_urban_accessibility     0.0508341\nt1_perfectionism_coarsen.L:t0_rural_gch2018remote                      0.0743191\nt1_perfectionism_coarsen.Q:t0_rural_gch2018remote                      0.0736540\nt1_perfectionism_coarsen.L:t0_rural_gch2018very_remote                 0.1618903\nt1_perfectionism_coarsen.Q:t0_rural_gch2018very_remote                 0.1609781\nt1_perfectionism_coarsen.L:t0_edu_z                                    0.0169322\nt1_perfectionism_coarsen.Q:t0_edu_z                                    0.0194200\nt1_perfectionism_coarsen.L:t0_employed_z                               0.0140222\nt1_perfectionism_coarsen.Q:t0_employed_z                               0.0157635\nt1_perfectionism_coarsen.L:t0_nz_dep2018_z                             0.0140633\nt1_perfectionism_coarsen.Q:t0_nz_dep2018_z                             0.0159476\nt1_perfectionism_coarsen.L:t0_nzsei13_z                                0.0163164\nt1_perfectionism_coarsen.Q:t0_nzsei13_z                                0.0184681\nt1_perfectionism_coarsen.L:t0_partner_z                                0.0140428\nt1_perfectionism_coarsen.Q:t0_partner_z                                0.0156797\nt1_perfectionism_coarsen.L:t0_parent_z                                 0.0160940\nt1_perfectionism_coarsen.Q:t0_parent_z                                 0.0178048\nt1_perfectionism_coarsen.L:t0_pol_orient_z                             0.0158645\nt1_perfectionism_coarsen.Q:t0_pol_orient_z                             0.0179807\nt1_perfectionism_coarsen.L:t0_agreeableness_z                          0.0144998\nt1_perfectionism_coarsen.Q:t0_agreeableness_z                          0.0173259\nt1_perfectionism_coarsen.L:t0_conscientiousness_z                      0.0135816\nt1_perfectionism_coarsen.Q:t0_conscientiousness_z                      0.0159942\nt1_perfectionism_coarsen.L:t0_extraversion_z                           0.0146683\nt1_perfectionism_coarsen.Q:t0_extraversion_z                           0.0167994\nt1_perfectionism_coarsen.L:t0_honesty_humility_z                       0.0155358\nt1_perfectionism_coarsen.Q:t0_honesty_humility_z                       0.0179105\nt1_perfectionism_coarsen.L:t0_openness_z                               0.0147235\nt1_perfectionism_coarsen.Q:t0_openness_z                               0.0171878\nt1_perfectionism_coarsen.L:t0_neuroticism_z                            0.0160470\nt1_perfectionism_coarsen.Q:t0_neuroticism_z                            0.0183189\nt1_perfectionism_coarsen.L:t0_modesty_z                                0.0153804\nt1_perfectionism_coarsen.Q:t0_modesty_z                                0.0172620\nt1_perfectionism_coarsen.L:t0_religion_identification_level_z          0.0149222\nt1_perfectionism_coarsen.Q:t0_religion_identification_level_z          0.0169249\nt1_perfectionism_coarsen.L:t0_perfectionism_z                          0.0178542\nt1_perfectionism_coarsen.Q:t0_perfectionism_z                          0.0214321\nt1_perfectionism_coarsen.L:t0_meaning_purpose_z                        0.0171386\nt1_perfectionism_coarsen.Q:t0_meaning_purpose_z                        0.0193535\nt1_perfectionism_coarsen.L:t0_meaning_sense_z                          0.0158506\nt1_perfectionism_coarsen.Q:t0_meaning_sense_z                          0.0185719\n                                                                      t value\n(Intercept)                                                            -4.814\nt1_perfectionism_coarsen.L                                             -3.405\nt1_perfectionism_coarsen.Q                                              2.151\nt0_maleNot_male                                                         4.950\nt0_eth_catmāori                                                         1.105\nt0_eth_catpacific                                                      -1.310\nt0_eth_catasian                                                        -1.054\nt0_gen_cohort.L                                                        -1.057\nt0_gen_cohort.Q                                                         1.547\nt0_gen_cohort.C                                                        -0.534\nt0_gen_cohort^4                                                        -1.135\nt0_rural_gch2018medium_urban_accessibility                              1.565\nt0_rural_gch2018low_urban_accessibility                                 0.401\nt0_rural_gch2018remote                                                  0.483\nt0_rural_gch2018very_remote                                             0.760\nt0_edu_z                                                                4.148\nt0_employed_z                                                           1.980\nt0_nz_dep2018_z                                                        -0.215\nt0_nzsei13_z                                                            2.031\nt0_partner_z                                                            1.926\nt0_parent_z                                                             3.854\nt0_pol_orient_z                                                         2.329\nt0_agreeableness_z                                                      1.958\nt0_conscientiousness_z                                                  2.249\nt0_extraversion_z                                                       4.951\nt0_honesty_humility_z                                                   2.302\nt0_openness_z                                                           1.725\nt0_neuroticism_z                                                       -6.064\nt0_modesty_z                                                           -0.372\nt0_religion_identification_level_z                                      6.566\nt0_perfectionism_z                                                      0.074\nt0_meaning_purpose_z                                                   36.657\nt0_meaning_sense_z                                                     24.426\nt1_perfectionism_coarsen.L:t0_maleNot_male                              1.975\nt1_perfectionism_coarsen.Q:t0_maleNot_male                             -1.226\nt1_perfectionism_coarsen.L:t0_eth_catmāori                              1.381\nt1_perfectionism_coarsen.Q:t0_eth_catmāori                              2.250\nt1_perfectionism_coarsen.L:t0_eth_catpacific                           -0.677\nt1_perfectionism_coarsen.Q:t0_eth_catpacific                            0.819\nt1_perfectionism_coarsen.L:t0_eth_catasian                             -2.221\nt1_perfectionism_coarsen.Q:t0_eth_catasian                              1.136\nt1_perfectionism_coarsen.L:t0_gen_cohort.L                             -0.298\nt1_perfectionism_coarsen.Q:t0_gen_cohort.L                              1.427\nt1_perfectionism_coarsen.L:t0_gen_cohort.Q                              2.274\nt1_perfectionism_coarsen.Q:t0_gen_cohort.Q                              0.510\nt1_perfectionism_coarsen.L:t0_gen_cohort.C                             -0.591\nt1_perfectionism_coarsen.Q:t0_gen_cohort.C                              0.777\nt1_perfectionism_coarsen.L:t0_gen_cohort^4                              0.567\nt1_perfectionism_coarsen.Q:t0_gen_cohort^4                              0.397\nt1_perfectionism_coarsen.L:t0_rural_gch2018medium_urban_accessibility   0.245\nt1_perfectionism_coarsen.Q:t0_rural_gch2018medium_urban_accessibility   0.012\nt1_perfectionism_coarsen.L:t0_rural_gch2018low_urban_accessibility     -1.007\nt1_perfectionism_coarsen.Q:t0_rural_gch2018low_urban_accessibility     -0.825\nt1_perfectionism_coarsen.L:t0_rural_gch2018remote                      -1.117\nt1_perfectionism_coarsen.Q:t0_rural_gch2018remote                      -1.880\nt1_perfectionism_coarsen.L:t0_rural_gch2018very_remote                  0.600\nt1_perfectionism_coarsen.Q:t0_rural_gch2018very_remote                 -0.310\nt1_perfectionism_coarsen.L:t0_edu_z                                     1.692\nt1_perfectionism_coarsen.Q:t0_edu_z                                    -0.147\nt1_perfectionism_coarsen.L:t0_employed_z                                1.800\nt1_perfectionism_coarsen.Q:t0_employed_z                                0.578\nt1_perfectionism_coarsen.L:t0_nz_dep2018_z                             -0.664\nt1_perfectionism_coarsen.Q:t0_nz_dep2018_z                              0.036\nt1_perfectionism_coarsen.L:t0_nzsei13_z                                 0.060\nt1_perfectionism_coarsen.Q:t0_nzsei13_z                                 0.174\nt1_perfectionism_coarsen.L:t0_partner_z                                 0.091\nt1_perfectionism_coarsen.Q:t0_partner_z                                 0.737\nt1_perfectionism_coarsen.L:t0_parent_z                                  1.683\nt1_perfectionism_coarsen.Q:t0_parent_z                                  0.509\nt1_perfectionism_coarsen.L:t0_pol_orient_z                             -0.043\nt1_perfectionism_coarsen.Q:t0_pol_orient_z                              0.962\nt1_perfectionism_coarsen.L:t0_agreeableness_z                          -1.304\nt1_perfectionism_coarsen.Q:t0_agreeableness_z                           0.141\nt1_perfectionism_coarsen.L:t0_conscientiousness_z                      -0.930\nt1_perfectionism_coarsen.Q:t0_conscientiousness_z                      -1.219\nt1_perfectionism_coarsen.L:t0_extraversion_z                           -0.015\nt1_perfectionism_coarsen.Q:t0_extraversion_z                            0.986\nt1_perfectionism_coarsen.L:t0_honesty_humility_z                       -0.180\nt1_perfectionism_coarsen.Q:t0_honesty_humility_z                       -0.787\nt1_perfectionism_coarsen.L:t0_openness_z                                0.443\nt1_perfectionism_coarsen.Q:t0_openness_z                               -0.735\nt1_perfectionism_coarsen.L:t0_neuroticism_z                            -1.460\nt1_perfectionism_coarsen.Q:t0_neuroticism_z                             0.340\nt1_perfectionism_coarsen.L:t0_modesty_z                                -0.706\nt1_perfectionism_coarsen.Q:t0_modesty_z                                 0.854\nt1_perfectionism_coarsen.L:t0_religion_identification_level_z           1.523\nt1_perfectionism_coarsen.Q:t0_religion_identification_level_z           1.649\nt1_perfectionism_coarsen.L:t0_perfectionism_z                           1.797\nt1_perfectionism_coarsen.Q:t0_perfectionism_z                          -2.123\nt1_perfectionism_coarsen.L:t0_meaning_purpose_z                         2.399\nt1_perfectionism_coarsen.Q:t0_meaning_purpose_z                         0.292\nt1_perfectionism_coarsen.L:t0_meaning_sense_z                          -0.579\nt1_perfectionism_coarsen.Q:t0_meaning_sense_z                          -0.602\n                                                                      Pr(&gt;|t|)\n(Intercept)                                                           1.50e-06\nt1_perfectionism_coarsen.L                                            0.000665\nt1_perfectionism_coarsen.Q                                            0.031498\nt0_maleNot_male                                                       7.53e-07\nt0_eth_catmāori                                                       0.269047\nt0_eth_catpacific                                                     0.190212\nt0_eth_catasian                                                       0.291706\nt0_gen_cohort.L                                                       0.290751\nt0_gen_cohort.Q                                                       0.121795\nt0_gen_cohort.C                                                       0.593687\nt0_gen_cohort^4                                                       0.256384\nt0_rural_gch2018medium_urban_accessibility                            0.117517\nt0_rural_gch2018low_urban_accessibility                               0.688730\nt0_rural_gch2018remote                                                0.628891\nt0_rural_gch2018very_remote                                           0.447226\nt0_edu_z                                                              3.38e-05\nt0_employed_z                                                         0.047744\nt0_nz_dep2018_z                                                       0.829534\nt0_nzsei13_z                                                          0.042327\nt0_partner_z                                                          0.054135\nt0_parent_z                                                           0.000117\nt0_pol_orient_z                                                       0.019857\nt0_agreeableness_z                                                    0.050304\nt0_conscientiousness_z                                                0.024516\nt0_extraversion_z                                                     7.51e-07\nt0_honesty_humility_z                                                 0.021375\nt0_openness_z                                                         0.084617\nt0_neuroticism_z                                                      1.38e-09\nt0_modesty_z                                                          0.710020\nt0_religion_identification_level_z                                    5.41e-11\nt0_perfectionism_z                                                    0.941074\nt0_meaning_purpose_z                                                   &lt; 2e-16\nt0_meaning_sense_z                                                     &lt; 2e-16\nt1_perfectionism_coarsen.L:t0_maleNot_male                            0.048261\nt1_perfectionism_coarsen.Q:t0_maleNot_male                            0.220253\nt1_perfectionism_coarsen.L:t0_eth_catmāori                            0.167437\nt1_perfectionism_coarsen.Q:t0_eth_catmāori                            0.024502\nt1_perfectionism_coarsen.L:t0_eth_catpacific                          0.498734\nt1_perfectionism_coarsen.Q:t0_eth_catpacific                          0.412784\nt1_perfectionism_coarsen.L:t0_eth_catasian                            0.026379\nt1_perfectionism_coarsen.Q:t0_eth_catasian                            0.256103\nt1_perfectionism_coarsen.L:t0_gen_cohort.L                            0.765867\nt1_perfectionism_coarsen.Q:t0_gen_cohort.L                            0.153524\nt1_perfectionism_coarsen.L:t0_gen_cohort.Q                            0.023014\nt1_perfectionism_coarsen.Q:t0_gen_cohort.Q                            0.609982\nt1_perfectionism_coarsen.L:t0_gen_cohort.C                            0.554808\nt1_perfectionism_coarsen.Q:t0_gen_cohort.C                            0.436987\nt1_perfectionism_coarsen.L:t0_gen_cohort^4                            0.570776\nt1_perfectionism_coarsen.Q:t0_gen_cohort^4                            0.691096\nt1_perfectionism_coarsen.L:t0_rural_gch2018medium_urban_accessibility 0.806558\nt1_perfectionism_coarsen.Q:t0_rural_gch2018medium_urban_accessibility 0.990102\nt1_perfectionism_coarsen.L:t0_rural_gch2018low_urban_accessibility    0.313823\nt1_perfectionism_coarsen.Q:t0_rural_gch2018low_urban_accessibility    0.409455\nt1_perfectionism_coarsen.L:t0_rural_gch2018remote                     0.263972\nt1_perfectionism_coarsen.Q:t0_rural_gch2018remote                     0.060076\nt1_perfectionism_coarsen.L:t0_rural_gch2018very_remote                0.548488\nt1_perfectionism_coarsen.Q:t0_rural_gch2018very_remote                0.756217\nt1_perfectionism_coarsen.L:t0_edu_z                                   0.090678\nt1_perfectionism_coarsen.Q:t0_edu_z                                   0.883169\nt1_perfectionism_coarsen.L:t0_employed_z                              0.071854\nt1_perfectionism_coarsen.Q:t0_employed_z                              0.563212\nt1_perfectionism_coarsen.L:t0_nz_dep2018_z                            0.506962\nt1_perfectionism_coarsen.Q:t0_nz_dep2018_z                            0.971423\nt1_perfectionism_coarsen.L:t0_nzsei13_z                               0.952065\nt1_perfectionism_coarsen.Q:t0_nzsei13_z                               0.861624\nt1_perfectionism_coarsen.L:t0_partner_z                               0.927110\nt1_perfectionism_coarsen.Q:t0_partner_z                               0.461182\nt1_perfectionism_coarsen.L:t0_parent_z                                0.092480\nt1_perfectionism_coarsen.Q:t0_parent_z                                0.610469\nt1_perfectionism_coarsen.L:t0_pol_orient_z                            0.965741\nt1_perfectionism_coarsen.Q:t0_pol_orient_z                            0.336148\nt1_perfectionism_coarsen.L:t0_agreeableness_z                         0.192401\nt1_perfectionism_coarsen.Q:t0_agreeableness_z                         0.888218\nt1_perfectionism_coarsen.L:t0_conscientiousness_z                     0.352392\nt1_perfectionism_coarsen.Q:t0_conscientiousness_z                     0.223033\nt1_perfectionism_coarsen.L:t0_extraversion_z                          0.988010\nt1_perfectionism_coarsen.Q:t0_extraversion_z                          0.324261\nt1_perfectionism_coarsen.L:t0_honesty_humility_z                      0.857325\nt1_perfectionism_coarsen.Q:t0_honesty_humility_z                      0.431181\nt1_perfectionism_coarsen.L:t0_openness_z                              0.657697\nt1_perfectionism_coarsen.Q:t0_openness_z                              0.462292\nt1_perfectionism_coarsen.L:t0_neuroticism_z                           0.144377\nt1_perfectionism_coarsen.Q:t0_neuroticism_z                           0.734185\nt1_perfectionism_coarsen.L:t0_modesty_z                               0.480232\nt1_perfectionism_coarsen.Q:t0_modesty_z                               0.393271\nt1_perfectionism_coarsen.L:t0_religion_identification_level_z         0.127762\nt1_perfectionism_coarsen.Q:t0_religion_identification_level_z         0.099281\nt1_perfectionism_coarsen.L:t0_perfectionism_z                         0.072437\nt1_perfectionism_coarsen.Q:t0_perfectionism_z                         0.033747\nt1_perfectionism_coarsen.L:t0_meaning_purpose_z                       0.016457\nt1_perfectionism_coarsen.Q:t0_meaning_purpose_z                       0.770611\nt1_perfectionism_coarsen.L:t0_meaning_sense_z                         0.562371\nt1_perfectionism_coarsen.Q:t0_meaning_sense_z                         0.547176\n                                                                         \n(Intercept)                                                           ***\nt1_perfectionism_coarsen.L                                            ***\nt1_perfectionism_coarsen.Q                                            *  \nt0_maleNot_male                                                       ***\nt0_eth_catmāori                                                          \nt0_eth_catpacific                                                        \nt0_eth_catasian                                                          \nt0_gen_cohort.L                                                          \nt0_gen_cohort.Q                                                          \nt0_gen_cohort.C                                                          \nt0_gen_cohort^4                                                          \nt0_rural_gch2018medium_urban_accessibility                               \nt0_rural_gch2018low_urban_accessibility                                  \nt0_rural_gch2018remote                                                   \nt0_rural_gch2018very_remote                                              \nt0_edu_z                                                              ***\nt0_employed_z                                                         *  \nt0_nz_dep2018_z                                                          \nt0_nzsei13_z                                                          *  \nt0_partner_z                                                          .  \nt0_parent_z                                                           ***\nt0_pol_orient_z                                                       *  \nt0_agreeableness_z                                                    .  \nt0_conscientiousness_z                                                *  \nt0_extraversion_z                                                     ***\nt0_honesty_humility_z                                                 *  \nt0_openness_z                                                         .  \nt0_neuroticism_z                                                      ***\nt0_modesty_z                                                             \nt0_religion_identification_level_z                                    ***\nt0_perfectionism_z                                                       \nt0_meaning_purpose_z                                                  ***\nt0_meaning_sense_z                                                    ***\nt1_perfectionism_coarsen.L:t0_maleNot_male                            *  \nt1_perfectionism_coarsen.Q:t0_maleNot_male                               \nt1_perfectionism_coarsen.L:t0_eth_catmāori                               \nt1_perfectionism_coarsen.Q:t0_eth_catmāori                            *  \nt1_perfectionism_coarsen.L:t0_eth_catpacific                             \nt1_perfectionism_coarsen.Q:t0_eth_catpacific                             \nt1_perfectionism_coarsen.L:t0_eth_catasian                            *  \nt1_perfectionism_coarsen.Q:t0_eth_catasian                               \nt1_perfectionism_coarsen.L:t0_gen_cohort.L                               \nt1_perfectionism_coarsen.Q:t0_gen_cohort.L                               \nt1_perfectionism_coarsen.L:t0_gen_cohort.Q                            *  \nt1_perfectionism_coarsen.Q:t0_gen_cohort.Q                               \nt1_perfectionism_coarsen.L:t0_gen_cohort.C                               \nt1_perfectionism_coarsen.Q:t0_gen_cohort.C                               \nt1_perfectionism_coarsen.L:t0_gen_cohort^4                               \nt1_perfectionism_coarsen.Q:t0_gen_cohort^4                               \nt1_perfectionism_coarsen.L:t0_rural_gch2018medium_urban_accessibility    \nt1_perfectionism_coarsen.Q:t0_rural_gch2018medium_urban_accessibility    \nt1_perfectionism_coarsen.L:t0_rural_gch2018low_urban_accessibility       \nt1_perfectionism_coarsen.Q:t0_rural_gch2018low_urban_accessibility       \nt1_perfectionism_coarsen.L:t0_rural_gch2018remote                        \nt1_perfectionism_coarsen.Q:t0_rural_gch2018remote                     .  \nt1_perfectionism_coarsen.L:t0_rural_gch2018very_remote                   \nt1_perfectionism_coarsen.Q:t0_rural_gch2018very_remote                   \nt1_perfectionism_coarsen.L:t0_edu_z                                   .  \nt1_perfectionism_coarsen.Q:t0_edu_z                                      \nt1_perfectionism_coarsen.L:t0_employed_z                              .  \nt1_perfectionism_coarsen.Q:t0_employed_z                                 \nt1_perfectionism_coarsen.L:t0_nz_dep2018_z                               \nt1_perfectionism_coarsen.Q:t0_nz_dep2018_z                               \nt1_perfectionism_coarsen.L:t0_nzsei13_z                                  \nt1_perfectionism_coarsen.Q:t0_nzsei13_z                                  \nt1_perfectionism_coarsen.L:t0_partner_z                                  \nt1_perfectionism_coarsen.Q:t0_partner_z                                  \nt1_perfectionism_coarsen.L:t0_parent_z                                .  \nt1_perfectionism_coarsen.Q:t0_parent_z                                   \nt1_perfectionism_coarsen.L:t0_pol_orient_z                               \nt1_perfectionism_coarsen.Q:t0_pol_orient_z                               \nt1_perfectionism_coarsen.L:t0_agreeableness_z                            \nt1_perfectionism_coarsen.Q:t0_agreeableness_z                            \nt1_perfectionism_coarsen.L:t0_conscientiousness_z                        \nt1_perfectionism_coarsen.Q:t0_conscientiousness_z                        \nt1_perfectionism_coarsen.L:t0_extraversion_z                             \nt1_perfectionism_coarsen.Q:t0_extraversion_z                             \nt1_perfectionism_coarsen.L:t0_honesty_humility_z                         \nt1_perfectionism_coarsen.Q:t0_honesty_humility_z                         \nt1_perfectionism_coarsen.L:t0_openness_z                                 \nt1_perfectionism_coarsen.Q:t0_openness_z                                 \nt1_perfectionism_coarsen.L:t0_neuroticism_z                              \nt1_perfectionism_coarsen.Q:t0_neuroticism_z                              \nt1_perfectionism_coarsen.L:t0_modesty_z                                  \nt1_perfectionism_coarsen.Q:t0_modesty_z                                  \nt1_perfectionism_coarsen.L:t0_religion_identification_level_z            \nt1_perfectionism_coarsen.Q:t0_religion_identification_level_z         .  \nt1_perfectionism_coarsen.L:t0_perfectionism_z                         .  \nt1_perfectionism_coarsen.Q:t0_perfectionism_z                         *  \nt1_perfectionism_coarsen.L:t0_meaning_purpose_z                       *  \nt1_perfectionism_coarsen.Q:t0_meaning_purpose_z                          \nt1_perfectionism_coarsen.L:t0_meaning_sense_z                            \nt1_perfectionism_coarsen.Q:t0_meaning_sense_z                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.4840355)\n\n    Null deviance: 9999.0  on 9999  degrees of freedom\nResidual deviance: 4795.3  on 9907  degrees of freedom\nAIC: 21217\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "slides/07-exercise.html#simulate-coefficients",
    "href": "slides/07-exercise.html#simulate-coefficients",
    "title": "Introduction to Causal Inference",
    "section": "simulate coefficients",
    "text": "simulate coefficients\n\n\nCode\nlibrary(clarify)\nnsims = 200\nsim_model_r2 &lt;- sim(m2, n = nsims)\n\n\n# set to number of cores on your machine, e.g.\ncores = 4\n\n# simulate effect as modified\nsim_estimand_r2 &lt;- sim_ame(sim_model_r2,\n                           var = X_pc,\n                           cl = cores,\n                           verbose = FALSE)\n\n\n\n\nsummary(sim_estimand_r2)\n\n\n             Estimate   2.5 %  97.5 %\nE[Y(low)]      0.0747  0.0548  0.0916\nE[Y(medium)]  -0.1220 -0.1617 -0.0849\nE[Y(high)]    -0.1872 -0.2440 -0.1236"
  },
  {
    "objectID": "slides/07-exercise.html#suppose-we-want-to-contrast-everyone-being-assigned-to-medium-with-everyone-being-assigned-to-high.",
    "href": "slides/07-exercise.html#suppose-we-want-to-contrast-everyone-being-assigned-to-medium-with-everyone-being-assigned-to-high.",
    "title": "Introduction to Causal Inference",
    "section": "Suppose we want to contrast everyone being assigned to medium with everyone being assigned to high.",
    "text": "Suppose we want to contrast everyone being assigned to medium with everyone being assigned to high.\n\n\nCode\nsim_estimand_r2_focal &lt;-\n  transform(sim_estimand_r2, RD = `E[Y(low)]` - `E[Y(high)]`)\n\n\n# RD describes the causal contrast on the risk difference scale.\nsummary(sim_estimand_r2_focal)\n\n\n             Estimate   2.5 %  97.5 %\nE[Y(low)]      0.0747  0.0548  0.0916\nE[Y(medium)]  -0.1220 -0.1617 -0.0849\nE[Y(high)]    -0.1872 -0.2440 -0.1236\nRD             0.2619  0.1987  0.3161\n\n\nCode\n# The ATE for the effect of moving from low to high is .26 a standard deviation unit in meaning.  What this means is that the expected loss of meaning from perfectionism is about a quarter standard deviation\n\n\n# &gt; summary(sim_estimand_r2)\n# Estimate   2.5 %  97.5 %\n# E[Y(low)]      0.0747  0.0519  0.0950\n# E[Y(medium)]  -0.1220 -0.1534 -0.0788\n# E[Y(high)]    -0.1872 -0.2408 -0.1360\n\n\n# We interpret these contrasts as the expected effects were everyone \"low\" verses everyone assigned \"medium\" versus everyone assigned \"high\""
  },
  {
    "objectID": "slides/07-exercise.html#causal-graphs",
    "href": "slides/07-exercise.html#causal-graphs",
    "title": "Introduction to Causal Inference",
    "section": "CAUSAL GRAPHS",
    "text": "CAUSAL GRAPHS\n\n\nCode\n# also load the ggdag package\nif(!require(ggdag)){\n  # Install the package if it is not already installed\n  install.packages(\"ggdag\")\n}\n# load the ggdag package\nlibrary(ggdag)"
  },
  {
    "objectID": "slides/07-exercise.html#problem-1-bilingualism-on-cognitive-abilities",
    "href": "slides/07-exercise.html#problem-1-bilingualism-on-cognitive-abilities",
    "title": "Introduction to Causal Inference",
    "section": "Problem 1: bilingualism on cognitive abilities",
    "text": "Problem 1: bilingualism on cognitive abilities\n\n\nCode\n## Using the DAG below as a starting point, create a DAG in R using the `ggdag` package that represents the causal relationships among bilingualism (B), cognitive abilities (C), and socioeconomic status (S).\n\n\n\ndag1 &lt;- dagify(C ~ B + S,\n               B ~ S,\n               coords = list(x = c(S = 1, B = 2, C = 3),\n                             y = c(S = 1, B = 2, C = 1)),\n               exposure = \"B\",\n               outcome = \"C\")\n\n# inspect\ntidy_dagitty(dag1)\n\n\n# A DAG with 3 nodes and 3 edges\n#\n# Exposure: B\n# Outcome: C\n#\n# A tibble: 4 × 8\n  name      x     y direction to     xend  yend circular\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt;   \n1 B         2     2 -&gt;        C         3     1 FALSE   \n2 C         3     1 &lt;NA&gt;      &lt;NA&gt;     NA    NA FALSE   \n3 S         1     1 -&gt;        B         2     2 FALSE   \n4 S         1     1 -&gt;        C         3     1 FALSE   \n\n\nCode\ndag1_t &lt;- tidy_dagitty(dag1)\n\n# # plot\n# ggdag(dag1_t)\n# \n# # view\n# ggdag::ggdag_paths(dag1_t)\n# \n# # inspect\n# ggdag_parents(dag1_t, \"B\")\n\n# find adjustment set: adjusting for S is sufficient to control for confounding (on the model's assumptions)\nggdag_adjustment_set(dag1_t)"
  },
  {
    "objectID": "content/07-content.html#excercises",
    "href": "content/07-content.html#excercises",
    "title": "Introduction to Causal Inference: Definitions, Assumptions, Causal Graphs",
    "section": "Excercises",
    "text": "Excercises\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "slides/07-slides.html",
    "href": "slides/07-slides.html",
    "title": "Introduction to Causal Inference",
    "section": "",
    "text": "Understand the definition of “causality” as it is used in the human and health sciences.\nUnderstand the assumptions required for consistently estimating causal effects.\nUnderstand how to use causal diagrammes to assess these assumptions.\n\n\n\n\n\nCause/Effect\nConfounder\nCollider"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "",
    "text": "Below is a link to the R script that will allow you to download the data and exercises. Copy the contents on your screen to a new R script, and run the script from the begging. Before class, it will be useful for you to:\n\nRun “source()” file.\nLast we you should have downloaded the synthetic data, and placed the dataset in a folder in your R project called “data.”\nIf you are stuck with this step, let us know.\n\nlink to script for this week"
  },
  {
    "objectID": "content/08-content.html#downloads",
    "href": "content/08-content.html#downloads",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "",
    "text": "Below is a link to the R script that will allow you to download the data and exercises. Copy the contents on your screen to a new R script, and run the script from the begging. Before class, it will be useful for you to:\n\nRun “source()” file.\nLast we you should have downloaded the synthetic data, and placed the dataset in a folder in your R project called “data.”\nIf you are stuck with this step, let us know.\n\nlink to script for this week"
  },
  {
    "objectID": "content/08-content.html#overview",
    "href": "content/08-content.html#overview",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Overview",
    "text": "Overview\n\nGoals\nBy the end of this lecture, you will understand the following concepts\n\nPropensity-score weighting: modelling the exposure or treatment, not the outcome.\nDoubly robust estimation: model both the treatment and the outcome.\nSubgroup analysis by doubly-robust estimation.\n\n\n\nWhy is this important?\nRecall that in psychology our task is to answer some question about how people think and behave. In cross-cultural psychology these questions are typically comparative.\nOur first task is clearly define that question. Our second task is to answer that question.\nThe methods you will learn today will help you to define and answer comparative questions in psychology."
  },
  {
    "objectID": "content/08-content.html#review-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/08-content.html#review-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Review: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Review: The Fundamental Problem of Causal Inference as a Missing Data Problem\nRecall the fundamental problem of causal inference, returning to the question of whether bilingualism improves cognitive abilities:\n\nY_i^{a = 1}: The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when A = 1.\nY_i^{a = 0}:: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = 0.\n\nThe causal effect of bilingualism on cognitive ability for individual i is then defined as the difference between these potential outcomes:\n\n\\text{Causal Effect}_i = Y^{a=1} - Y^{a=0}\n\nWe say there is a causal effect if:\n\nY^{a=1} - Y^{a=0}  \\neq 0\n\nHowever, we only observe one of the potential outcomes for each child. The other outcome is not observed because physics prevents a child from both receiving and not receiving bilingual exposure.\nThe fact that causal contrasts are not observed on individuals is called “The fundamental problem of causal inference.”\nAlthough we typically cannot observe individual causal effects, under certain assumptions we can obtain average causal effects.\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\nWe may identify average causal effects from the data when the following assumptions are met:\n\nCausal Consistency: The values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.(see: Chatton, Hernan & Robbins)\nPositivity: The probability of receiving every value of the exposure within all strata of co-variates is greater than zero\nExchangeablility: The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates (see: Chatton, Hernan & Robbins)\n\nFurther assumptions:\n\nNo Interference: also known as the Stable Unit Treatment Value Assumption (SUTVA), requires that the treatment given to one unit (e.g., person, group, organization) does not interfere with the potential outcomes of another unit. Put differently, there are no “spillover” effects. Note: this assumption may be thought to be part of causal consistency, namely individual has only one potential outcome under each treatment condition.\nCorrectly specified model: the requirement that the underlying statistical model used to estimate causal effects accurately represents the true relationships between the variables of interest. We say the model should be able to capture “the functional form” of the relationship between the treatment, the outcome, and any covariates. The functional form of the model should be flexible enough to capture the true underlying relationship. If the model’s functional form is incorrect, the estimated causal effects may be biased. Additionally, the model must handle omitted variable bias by including all relevant confounders and should correctly handle missing data. We will return to the bias arising from missing data in the weeks ahead. For now, it is important to note that an assumption of causal inference is that our model is correctly specified."
  },
  {
    "objectID": "content/08-content.html#propensity-scores-and-confounding-control",
    "href": "content/08-content.html#propensity-scores-and-confounding-control",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Propensity Scores and Confounding Control",
    "text": "Propensity Scores and Confounding Control\nRecall that last week, we considered confounding control by regression adjustment.\n\n\\begin{aligned}\nATE = E[Y^{a=1}|L = l] - E[Y^{a=0}|L = l] ~ \\text{for any value}~l\n\\end{aligned}\n\n\n“We say that a set L of measured non-descendants of L is a sufficient set for confounding adjustment when conditioning on L blocks all backdoor paths–that is, the treated and the untreated are exchangeable within levels of L” (Hernan & Robins, What IF p. 86)\n\nThe equation you provided represents the average treatment effect (ATE) when conditioning on a set of covariates L:\n\n\\begin{aligned}\nATE = E[Y^{a=1}|L = l] - E[Y^{a=0}|L = l] ~ \\text{for any value}~l\n\\end{aligned}\n\nThis formula calculates the expected difference in outcomes between treated (a=1) and untreated (a=0) groups, given a specific value of the covariates l.\nInverse probability of treatment weighting (IPTW) is a technique used in causal inference to estimate the average treatment effect (ATE) when comparing two groups in observational studies. The idea is to create a pseudo-population where the treatment assignment is independent of the observed covariates by assigning weights to each individual based on their propensity scores (i.e., the probability of receiving the treatment given their covariates).\nLet’s denote the treatment indicator by A, where A = 1 if an individual receives treatment and A = 0otherwise. Let L represent the vector of observed covariates, and Y^a be the potential outcomes. The propensity score, e(L), is defined as the probability of receiving treatment given the observed covariates:\ne(L) = P(A = 1 \\mid L)\nTo estimate the ATE using IPTW, we first compute the inverse probability of treatment weights, which are defined as:\nW_i = \\frac{A_i}{e(L_i)} + \\frac{1 - A_i}{1 - e(L_i)}\nwhere W_i is the IPTW weight for individual i, A_i is the treatment indicator for individual i, and e(L_i) is the propensity score for individual i.\nNow, the ATE can be estimated as:\nATE_{\\text{iptw}} = \\frac{\\sum [W_i \\cdot (A_i \\cdot Y_i)]}{\\sum [W_i \\cdot A_i]} - \\frac{\\sum [W_i \\cdot ((1 - A_i) \\cdot Y_i)]}{\\sum [W_i \\cdot (1 - A_i)]}\nIn this equation, the numerator of the first term represents the sum of the weighted potential outcomes for the treated group, and the denominator is the sum of the weights for the treated group. The second term is similar but for the control group (those who did not receive the treatment). The ATE_{\\text{iptw}} represents the difference in outcomes between the treated and control groups in the pseudo-population created by applying IPTW."
  },
  {
    "objectID": "content/08-content.html#doubly-robust-estimation",
    "href": "content/08-content.html#doubly-robust-estimation",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nWe can combine regression based estimation and doubly robust estimation. I will walk you through the steps in today’s exercises. The TL;DR is this: doubly robust estimation leads to lower reliance on correct model specification. If either the PS model or the regression model is correctly specified, the model will be unbiased – if the other assumptions of causal inference are met.\nWe cannot know whether these assumptions are met, we will need to do sensitivity analysis, the topic of next week."
  },
  {
    "objectID": "content/08-content.html#subgroup-analysis",
    "href": "content/08-content.html#subgroup-analysis",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nIn causal inference, these two concepts are related but have distinct meanings.\nLet Y_{a} denote the counterfactual outcome Y when the experimental intervention A is set to level a. Let Y_{r} denote the counterfactual outcome Y when another experimental intervention R is set to level r. Following VanderWeele (2009), we can define interaction and effect modification as follows:\n\nInteraction (causal interaction) on the difference scale, conditional on confounders L, occurs when:\n\nE(Y^{a1,r1}|L=l) - E(Y^{a0,r1}|L=l) \\neq E(Y^{a1,r0}|L=l) - E(Y^{a0,r0}|L=l)\nIn this case, we are considering a double intervention, and interaction occurs when the combined effect of interventions A and R is not equal to the sum of their individual effects.\n\nEffect Modification (also known as “heterogeneity of treatment effects”) occurs when the causal effect of intervention A varies across different levels of another variable R:\n\nE(Y^{a=1}|R=r_1, L=l) - E(Y^{a=0}|R=r_1, L=l) \\neq E(Y^{a=1}|R=r_2, L=l) - E(Y^{a=0}|R=r_2, L=l)\nEffect modification indicates that the magnitude of the causal effect of intervention A depends on the level of the modifier variable R. It is important to note that effect modification can be observed even when there is no direct causal interaction between the treatment and the modifier variable.\nIn short, interaction in causal inference refers to a situation where the combined effect of two interventions is not equal to the sum of their individual effects. Effect modification, on the other hand, occurs when the causal effect of one intervention varies across different levels of another variable.\nBy clearly distinguishing between these two concept, researchers can better ask, and answer, questions about human thinking and behaviour. For comparative research, we are typically interested in effect-modification which requires subgroup analysis.\nIn today’s workbook, you will learn how to conduct doubly robust subgroup analysis."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Causal Inference: Propensity Scores & Subgroup Analysis",
    "section": "Readings:",
    "text": "Readings:\nNoah Griefer’s Software and Blogs: https://ngreifer.github.io/blog/subgroup-analysis-psm/"
  },
  {
    "objectID": "slides/07-exercise.html",
    "href": "slides/07-exercise.html",
    "title": "Introduction to Causal Inference",
    "section": "",
    "text": "#|label: source\nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")"
  },
  {
    "objectID": "content/common_graphs.html",
    "href": "content/common_graphs.html",
    "title": "Causal Graphs: A Practical Guide",
    "section": "",
    "text": "Figure 1: Causal graph reveals bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Causal graph reveals bias: solve by stratification\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Causal graph:more general panel design\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Causal graph: three-wave panel design\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Causal graph: three-wave panel design with selection bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Causal graph: three-wave panel design with selection bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Confounder Treatement Feedback"
  },
  {
    "objectID": "content/common_graphs.html#common-causal-graphs",
    "href": "content/common_graphs.html#common-causal-graphs",
    "title": "Causal Graphs: A Practical Guide",
    "section": "",
    "text": "Figure 1: Causal graph reveals bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Causal graph reveals bias: solve by stratification\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Causal graph:more general panel design\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Causal graph: three-wave panel design\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Causal graph: three-wave panel design with selection bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Causal graph: three-wave panel design with selection bias\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Confounder Treatement Feedback"
  },
  {
    "objectID": "content/common_graphs.html#common-cause-of-exposure-and-outcome.",
    "href": "content/common_graphs.html#common-cause-of-exposure-and-outcome.",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Common cause of exposure and outcome.",
    "text": "Common cause of exposure and outcome.\n\n\n\n\n\nFigure 9: Common cause of exposure and outcome: example\n\n\n\n\n\nSolution: Adjust for Confounder\n\n\n\n\n\nFigure 10: Solution to this problem.\n\n\n\n\n\n\nBias: exposure at baseline is a common cause of the exposure at t1 and outcome at t2\n\n\n\n\n\nFigure 11: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\nSolution: adjust for confounder at baseline\n\n\n\n\n\nFigure 12: Solution to this problem\n\n\n\n\n\n\nA more thorough confounding control\n\n\n\n\n\nFigure 13: Causal graph:more general panel design\n\n\n\n\n\n\nGeneric 3-wave panel design (VanderWeeele 2020)\n\n\n\n\n\nFigure 14: Causal graph: three-wave panel design\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelection bias\n\n\n\n\n\nFigure 15: Causal graph: three-wave panel design with selection bias"
  },
  {
    "objectID": "content/common_graphs.html#more-general-3-wave-panel-design",
    "href": "content/common_graphs.html#more-general-3-wave-panel-design",
    "title": "Causal Graphs: A Practical Guide",
    "section": "More general 3-wave panel design",
    "text": "More general 3-wave panel design\n\nA more general solution\n\n\n\n\n\nFigure 7: Causal graph reveals bias from pre-exosure indicator: solve by stratification on this indicator"
  },
  {
    "objectID": "content/common_graphs.html#graphs-of-canonical-confounders",
    "href": "content/common_graphs.html#graphs-of-canonical-confounders",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Graphs of Canonical confounders",
    "text": "Graphs of Canonical confounders\n\nConfounding by Common Cause.\nThe problem of confounding by common cause arises when there is an unmeasured or unaccounted-for variable, denoted as “L,” that influences both the treatment variable, denoted by A, and the outcome variable, denoted as Y. This confounder, L, creates an association between A and Y that is not solely due to the direct causal effect of A on Y. Instead, the observed association between A and Y may be partially or entirely driven by the presence of L, making it difficult to isolate and accurately estimate the true causal effect of A on Y.\n\n\n\n\n\nFigure 1: Counfounding by common cause.\n\n\n\n\n\n\nSolution: adjust for the pre-exposure confounder\nConfounding by common cause can be addressed by adjusting for it. If L is measured before the treatment (or exposure) is assigned, we may adjust for this confounder to account for its influence. Typically we adjust through through statistical techniques such as stratification, regression, matching, or inverse probability of treatment weighting. Sucg adjustment helps to mitigate the bias caused by the confounder, allowing for a more accurate estimation of the true causal relationship.\n\n\n\n\n\nFigure 2: Solution: adjust for pre-exposure confounder.\n\n\n\n\n\n\nSecond problem: Collider Stratification: conditioning on a common effect.\nConditioning on a common effect refers to a situation where there is a variable, L, is affected by both the treatment A and an outcome Y. Conditioning on L creates a spurious association between A and Y, biasing the true causal relationship. This occurs because the relationship between A and Y becomes confounded by the common effect L. The observed association between A and Y may be solely driven by the influence of L.\n\n\n\n\n\nFigure 3: Solution: ensure confounder is measured prior to the exposure.\n\n\n\n\n\n\nSolution: ensure all confounders are measured before the exposure\nTo address the problem of conditioning on a common effect, it is crucial to ensure that every potential confounder L that may affect A is measured before A. If such temporal order is preserved, L cannot be an effect of A, and thus neither of Y. By measuring all relevant confounders in advance, researchers can minimize bias and obtain more reliable estimates of the true causal relationship between A and Y. Note that collider stratification may arise even if L occurs before A, when L does not affect A or Y. This is called M-bias. We describe this case below. Note, however, that if L is not a common cause of A and Y, L should not be included in our model because it is not a source of confounding.\n\n\n\n\n\nFigure 4: Causal graph reveals bias: solve by stratification\n\n\n\n\n\n\nThird problem: conditioning on a mediator\nConditioning on a mediator refers to a situation where L lies on the causal pathway between the treatment A and the outcome Y. Conditioning on L can lead to biased estimates by blocking or distort the true causal pathway between A and Y, obscuring the total effect of A on Y. Where L is a collider between A and an unmeasured confouder U, then including L may increase the strength of association between A and Y. We review this second possibility next. In either case, unless one is interested in mediation analysis (see below), conditioning on a post-treatment variable is always a bad idea.\n\n\n\n\n\nFigure 5: Third problem: conditioning on a mediator.\n\n\n\n\n\n\nSolution: ensure confounders are measured before the exposure.\nTo address the problem of mediator bias, do not condition on a mediator. Ensure that L occurs before A (and Y). Again we discover the importance of an explicit temporal ordering for our variables.\n\n\n\n\n\nFigure 6: Do not condition on a mediator\n\n\n\n\n\n\nFourth problem: conditioning on a descendant\nSay X is a cause of X*. There is a theorem that proves that when we condition on X* we partially condition on X. There are both negative and positive implications for causal estimation. First the negative. Suppose there is a confounder L that is caused by an unobserved variable U, and is affected by the treatment A. Suppose further that U causes the outcome Y. In this scenario, as described in Figure 7, conditioning on L, which is a descendant of A and U, can lead to a spurious association between A and Y through the path A \\to L \\to U \\to Y.\n\n\n\n\n\nFigure 7: Conditioning on a descendant\n\n\n\n\n\n\nSolution: (yet again!) ensure that counfounders are measured before the exposure\nEnsuring the confounder (L) is measured before the exposure (A) has two beneficial properties. Firstly, if L is a confounder, that is, if L is a variable which if we fail to condition on it will bias the association between treatment and outcome, the strategy of including only pre-treatment indicators of L will eliminate collider bias. Secondly, there is the positive side to the theorem that conditioning on a descendent is equivalent to partially conditioning on its parent: if an unmeasured confounder is associated with both A, Y, and L, then adjusting for L helps to reduce confounding caused by the unmeasured confounder. By obtaining measure of L that occur before A, such advantages can be achieved, allowing for more accurate estimation for the causal effect of A on Y. We use the convention of a blue dotted arrow to indicate that the association between A and Y may still be biased, but that bias is reduced by inluding L\n\n\n\n\n\nFigure 8: Solution: conditioning on a descendent is part of the solution, not the problem, when baseline confounders are measured before the exposure.\n\n\n\n\n\n\nThis ends the examples of cannoical casual diagrammes"
  },
  {
    "objectID": "content/common_graphs.html#examples",
    "href": "content/common_graphs.html#examples",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Examples",
    "text": "Examples\nNext some worked examples."
  },
  {
    "objectID": "content/09-content.html#methods",
    "href": "content/09-content.html#methods",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "Methods",
    "text": "Methods\nThere are two key steps to causal estimation:\n\nFormulate a causal question\nAnswer that question\n\nStep 1: Formulate a causal question\nFormulating a causal question involves describing the outcome, exposure, measured confounders, unmeasured confounders, and causal contrasts.\n\nIdentify the outcome(s) of interest: The outcome is traditionally symbolised by the letter Y. For instance, we could take “Distress as gauged by the Kessler-6 scale” as our selected outcome. It’s crucial to specify the units in which the outcome is measured.\n\nConsider:\n\nThe outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).\nIn cases where outcomes are multiple or continuous, transforming the outcome into standard deviation units can be beneficial.\nWe must designate a time period within which the outcome occurs. In three-wave panel designs, this period will typically be one year after the exposure has occurred.\n\n\nDefine the exposure or intervention: Often referred to by the letter A, the exposure is a variable that we hypothesise impacts the outcome. Let’s take A to represent ‘Church attendance’. The exposure, in this instance, remains nebulous until we delineate the unit change in A we are interested in investigating. We may characterise ‘Church Attendance’ as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly).\nIdentify pre-exposure covariates for confounding control: Symbolised as L, these covariates are crucial. For a three-wave panel design, we employ VanderWeele’s modified disjunctive cause criterion: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.\nHighlight unmeasured pre-treatment covariates: These covariates, symbolised by U, can potentially bias the statistical association between A and Y independently of the measured covariates. We recommend creating a causal diagram to illustrate potential sources of bias. Since we can’t ensure the absence of unmeasured confounders in observational settings, it’s vital to conduct sensitivity analyses for the results.\nChoose the scale for a causal contrast: Average causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\n\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is defined as:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE can be expressed as:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT):\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]\nOther concepts include the Population Average Treatment Effect (PATE), which signifies the effect the treatment would have on the entire population if applied universally. It might be beneficial to apply weights to our models to account for discrepancies between the NZAVS national probability sample and the population of New Zealand.\nPATE_{\\text{risk difference}} = E[Y(1) - Y(0)|L]\nPATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nIn real-world applications, these expectations might be estimated via a weighted statistical model where the weights correspond to the inverse probability of being sampled or the survey weights provided with the data. This ensures that the results are generalisable to the wider population.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Let’s denote a stratum of interest as S, then we can compute:\nATE_{S,\\text{risk difference}} = E[Y(1) - Y(0)|S, L]\nATE_{S,\\text{risk ratio}} = \\frac{E[Y(1)|S, L]}{E[Y(0)|S, L]}\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\nDescribe the population(s) for whom the intended study is meant to generalise. The potential outcomes literature in causal inference distinguishes between the concepts of generalisability and transportability.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\\text{Generalizability} = PATE \\approx ATE_{\\text{sample}}\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, \\Delta)\nIn summary, asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?” Following these steps will allow us to emulate a target trial and derive more accurate and meaningful results from our analyses."
  },
  {
    "objectID": "content/09-content.html#how-do-causal-estimation",
    "href": "content/09-content.html#how-do-causal-estimation",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "How do causal estimation",
    "text": "How do causal estimation\nThere are two key steps to causal estimation:\n\nFormulate a causal question\nAnswer that question\n\nStep 1: Formulate a causal question\nFormulating a causal question involves describing the outcome, exposure, measured confounders, unmeasured confounders, and causal contrasts.\n\nIdentify the outcome(s) of interest: The outcome is traditionally symbolised by the letter Y. For instance, we could take “Distress as gauged by the Kessler-6 scale” as our selected outcome. It’s crucial to specify the units in which the outcome is measured.\n\nConsider:\n\nThe outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).\nIn cases where outcomes are multiple or continuous, transforming the outcome into standard deviation units can be beneficial.\nWe must designate a time period within which the outcome occurs. In three-wave panel designs, this period will typically be one year after the exposure has occurred.\n\n\nDefine the exposure or intervention: Often referred to by the letter A, the exposure is a variable that we hypothesise impacts the outcome. Let’s take A to represent ‘Church attendance’. The exposure, in this instance, remains nebulous until we delineate the unit change in A we are interested in investigating. We may characterise ‘Church Attendance’ as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly).\nIdentify pre-exposure covariates for confounding control: Symbolised as L, these covariates are crucial. For a three-wave panel design, we employ VanderWeele’s modified disjunctive cause criterion: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.\nHighlight unmeasured pre-treatment covariates: These covariates, symbolised by U, can potentially bias the statistical association between A and Y independently of the measured covariates. We recommend creating a causal diagram to illustrate potential sources of bias. Since we can’t ensure the absence of unmeasured confounders in observational settings, it’s vital to conduct sensitivity analyses for the results.\nChoose the scale for a causal contrast: Average causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\n\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is defined as:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE can be expressed as:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT):\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]\nOther concepts include the Population Average Treatment Effect (PATE), which signifies the effect the treatment would have on the entire population if applied universally. It might be beneficial to apply weights to our models to account for discrepancies between the NZAVS national probability sample and the population of New Zealand.\nPATE_{\\text{risk difference}} = E[Y(1) - Y(0)|L]\nPATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nIn real-world applications, these expectations might be estimated via a weighted statistical model where the weights correspond to the inverse probability of being sampled or the survey weights provided with the data. This ensures that the results are generalisable to the wider population.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Let’s denote a stratum of interest as S, then we can compute:\nATE_{S,\\text{risk difference}} = E[Y(1) - Y(0)|S, L]\nATE_{S,\\text{risk ratio}} = \\frac{E[Y(1)|S, L]}{E[Y(0)|S, L]}\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\nDescribe the population(s) for whom the intended study is meant to generalise. The potential outcomes literature in causal inference distinguishes between the concepts of generalisability and transportability.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\\text{Generalizability} = PATE \\approx ATE_{\\text{sample}}\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, \\Delta)\nIn summary, asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?” Following these steps will allow us to emulate a target trial and derive more accurate and meaningful results from our analyses."
  },
  {
    "objectID": "content/09-content.html#how-do-we-do-causal-estimation",
    "href": "content/09-content.html#how-do-we-do-causal-estimation",
    "title": "Causal inference: a step by step guide",
    "section": "How do we do causal estimation?",
    "text": "How do we do causal estimation?\nThere are two steps to causal estimation:\n\nState a causal question\nAnswer that question [Cite Hernan]"
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Causal inference: a step by step guide",
    "section": "",
    "text": "link to template for option 2 assignment 3"
  },
  {
    "objectID": "content/09-content.html#overview",
    "href": "content/09-content.html#overview",
    "title": "Causal inference: a step by step guide",
    "section": "Overview",
    "text": "Overview\nRecall that psychology begins with a question. What do I want to know about thought and behaviour? In cross-cultural psychology, these questions relate to differences, and similarities, between groups.\nSuppose we have asked a question. How can we address it using observational data?\nToo fast.\nOur question must be made precise.\nToday we will consider how to make psychological questions precise, and how to answer them, using 3-wave panel designs (VanderWeele, Mathur, and Chen 2020).\nThe order is as follows:\n\nMotivate Three Wave Longitudinal Designs Using Causal Graphs\nChecklist For Causal Estimation in Three Wave Longitudinal Designs\nExplanation of the the Checklist\n\nLet’s dive in!"
  },
  {
    "objectID": "content/09-content.html#step-1-state-a-causal-question",
    "href": "content/09-content.html#step-1-state-a-causal-question",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "Step 1: State a causal question",
    "text": "Step 1: State a causal question\nStating a causal question requires describing: a. outcome(s), b. exposure, c. measured confounders, d. (suspected) unmeasured confounders, e. scale of causal contrasts, f. target population for whom the inferences apply.\nWe consider each of these processes in turn.\n\na. Identify the outcome(s) of interest\nWe use Y to denote an outcome of interest. This is the “effect” of interest.\n\nConsider:\n\nThe outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).\nIt is crucial to specify the units in which the outcome is measured.\nTransforming the outcome into standard deviation units can be beneficial.\nThe outcome must occur after the exposure or treatment.\nWe must designate a time period within which the outcome occurs, e.g, “the one-year effect of a treatment on well-being as measured by Kessler-6.”\nWe may be interested in multiple outcomes. This is the rational behind outcomewide science [cite tyler]\nWe must remember that the outcome might be measured with error, that such errors may be affected by the treatment or correlated with the measurement error of the treatment. (A topic of future seminars, which we will set to the side for now)\n\nHere, imagine we are interested in understanding only one outcome in our study: well-being as measured by the Kessler-6 depression/anxiety scale.\n\n\n\nb. Define the exposure or intervention\nWe use A to denote the the exposure or treatment. This is the variable that we hypothesise might affect the outcome. This variable denotes the cause of interest. We will restrict our focus to consider in which there is only one treatment (i.e. we will not consider complex multi-treatment regimes.)\nHere, imagine we are interested in understanding the causal effect of ‘Church attendance’.\n\nConsider:\n\nTo affect Y, A must occur before Y.\nWhat does A indicate? We may characterise ‘Church Attendance’ as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly). The exposure, in this instance, remains nebulous until we delineate the unit change in A we are interested in investigating.\nFor theoretical and practical purpose we might want to trunctate the variable into categories: (none/some; none/less than weekly, weekly or more)… Why? Because our causal question requires stating the contrast between the states of the world in which we are interested. Additionally, experts often make decisions on the basis of discrete thresholds (has risk of depression/does not).\nWe must remember that an exposure or treatment might be measured with error, that such errors might be correlated with the measurement error of the outcome or affect it. (Again, a topic of future seminars, which we will set to the side for now).\n\n\n\n\nc. Identify pre-exposure covariates for confounding control\nWe use L to denote the set of measured baseline confounders of the the exposure-outcome association. For a three-wave panel design, we employ VanderWeele’s modified disjunctive cause criterion, we advises: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.\n\nConsider:\n\nTo affect Y, L must occur before Y.\n\nTo affect A, L must occur before A.\nAlthough we may gain precision by including an L that affects Y but is unrelated to A, including variables that occur after A will be hazardous if it is possible that A affects L (or its measurement).\nA useful set of default counfounders in NZAVS studies is given in your workbooks.\nNote we have left out the concept of “selection bias.” There are indeed sources of bias that may occure after A. This is another topic for the week ahead.\n\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 1 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar.\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\ne. Choose the scale for a causal contrast\nAverage causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is expressed:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE is expressed:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) :\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\nAnother common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\nwhere f is a function that incorporates weights W into the estimation of the expected outcomes. These weights may correspond to the inverse probability of being sampled or in the case of NZAVS data, the survey weights are given from census estimates for the wider population. Note: I will show you how to use weights in future seminars.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Denote a stratum of interest by G. We may then compute:\nATE_{G,\\text{risk difference}} = E[Y(1) - Y(0)|G, L]\nATE_{G,\\text{risk ratio}} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\n\nConsider:\n\n** In this course, we are interested in stratum specific comparisons **\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs from that of intereaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise.\nThe potential outcomes literature in causal inference distinguishes between the concepts of generalisability and transportability.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\\text{Generalizability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/09-content.html#step-2-answer-a-causal-question",
    "href": "content/09-content.html#step-2-answer-a-causal-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 2: ANSWER A CAUSAL QUESTION",
    "text": "STEP 2: ANSWER A CAUSAL QUESTION\n\nObtain longitudinal data\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\nRecall we have defined.\n\nA: Our exposure or treatment variable, denoted as A. Here we consider the example of ‘Church attendance’.\nY: The outcome variable we are interested in, represented by Y, is psychological distress. We operationalise this variable through the ‘Kessler-6’ distress scale.\nL: The confounding variables, collectively referred to as L, represent factors that can independently influence both A and Y. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\nt \\in T: Let t denote within a multiwave panel study with T measurement intervals.\n\nWhere t/\\text{{exposure}} denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that:\nt_{confounders} &lt; t_{exposure}&lt; t_{outcome}\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of \\mathbf{L} preceding A and the latest values of A before Y.\nNote in Figure 7, We use the prefixes “t0, t1, and t2” to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of A and Y. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.\n\n\nInclude the measured exposure with baseline covariates\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the “incidence exposure” rather than the “prevalence exposure” (Danaei, Tavakkoli, and Hernán 2012). It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to explain away the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\n\nState the eligibility criteria for participation\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\nConsider:\n\nGeneralisability: we cannot evaluate inferences to a target group from the source population if we do not describe the source population\nEligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country’s electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\nAside: discuss per protocol effects/ intention to treat effects\n\n\n\n\nDetermine how missing data will be handled\n\nAs we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n\n\nState a statistical model\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n\n\nReporting\nConsider the following ideas about how to report one’s model:\n\nEstimator: Doubly robust where possible.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations.\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process.\nMethod Variations: Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nContinuous Exposures: Highlight that for continuous exposures, only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation.\nSubgroup Interaction: Address whether the subgroup was included separately as an interaction in the outcome model, and if the model successfully converged.\nModel Coefficients: Note that the model coefficients should not be interpreted, as they are not meaningful in this context.\nConfidence Intervals and Standard Errors: Describe the methods used to derive confidence intervals and standard errors, noting the use of the ‘clarify’ package in R for simulation based inference.\n\n\n\nExample of how to report a doubly robust method in your report\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\nInference\nConsider the following ideas about what to discuss in one’s findings:\n\nInterpretation of Propensity Scores: Understand and articulate the meaning of the propensity scores, and how they relate to the exposure of interest in the study.\nAnalysis of Outcome Models: Analyze the outcome models to understand the relationships between the outcome, exposure, confounders, and subgroups.\nHypothetical Scenarios: Reason through the hypothetical scenarios used in the simulation of potential outcomes, and discuss their relevance to the research question.\nEffect of Intervention Levels: Consider the implications of different intervention levels on potential outcomes, and how they inform the average causal effect within each subgroup.\nCausal Effect Interpretation: Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Analyze and discuss the differences in causal effects between different subgroups, and how these differences contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nPositivity Violations: Reflect on any violations of the positivity assumption. Ensure that for each combination of covariates, there is a positive probability of receiving each level of treatment.\nCausal Consistency and Treatment Heterogeneity: Consider the assumption of causal consistency and the potential for treatment heterogeneity. Discuss whether the same version of treatment was consistently applied and if different versions of treatment might lead to different outcomes.\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. Understand what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study.\nConsiderations of Transportability: Discuss how the causal effects could be transported or applied to other settings, populations, or contexts, and what factors might affect this transportability.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/test.html",
    "href": "content/test.html",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "",
    "text": "Recall that psychology begings with a question about cognition and behavior. What do we want to know? Before all else, we must ask, and motivate this question.\nSuppose we have defined a question. How can we address it using observational data?\nThis is the topic of today’s siminar."
  },
  {
    "objectID": "content/test.html#overview",
    "href": "content/test.html#overview",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "",
    "text": "Recall that psychology begings with a question about cognition and behavior. What do we want to know? Before all else, we must ask, and motivate this question.\nSuppose we have defined a question. How can we address it using observational data?\nThis is the topic of today’s siminar."
  },
  {
    "objectID": "content/test.html#how-do-we-do-causal-estimation",
    "href": "content/test.html#how-do-we-do-causal-estimation",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "How do we do causal estimation?",
    "text": "How do we do causal estimation?\nThere are two steps to causal estimation:\n\nState a causal question\nAnswer that question [Cite Hernan]"
  },
  {
    "objectID": "content/test.html#step-1-state-a-causal-question",
    "href": "content/test.html#step-1-state-a-causal-question",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "Step 1: State a causal question",
    "text": "Step 1: State a causal question\nStating a causal question requires describing: a. outcome(s), b. exposure, c. measured confounders, d. (suspected) unmeasured confounders, e. scale of causal contrasts, f. target population for whom the inferences apply.\nWe consider each of these processes in turn."
  },
  {
    "objectID": "content/09-content.html#step-1-ask-a-causal-question",
    "href": "content/09-content.html#step-1-ask-a-causal-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 1: ASK A CAUSAL QUESTION",
    "text": "STEP 1: ASK A CAUSAL QUESTION\nStating a causal question requires describing: a. outcome(s), b. exposure, c. measured confounders, d. (suspected) unmeasured confounders, e. scale of causal contrasts, f. target population for whom the inferences apply.\nWe consider each of these processes in turn.\n\na. Identify the outcome(s) of interest\nWe use Y to denote an outcome of interest. This is the “effect” of interest.\n\nConsider:\n\nThe outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).\nIt is crucial to specify the units in which the outcome is measured.\nTransforming the outcome into standard deviation units can be beneficial.\nThe outcome must occur after the exposure or treatment.\nWe must designate a time period within which the outcome occurs, e.g, “the one-year effect of a treatment on well-being as measured by Kessler-6.”\nWe may be interested in multiple outcomes. This is the rational behind outcomewide science [cite tyler]\nWe must remember that the outcome might be measured with error, that such errors may be affected by the treatment or correlated with the measurement error of the treatment. (A topic of future seminars, which we will set to the side for now)\n\nHere, imagine we are interested in understanding only one outcome in our study: well-being as measured by the Kessler-6 depression/anxiety scale.\n\n\n\nb. Define the exposure or intervention\nWe use A to denote the the exposure or treatment. This is the variable that we hypothesise might affect the outcome. This variable denotes the cause of interest. We will restrict our focus to consider in which there is only one treatment (i.e. we will not consider complex multi-treatment regimes.)\nHere, imagine we are interested in understanding the causal effect of ‘Church attendance’.\n\nConsider:\n\nTo affect Y, A must occur before Y.\nWhat does A indicate? We may characterise ‘Church Attendance’ as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly). The exposure, in this instance, remains nebulous until we delineate the unit change in A we are interested in investigating.\nFor theoretical and practical purpose we might want to trunctate the variable into categories: (none/some; none/less than weekly, weekly or more)… Why? Because our causal question requires stating the contrast between the states of the world in which we are interested. Additionally, experts often make decisions on the basis of discrete thresholds (has risk of depression/does not).\nWe must remember that an exposure or treatment might be measured with error, that such errors might be correlated with the measurement error of the outcome or affect it. (Again, a topic of future seminars, which we will set to the side for now).\nIt must be conceptually possible to intervene within all stratums of baseline confounders (strong positivity).\nIt must be conceptually possible to understand an intervention on the exposure (consistency)\nThe multiple version of the intervention must exchangeable within levels of measured baseline confounders (consistency).\n\n\n\n\nc. Identify pre-exposure covariates for confounding control\nWe use L to denote the set of measured baseline confounders of the the exposure-outcome association. For a three-wave panel design, we employ VanderWeele’s modified disjunctive cause criterion, we advises: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.\n\nConsider:\n\nTo affect Y, L must occur before Y.\n\nTo affect A, L must occur before A.\nWe must acheive balence within levels of the exposure such that A\\coprod Y| L\nAlthough we may gain precision by including an L that affects Y but is unrelated to A, including variables that occur after A will be hazardous if it is possible that A affects L (or its measurement).\nA useful set of default counfounders in NZAVS studies is given in your workbooks.\nNote we have left out the concept of “selection bias.” There are indeed sources of bias that may occure after A. This is another topic for the week ahead.\n\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 1 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar.\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\ne. Choose the scale for a causal contrast\nAverage causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is expressed:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE is expressed:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) :\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\nAnother common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\nwhere f is a function that incorporates weights W into the estimation of the expected outcomes. These weights may correspond to the inverse probability of being sampled or in the case of NZAVS data, the survey weights are given from census estimates for the wider population. Note: I will show you how to use weights in future seminars.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Denote a stratum of interest by G. We may then compute:\nATE_{G,\\text{risk difference}} = E[Y(1) - Y(0)|G, L]\nATE_{G,\\text{risk ratio}} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\n\nConsider:\n\n** In this course, we are interested in stratum specific comparisons **\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs from that of intereaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\nConsider the following concepts:\n\nSource population: A source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\nTarget population: The target population is the larger group we aim to apply our study’s results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study.",
    "href": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study.",
    "title": "Causal inference: a step by step guide",
    "section": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study.",
    "text": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study.\n\nFormulating the Research Question\n\nStating the Question: Is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nCausality of the Question: Is my question causal? If not, refine your question.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nUnderstanding the Framework: Can I explain the potential outcomes framework, individual causal effects, the experimental method to obtain average causal effects, the fundamental assumptions of causal inference, and the estimation of causal effects in observational data? If not, review course materials.\n\n\nData Requirements\n\nType of Data: Is my data non-experimental? If not, your project may not fit this course.\nTime-Series Data: Is my data time-series? If not, reconsider your causal question.\nData Waves: Do I have at least three waves of data? If not, beware of confounding control issues.\nData Source: Is my data from the NZAVS simulated data set? If not, consult with me.\n\n\n\nDefining the Outcome\n\nOutcome Variable: Is the outcome variable Y defined? If not, define it.\nMultiple Outcomes: Are there multiple outcomes? If yes, write them down.\nOutcome Relevance: Can I explain how the outcome variable/s relate to my question? If not, clarify.\nOutcome Type: Is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorizing it (consult an expert).\nOutcome Timing: Does the outcome appear after the exposure? It should.\n\n\n\nDetermining the Exposure\n\nExposure Variable: Is the exposure variable A defined? If not, define it.\nMultiple Exposures: Are there multiple exposures? If yes, proceed; if not, reassess.\nExposure Relevance: Can I explain how the exposure variable relates to my question? If not, clarify.\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\nExposure Type: Is the exposure binary or continuous? If continuous, z-transform it or consider categorizing it (consult an expert).\nExposure Timing: Does the exposure appear before the outcome? It should.\n\n\n\nAccounting for Confounders\n\nBaseline Confounders: Have I defined my baseline confounders L? I should have.\nJustification: Can I explain how the baseline confounders could affect both A and Y? I should be able to.\nTiming: Are the baseline confounders measured before the exposure? They should be.\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, plan a sensitivity analysis.\nConfounder Type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n\n\nDrawing a Causal Diagram with Unmeasured Confounders\n\nUnmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\nCausal Diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\nM-Bias: Have I considered the possibility of M-Bias? If not familiar, we’ll discuss later.\nMeasurement Error: Have I described potential biases from measurement errors? If not, we’ll discuss later.\nTemporal Order: Does my DAG have time indicators to ensure correct temporal order? It should.\nTime Consistency: Is my DAG organized so that time follows in a consistent direction? It should.\n\n\n\nIdentifying the Estimand\n\nCausal Estimand: Is my causal estimand one of the following:\n\nATE_{G,(A,A')} = E[Y(1) - Y(0)|G, L]\nATE_{G,(A/A')} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\nIf yes, you’re on the right track.\n\n\nUnderstanding Source and Target Populations\n\nPopulations Identified: Have I differentiated between my source and target populations? I should have.\nGeneralisability and Transportability: Have I considered whether my results generalise to the source population and transport to a different population? I should have.\n\n\n\nSetting Eligibility Criteria\n\nCriteria Stated: Have I stated the eligibility criteria for the study? I should have.\n\n\n\nDescribing Sample Characteristics\n\nDescriptive Statistics: Have I provided descriptive statistics for demographic information taken at baseline? I should have.\nExposure Change: Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\nReferences: Have I included references for more information about the sample? I should have.\n\n\n\nAddressing Missing Data\n\nMissing Data Check: Have I checked for missing data? I should have.\nMissing Data Plan: If there is missing data, have I described how I will address it? I should have.\n\n\n\nSelecting the Model Approach\n\nApproach Decision: Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\nInteraction Inclusion: Have I included the interaction of the exposure and baseline covariates? I should have.\nLarge Data Set: If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\nModel Specification: Have I double-checked the model specification? I should.\nOutcome Specifics: If the outcome is rare and binary, have I specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity Analysis: Am I planning a sensitivity analysis using simulation? If yes, describe it."
  },
  {
    "objectID": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2",
    "href": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2",
    "title": "Causal inference: a step by step guide",
    "section": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study (E.g. Assessment 3 option 2)",
    "text": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study (E.g. Assessment 3 option 2)"
  },
  {
    "objectID": "content/09-content.html#causal-inference-a-step-by-step-guide.",
    "href": "content/09-content.html#causal-inference-a-step-by-step-guide.",
    "title": "Causal inference: a step by step guide",
    "section": "Causal inference: a Step by Step Guide.",
    "text": "Causal inference: a Step by Step Guide.\nWe can break down causal estimation into two steps:\n\nPose a causal question.\nAnswer it (Hernán, Wang, and Leaf 2022)."
  },
  {
    "objectID": "content/09-content.html#appendix",
    "href": "content/09-content.html#appendix",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix",
    "text": "Appendix\n\nG-computation for Subgroup Analysis Estimator\nStep 1 Estimate the outcome model. Fit a model for the outcome Y, conditional on the exposure A, the covariates L, and subgroup indicator G. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.\n \\hat{E}(Y|A,L,G) = f_Y(A,L,G; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, and subgroup G, as modeled by the function f_Y with parameters \\theta_Y. This formulation allows for the prediction of the average outcome Y given certain values of A, L, and G.\nStep 2 Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention A=a using the estimated outcome model:\n\\hat{E}(a|(G=g)  = \\hat{E}[Y|A=a,L,G; \\hat{\\theta}_Y]\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to A=a':\n\\hat{E}(a'|G=g) = E[Y|A=a',L,G; \\hat{\\theta}_Y]\nIn these equations, Y represents the potential outcome, A is the intervention, L are the covariates, G=g represents the subgroup, and \\theta_Y are the parameters of the outcome model.\nStep 3 Calculate the estimated difference for each subgroup g:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThis difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup g.\nWe use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\nStep 4 Compare differences in causal effects by subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nThis difference \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g'. It measures the interaction effect of the exposure A and the subgroup G on the outcome Y.\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nInverse Probability of Treatment Weighting (IPTW) for Subgroup Analysis Estimator\nStep 1 Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nThus, v depends on A, and is calculated as the inverse of the propensity score for exposed individuals and as the inverse of 1-e for unexposed individuals.\nNote that we estimate propensity scores separately within strata of the subgroup for whom we are interested in effect modification. v is the weight for each individual in a given subgroup G.\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and subgroup G. This can be represented as:\n \\hat{E}(Y|A, G; V) = f_Y(A, G ; \\theta_Y, V) \nIn this model, f_Y is a function (such as a weighted regression model) with parameters θ_Y. The weights V are incorporated into the estimation process, affecting how much each observation contributes to the estimation of θ_Y, but they are not themselves an additional variable within the model.\nNote that in this formulation, unlike doubly-robust estimation, L is not included as a variable in the outcome model, but its effect is accounted for through the weights V, which are calculated based on the estimated propensity scores that do include L.\nStep 3 Simulate potential outcomes. For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(a|(G=g) = \\hat{E}[Y_i|A=a,G=g; \\hat{\\theta}_Y, v_i]\nand also under the hypothetical scenario where everyone is exposed to intervention A=a':\n\\hat{E}(a'|G=g) = \\hat{E}[Y_i|A=a',G=g; \\hat{\\theta}_Y, v_i]\nThus the expectation is calculated for each individual i in each subgroup g, with individual-specific weights v_i.\nStep 4 Estimate the average causal effect for each subgroup as the difference in the predicted outcomes:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect withing group g. We use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nDoubly Robust Estimation for Subgroup Analysis Estimator\nDoubly Robust Estimation is a powerful technique that combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\nStep 1 Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nThus, v depends on A, and is calculated as the inverse of the propensity score for exposed individuals and as the inverse of 1-e for unexposed individuals.\nNote that we estimate propensity scores separately within strata of the subgroup for whom we are interested in effect modification. v is the weight for each individual in a given subgroup G.\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and subgroup G. This can be represented as:\n \\hat{E}(Y|A, L, G; V) = f_Y(A, L, G ; \\theta_Y, V) \nIn this model, f_Y is a function (such as a weighted regression model) with parameters θ_Y. The weights V are incorporated into the estimation process, affecting how much each observation contributes to the estimation of θ_Y, but they are not themselves an additional variable within the model.\nNote that in this formulation, unlike propensity score model, estimation, L is included as a variable in the outcome model, Although imbalance in L on A is accounted for by the weights V, by including L in the outcome model this method is doubly robust insofar as we will obtain a consistent effect estimate if either the propensity score or the outcome model is correctly specified. This property reduces the strength of the assumption of correct model specification.\nStep 3 For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(a|G=g) = \\hat{E}[Y_i|A=a,G=g; L,\\hat{\\theta}_Y, v_i]\nand also under the hypothetical scenario where everyone in each subgroup is exposed to intervention A=a':\n\\hat{E}(a'|G=g) = \\hat{E}[Y_i|A=a',G=g; L; \\hat{\\theta}_Y, v_i]\nThus the expectation is calculated for each individual i in each subgroup g, with individual-specific weights v_i.\nStep 4 Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe difference \\delta_g represents the average causal effect of changing the exposure from level a^{\\prime} to level a within each subgroup.\nWe use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#step-1-pose-a-causal-question",
    "href": "content/09-content.html#step-1-pose-a-causal-question",
    "title": "Causal inference: a step by step guide",
    "section": "Step 1: Pose a Causal Question**",
    "text": "Step 1: Pose a Causal Question**\nThis involves defining: a) outcome(s), b) exposure, c) measured confounders, d) suspected unmeasured confounders, e) scale of causal contrasts, and f) target population for the inferences.\n\na. Define the Outcome(s)\nWe symbolise the outcome of interest as Y, the ‘effect’.\nConsiderations:\n\nOutcomes can be binary (severely distressed/not severely distressed), continuous (average of sum of indicators), or rate variables (sum of indicators).\nSpecify the outcome units.\nStandardizing the outcome can be helpful.\nOutcome must follow exposure.\nDesignate a time frame for the outcome, e.g., “one-year effect of treatment on well-being as measured by Kessler-6.”\nMultiple outcomes are possible, aligning with outcome-wide science.\nBe aware of possible measurement errors in outcomes. For now, imagine we’re studying one outcome: well-being as per the Kessler-6 depression/anxiety scale.\n\n\n\nb. Define the Exposure or Intervention\nThe exposure or treatment, which we believe may affect the outcome, is denoted by A.\nConsiderations:\n\nA should precede Y.\nHow is A defined? ‘Church Attendance’ might be binary (attend/not attend) or continuous (attend weekly/attend monthly).\nTruncating the variable into categories could be beneficial for stating causal contrasts and for practicality.\nMeasurement errors in exposure are possible.\nIntervention should be conceptually possible across all baseline confounder strata (strong positivity).\nIt should be feasible to understand the exposure intervention (consistency).\nDifferent intervention versions should be exchangeable within measured baseline confounder levels (consistency).\n\n\n\nc. Identify Pre-Exposure Covariates for Confounding Control\nWe use L to denote measured baseline confounders of the exposure-outcome association. For a three-wave panel design, we apply VanderWeele’s modified disjunctive cause criterion.\nConsiderations:\n\nL should precede both Y and A.\nWe should achieve balance within exposure levels such that A\\coprod Y| L\nIncluding variables that occur after A can be risky if A possibly affects L.\nA handy set of default confounders for NZAVS studies is provided in your workbooks.\nNote: We’re omitting ‘selection bias’ for now. Some biases may occur after A, but that’s a topic for another time.”\n\n\n\nd. Identify Unmeasured Pre-Treatment Covariates\nLet U represent unmeasured pre-treatment covariates that could bias the A and Y association, apart from measured covariates.\nConsiderations:\n\nU must precede A to affect Y and A.\nCausal diagrams help visualize potential bias sources.\nCausal diagrams require expertise and can’t usually be derived from data.\nDiagrams should include only necessary information to assess confounding.\nSensitivity analyses are crucial in observational settings due to potential unmeasured confounders.\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design.\n\n\n\n\n\n\ne. Select Causal Contrast Scale\nAverage causal effects are inferred by contrasting expected outcomes at different exposure levels: E[Y(A = a)] and E[Y(A=a')].\nBinary treatment A=0 and A=1 Average Treatment Effect (ATE) is expressed as:\nRisk difference scale: ATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nRisk ratio scale: ATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther scales (incidence rate ratio, incidence rate difference, or hazard ratio) may also apply. Average Treatment Effect on the Treated (ATT) and Population Average Treatment Effect (PATE) can be defined as:\nATT:\n\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\n\nPATE:\n\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\n\nHere, f is a function incorporating weights W into outcome estimation. These weights may be inverse sampling probability or, in NZAVS data, census-based survey weights for the general population.\nWe may also compute effects specific to strata, denoted by G:\n\nATE_{G,\\text{risk difference}} = E[Y(1) - Y(0)|G, L]\nATE_{G,\\text{risk ratio}} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\n\nNote that we often explore stratum-specific comparisons using “effect modification.”\n\n\nf. Define Source and Target Populations\n\nSource population: Where we collect data. It should resemble the broader group to ensure valid and generalizable conclusions.\nTarget population: The group we aim to apply our study results to, defined by location, demographics, or specific conditions.\n\nThe generalizability or external validity refers to the application of causal effects from a sample to the broader population:\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\nTransportability refers to extrapolating causal effects from a source population to a target population when certain conditions are met:\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nHere, f is a function and T is a function mapping the results from our source population to another population.\nSummary Step 1: A lot goes into asking a causal question!\nAsking a causal question involves defining the outcome, exposure, and covariates, choosing the right scale for causal contrast, managing confounding, and possibly adjusting for sample weights or stratification. You also have to consider the population to which the results apply. Only then can you ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/common_graphs.html#important-causal-diagrammes",
    "href": "content/common_graphs.html#important-causal-diagrammes",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Important Causal Diagrammes",
    "text": "Important Causal Diagrammes"
  },
  {
    "objectID": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "href": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "title": "Causal inference: a step by step guide",
    "section": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.",
    "text": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.\nREVIEW: Causal Diagrammes (DAGS) are a remarkably powerful and simple tool for understanding confounding https://go-bayes.github.io/psych-434-2023/content/common_graphs.html\n\nCommon cause of exposure and outcome.\nOur question: does visiting a clinical psychologist reduce the 10 year incidence of heart attacks?\n\n\n\n\n\nFigure 1: Common cause of exposure and outcome: example\n\n\n\n\n\n\nSolution: Adjust for Confounder\n\n\n\n\n\nFigure 2: Solution to this problem.\n\n\n\n\n\n\nBias: exposure at baseline is a common cause of the exposure at t1 and outcome at t2\n\n\n\n\n\nFigure 3: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\nSolution: adjust for confounder at baseline\n\n\n\n\n\nFigure 4: Solution to this problem\n\n\n\n\n\n\nA more thorough confounding control\n\n\n\n\n\nFigure 5: Causal graph:more general panel design\n\n\n\n\n\n\nGeneric 3-wave panel design (VanderWeeele 2020)\n\n\n\n\n\nFigure 6: Causal graph: three-wave panel design"
  },
  {
    "objectID": "content/common_graphs.html#how-do-we-draw-interactions",
    "href": "content/common_graphs.html#how-do-we-draw-interactions",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "How do we draw interactions?",
    "text": "How do we draw interactions?"
  },
  {
    "objectID": "content/common_graphs.html#common-cause-of-exposure-and-outcome.-1",
    "href": "content/common_graphs.html#common-cause-of-exposure-and-outcome.-1",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Common cause of exposure and outcome.",
    "text": "Common cause of exposure and outcome.\n\n\n\n\n\nFigure 16: A simple graph for effect-modification.\n\n\n\n\n\nAnother graph for interaction\n\n\n\n\n\nFigure 17: A simple graph for effect-modification.\n\n\n\n\n\n\nM-Bias\n\n\n\n\n\nFigure 18: M-bias: confounding control by including previous measures of the outcome\n\n\n\n\n\n\nWhat if mediation is of interest?\nConsider the assumptions required for mediation analysis:\n\nNo unmeasured exposure-outcome confounders given L\n\nY^{am}\\coprod A|L 2. No unmeasured meadiator-outcome confounders given L\nY^{am}\\coprod M|L\n\nNo unmeasured exposure-mediator confounders given L\n\nM^{a}\\coprod A|L\n\nNo mediator-outcome confounder affected by the exposure (no red arrow)\n\nY^{am}\\coprod M^{a*}|L\n\n\n\n\n\nFigure 19: Assumptions for mediation analysis\n\n\n\n\n\n\nConfounder-Treatment Feedback\n\n\n\n\n\nFigure 20: Confounder Treatement Feedback"
  },
  {
    "objectID": "content/common_graphs.html#to-do-still-measurement-error",
    "href": "content/common_graphs.html#to-do-still-measurement-error",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "TO DO STILL: Measurement Error",
    "text": "TO DO STILL: Measurement Error"
  },
  {
    "objectID": "content/09-content.html#link-to-workbook-for-this-week",
    "href": "content/09-content.html#link-to-workbook-for-this-week",
    "title": "Causal inference: a step by step guide",
    "section": "",
    "text": "link to template for option 2 assignment 3"
  },
  {
    "objectID": "content/09-content.html#step-1-formulate-the-research-question",
    "href": "content/09-content.html#step-1-formulate-the-research-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 1 Formulate the Research Question",
    "text": "STEP 1 Formulate the Research Question\n\nStating the Question: Is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nCausality of the Question: Is my question causal? If not, refine your question.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nUnderstanding the Framework: Can I explain the potential outcomes framework, individual causal effects, the experimental method to obtain average causal effects, the fundamental assumptions of causal inference, and the estimation of causal effects in observational data? If not, review course materials.\n\n\nData Requirements\n\nType of Data: Are my data experimental? If yes, your project may not fit this course.\nTime-Series Data: Are my data time-series? If not, reconsider your causal question.\nData Waves: Do I have at least three waves of data? If not, beware of confounding control issues.\nData Source: Are my data from the NZAVS simulated data set? If not, consult with me.\n\n\n\nDefining the Outcome\n\nOutcome Variable: Is the outcome variable Y defined? If not, define it.\nMultiple Outcomes: Are there multiple outcomes? If yes, write them down.\nOutcome Relevance: Can I explain how the outcome variable/s relate to my question? If not, clarify.\nOutcome Type: Is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorising it (consult an expert).\nOutcome Timing: Does the outcome appear after the exposure? It should.\n\n\n\nDetermining the Exposure\n\nExposure Variable: Is the exposure variable A defined? If not, define it.\nMultiple Exposures: Are there multiple exposures? If yes, reassess; if only one exposure, proceed.\nExposure Relevance: Can I explain how the exposure variable relates to my question? If not, clarify.\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\nExposure Type: Is the exposure binary or continuous? If continuous, z-transform it or consider categorising it (consult an expert).\nExposure Timing: Does the exposure appear before the outcome? It should.\n\n\n\nAccounting for Confounders\n\nBaseline Confounders: Have I defined my baseline confounders L? I should have.\nJustification: Can I explain how the baseline confounders could affect both A and Y? I should be able to.\nTiming: Are the baseline confounders measured before the exposure? They should be.\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, plan a sensitivity analysis.\nConfounder Type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n\n\nDrawing a Causal Diagram with Unmeasured Confounders\n\nUnmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\nCausal Diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\nM-Bias: Have I considered the possibility of M-Bias? If not familiar, we’ll discuss later.\nMeasurement Error: Have I described potential biases from measurement errors? If not, we’ll discuss later.\nTemporal Order: Does my DAG have time indicators to ensure correct temporal order? It should.\nTime Consistency: Is my DAG organized so that time follows in a consistent direction? It should.\n\n\n\nIdentifying the Estimand\n\nCausal Estimand: Is my causal estimand one of the following:\n\nATE_{G,(A,A')} = E[Y(1) - Y(0)|G, L]\nATE_{G,(A/A')} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\nIf yes, you’re on the right track.\n\n\nUnderstanding Source and Target Populations\n\nPopulations Identified: Have I differentiated between my source and target populations? I should have.\nGeneralisability and Transportability: Have I considered whether my results generalise to the source population and transport to a different population? I should have.\n\n\n\nSetting Eligibility Criteria\n\nCriteria Stated: Have I stated the eligibility criteria for the study? I should have.\n\n\n\nDescribing Sample Characteristics\n\nDescriptive Statistics: Have I provided descriptive statistics for demographic information taken at baseline? I should have.\nExposure Change: Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\nReferences: Have I included references for more information about the sample? I should have.\n\n\n\nAddressing Missing Data\n\nMissing Data Check: Have I checked for missing data? I should have.\nMissing Data Plan: If there is missing data, have I described how I will address it? I should have.\n\n\n\nSelecting the Model Approach\n\nApproach Decision: Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\nInteraction Inclusion: Have I included the interaction of the exposure and baseline covariates? I should have.\nLarge Data Set: If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\nModel Specification: Have I double-checked the model specification? I should.\nOutcome Specifics: If the outcome is rare and binary, have I specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity Analysis: Am I planning a sensitivity analysis using simulation? If yes, describe it (e.g. E-values.)\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 7 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar.\n\n\n\n\n\n\nFigure 7: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\ne. Choose the scale for a causal contrast\nAverage causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is expressed:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE is expressed:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) :\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\nAnother common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\nwhere f is a function that incorporates weights W into the estimation of the expected outcomes. These weights may correspond to the inverse probability of being sampled or in the case of NZAVS data, the survey weights are given from census estimates for the wider population. Note: I will show you how to use weights in future seminars.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Denote a stratum of interest by G. We may then compute:\nATE_{G,\\text{risk difference}} = E[Y(1) - Y(0)|G, L]\nATE_{G,\\text{risk ratio}} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\n\nConsider:\n\nIn this course, we are interested in stratum specific comparisons\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs slightly from that of interaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\nConsider the following concepts:\n\nSource population: A source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\nTarget population: The target population is the larger group we aim to apply our study’s results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "scripts/experiment_template.html",
    "href": "scripts/experiment_template.html",
    "title": "experiment_template.qmd",
    "section": "",
    "text": "Today we will do an analysis, proceeding in a step-by-step way.\nRecall our checklist:"
  },
  {
    "objectID": "scripts/experiment_template.html#descriptive-table",
    "href": "scripts/experiment_template.html#descriptive-table",
    "title": "experiment_template.qmd",
    "section": "Descriptive table",
    "text": "Descriptive table\nI created a simple function\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n\n\n\n|                                     |baseline              |\n|:------------------------------------|:---------------------|\n|                                     |(N=10000)             |\n|EDU                                  |                      |\n|Mean (SD)                            |5.85 (2.59)           |\n|Median [Min, Max]                    |6.96 [-0.128, 10.1]   |\n|MALE                                 |                      |\n|Male                                 |3905 (39.1%)          |\n|Not_male                             |6095 (61.0%)          |\n|ETH_CAT                              |                      |\n|euro                                 |8641 (86.4%)          |\n|māori                                |821 (8.2%)            |\n|pacific                              |190 (1.9%)            |\n|asian                                |348 (3.5%)            |\n|EMPLOYED                             |                      |\n|Mean (SD)                            |0.836 (0.370)         |\n|Median [Min, Max]                    |1.00 [0, 1.00]        |\n|GEN_COHORT                           |                      |\n|Gen_Silent: born&lt; 1946               |166 (1.7%)            |\n|Gen Boomers: born &gt;= 1946 & b.&lt; 1965 |4257 (42.6%)          |\n|GenX: born &gt;=1961 & b.&lt; 1981         |3493 (34.9%)          |\n|GenY: born &gt;=1981 & b.&lt; 1996         |1883 (18.8%)          |\n|GenZ: born &gt;= 1996                   |201 (2.0%)            |\n|NZ_DEP2018                           |                      |\n|Mean (SD)                            |4.46 (2.65)           |\n|Median [Min, Max]                    |4.01 [0.835, 10.1]    |\n|NZSEI13                              |                      |\n|Mean (SD)                            |57.0 (16.1)           |\n|Median [Min, Max]                    |61.0 [9.91, 90.1]     |\n|PARTNER                              |                      |\n|Mean (SD)                            |0.795 (0.404)         |\n|Median [Min, Max]                    |1.00 [0, 1.00]        |\n|PARENT                               |                      |\n|Mean (SD)                            |0.706 (0.456)         |\n|Median [Min, Max]                    |1.00 [0, 1.00]        |\n|POL_ORIENT                           |                      |\n|Mean (SD)                            |3.47 (1.40)           |\n|Median [Min, Max]                    |3.09 [0.862, 7.14]    |\n|URBAN                                |                      |\n|rural                                |1738 (17.4%)          |\n|urban                                |8262 (82.6%)          |\n|AGREEABLENESS                        |                      |\n|Mean (SD)                            |5.36 (0.986)          |\n|Median [Min, Max]                    |5.48 [0.977, 7.13]    |\n|CONSCIENTIOUSNESS                    |                      |\n|Mean (SD)                            |5.19 (1.03)           |\n|Median [Min, Max]                    |5.28 [0.938, 7.16]    |\n|EXTRAVERSION                         |                      |\n|Mean (SD)                            |3.85 (1.21)           |\n|Median [Min, Max]                    |3.80 [0.861, 7.07]    |\n|HONESTY_HUMILITY                     |                      |\n|Mean (SD)                            |5.52 (1.12)           |\n|Median [Min, Max]                    |5.71 [1.14, 7.15]     |\n|OPENNESS                             |                      |\n|Mean (SD)                            |5.06 (1.10)           |\n|Median [Min, Max]                    |5.12 [0.899, 7.15]    |\n|NEUROTICISM                          |                      |\n|Mean (SD)                            |3.41 (1.17)           |\n|Median [Min, Max]                    |3.31 [0.860, 7.08]    |\n|MODESTY                              |                      |\n|Mean (SD)                            |6.07 (0.860)          |\n|Median [Min, Max]                    |6.24 [2.17, 7.17]     |\n|RELIGION_IDENTIFICATION_LEVEL        |                      |\n|Mean (SD)                            |2.19 (2.07)           |\n|Median [Min, Max]                    |1.00 [1.00, 7.00]     |\n|HOURS_EXERCISE_COARSEN               |                      |\n|inactive                             |3805 (38.1%)          |\n|active                               |4342 (43.4%)          |\n|very_active                          |1853 (18.5%)          |\n|KESSLER_LATENT_ANXIETY               |                      |\n|Mean (SD)                            |1.16 (0.719)          |\n|Median [Min, Max]                    |1.03 [-0.0800, 4.03]  |\n|KESSLER_LATENT_DEPRESSION            |                      |\n|Mean (SD)                            |0.744 (0.686)         |\n|Median [Min, Max]                    |0.646 [-0.0871, 4.02] |\n\n\nHowever, if would like like a nicer table, try this:\n\n\nCode\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all(~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\n\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n\nCode\n# another method for making a table\n# x &lt;- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |&gt;\n#   kable_material(c(\"striped\", \"hover\"))\n\n\nSome more data wrangling.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n\nCode\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n# view object\nskimr::skim(dt)\n\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndt\n\n\n\n\nNumber of rows\n\n\n9462\n\n\n\n\nNumber of columns\n\n\n26\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n7\n\n\n\n\nnumeric\n\n\n19\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nid\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n9462\n\n\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\n\nt0_male\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nNot: 5767, Mal: 3695\n\n\n\n\nt0_eth_cat\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\neur: 8641, māo: 821\n\n\n\n\nt0_gen_cohort\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n5\n\n\nGen: 4107, Gen: 3311, Gen: 1716, Gen: 164\n\n\n\n\nt0_urban\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nurb: 7762, rur: 1700\n\n\n\n\nt0_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4131, ina: 3557, ver: 1774\n\n\n\n\nt1_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4281, ina: 3187, ver: 1994\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nt0_edu_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.29\n\n\n-1.05\n\n\n0.44\n\n\n0.82\n\n\n1.66\n\n\n▂▃▃▇▂\n\n\n\n\nt0_employed_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.26\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n▂▁▁▁▇\n\n\n\n\nt0_nz_dep2018_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.36\n\n\n-0.92\n\n\n-0.16\n\n\n0.63\n\n\n2.17\n\n\n▇▆▆▅▂\n\n\n\n\nt0_nzsei13_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.94\n\n\n-0.75\n\n\n0.25\n\n\n0.81\n\n\n2.07\n\n\n▁▃▅▇▁\n\n\n\n\nt0_partner_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.99\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n▂▁▁▁▇\n\n\n\n\nt0_parent_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.58\n\n\n-1.58\n\n\n0.63\n\n\n0.63\n\n\n0.63\n\n\n▃▁▁▁▇\n\n\n\n\nt0_pol_orient_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.87\n\n\n-1.02\n\n\n-0.28\n\n\n0.44\n\n\n2.62\n\n\n▇▆▇▅▂\n\n\n\n\nt0_agreeableness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.46\n\n\n-0.62\n\n\n0.12\n\n\n0.68\n\n\n1.79\n\n\n▁▁▃▇▆\n\n\n\n\nt0_conscientiousness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.13\n\n\n-0.65\n\n\n0.08\n\n\n0.76\n\n\n1.91\n\n\n▁▁▅▇▅\n\n\n\n\nt0_extraversion_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.48\n\n\n-0.71\n\n\n-0.04\n\n\n0.72\n\n\n2.67\n\n\n▂▆▇▅▁\n\n\n\n\nt0_honesty_humility_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.95\n\n\n-0.69\n\n\n0.17\n\n\n0.84\n\n\n1.45\n\n\n▁▁▃▆▇\n\n\n\n\nt0_openness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.76\n\n\n-0.71\n\n\n0.05\n\n\n0.81\n\n\n1.90\n\n\n▁▂▆▇▅\n\n\n\n\nt0_neuroticism_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.18\n\n\n-0.76\n\n\n-0.09\n\n\n0.71\n\n\n3.14\n\n\n▃▇▇▃▁\n\n\n\n\nt0_modesty_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.67\n\n\n-0.66\n\n\n0.19\n\n\n0.83\n\n\n1.26\n\n\n▁▁▂▅▇\n\n\n\n\nt0_religion_identification_level_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-0.56\n\n\n-0.56\n\n\n-0.56\n\n\n-0.08\n\n\n2.37\n\n\n▇▁▁▁▂\n\n\n\n\nt0_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.72\n\n\n-0.69\n\n\n-0.19\n\n\n0.70\n\n\n4.01\n\n\n▇▇▆▁▁\n\n\n\n\nt0_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.21\n\n\n-0.63\n\n\n-0.13\n\n\n0.42\n\n\n4.83\n\n\n▇▂▂▁▁\n\n\n\n\nt2_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.23\n\n\n-0.65\n\n\n-0.16\n\n\n0.39\n\n\n4.75\n\n\n▇▃▂▁▁\n\n\n\n\nt2_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.74\n\n\n-0.70\n\n\n-0.20\n\n\n0.68\n\n\n3.97\n\n\n▇▇▆▁▁\n\n\n\n\n\n\n\n\n\nCode\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))"
  },
  {
    "objectID": "scripts/experiment_template.html#propensity-scores",
    "href": "scripts/experiment_template.html#propensity-scores",
    "title": "experiment_template.qmd",
    "section": "Propensity scores",
    "text": "Propensity scores\nNext we generate propensity scores. Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\nThe first step is to obtain the baseline variables. note that we must remove “t0_eth_cat” because we are performing separate weighting for each stratum within this variable.\n\n\nCode\n# read -- you may start here if you need to repeat the analysis\n\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n\n\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use “ps” (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n\nCode\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\n\nResults, first for Europeans\n\n\nCode\ndt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\ndt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\nbal.tab(dt_match_energy$euro)   #  good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0004\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0002\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0009\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0006\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0011\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0003\nt0_urban_urban                                      Binary       0.0003\nt0_hours_exercise_coarsen_inactive                  Binary       0.0064\nt0_hours_exercise_coarsen_active                    Binary       0.0007\nt0_hours_exercise_coarsen_very_active               Binary       0.0057\nt0_edu_z                                           Contin.       0.0026\nt0_employed_z                                      Contin.       0.0011\nt0_nz_dep2018_z                                    Contin.       0.0032\nt0_nzsei13_z                                       Contin.       0.0026\nt0_partner_z                                       Contin.       0.0018\nt0_parent_z                                        Contin.       0.0019\nt0_pol_orient_z                                    Contin.       0.0030\nt0_agreeableness_z                                 Contin.       0.0018\nt0_conscientiousness_z                             Contin.       0.0020\nt0_extraversion_z                                  Contin.       0.0014\nt0_honesty_humility_z                              Contin.       0.0021\nt0_openness_z                                      Contin.       0.0003\nt0_neuroticism_z                                   Contin.       0.0019\nt0_modesty_z                                       Contin.       0.0004\nt0_religion_identification_level_z                 Contin.       0.0033\nt0_kessler_latent_anxiety_z                        Contin.       0.0006\nt0_kessler_latent_depression_z                     Contin.       0.0022\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted     1852 4464.       2325.  \nAdjusted        975 3534.71     1349.22\n\n\nCode\nbal.tab(dt_match_ebal$euro)   #  best\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n\nCode\nbal.tab(dt_match_ps$euro)   #  not as good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0664\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0038\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0305\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0236\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0107\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0025\nt0_urban_urban                                      Binary       0.0447\nt0_hours_exercise_coarsen_inactive                  Binary       0.0832\nt0_hours_exercise_coarsen_active                    Binary       0.1000\nt0_hours_exercise_coarsen_very_active               Binary       0.0728\nt0_edu_z                                           Contin.       0.1782\nt0_employed_z                                      Contin.       0.0636\nt0_nz_dep2018_z                                    Contin.       0.1321\nt0_nzsei13_z                                       Contin.       0.1387\nt0_partner_z                                       Contin.       0.0698\nt0_parent_z                                        Contin.       0.0186\nt0_pol_orient_z                                    Contin.       0.0644\nt0_agreeableness_z                                 Contin.       0.1456\nt0_conscientiousness_z                             Contin.       0.0769\nt0_extraversion_z                                  Contin.       0.0607\nt0_honesty_humility_z                              Contin.       0.0181\nt0_openness_z                                      Contin.       0.0488\nt0_neuroticism_z                                   Contin.       0.0338\nt0_modesty_z                                       Contin.       0.0137\nt0_religion_identification_level_z                 Contin.       0.0757\nt0_kessler_latent_anxiety_z                        Contin.       0.0327\nt0_kessler_latent_depression_z                     Contin.       0.0768\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1585.48 3760.58      949.56\n\n\nResults for Maori\n\n\nCode\nbal.tab(dt_match_energy$māori)   #  good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0048\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0035\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0160\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0027\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0087\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0067\nt0_urban_urban                                      Binary       0.0080\nt0_hours_exercise_coarsen_inactive                  Binary       0.0337\nt0_hours_exercise_coarsen_active                    Binary       0.0034\nt0_hours_exercise_coarsen_very_active               Binary       0.0341\nt0_edu_z                                           Contin.       0.0197\nt0_employed_z                                      Contin.       0.0217\nt0_nz_dep2018_z                                    Contin.       0.0271\nt0_nzsei13_z                                       Contin.       0.0209\nt0_partner_z                                       Contin.       0.0108\nt0_parent_z                                        Contin.       0.0230\nt0_pol_orient_z                                    Contin.       0.0238\nt0_agreeableness_z                                 Contin.       0.0237\nt0_conscientiousness_z                             Contin.       0.0470\nt0_extraversion_z                                  Contin.       0.0172\nt0_honesty_humility_z                              Contin.       0.0050\nt0_openness_z                                      Contin.       0.0188\nt0_neuroticism_z                                   Contin.       0.0064\nt0_modesty_z                                       Contin.       0.0014\nt0_religion_identification_level_z                 Contin.       0.0247\nt0_kessler_latent_anxiety_z                        Contin.       0.0169\nt0_kessler_latent_depression_z                     Contin.       0.0048\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   191.    431.       199.  \nAdjusted     134.06  351.3      126.38\n\n\nCode\nbal.tab(dt_match_ebal$māori)   #  best\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n\nCode\nbal.tab(dt_match_ps$māori)   #  not good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0839\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0048\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0692\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0203\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0548\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0131\nt0_urban_urban                                      Binary       0.0790\nt0_hours_exercise_coarsen_inactive                  Binary       0.0810\nt0_hours_exercise_coarsen_active                    Binary       0.1137\nt0_hours_exercise_coarsen_very_active               Binary       0.1082\nt0_edu_z                                           Contin.       0.1130\nt0_employed_z                                      Contin.       0.0998\nt0_nz_dep2018_z                                    Contin.       0.1613\nt0_nzsei13_z                                       Contin.       0.1672\nt0_partner_z                                       Contin.       0.1720\nt0_parent_z                                        Contin.       0.1720\nt0_pol_orient_z                                    Contin.       0.1194\nt0_agreeableness_z                                 Contin.       0.1196\nt0_conscientiousness_z                             Contin.       0.0692\nt0_extraversion_z                                  Contin.       0.1158\nt0_honesty_humility_z                              Contin.       0.0994\nt0_openness_z                                      Contin.       0.2098\nt0_neuroticism_z                                   Contin.       0.0706\nt0_modesty_z                                       Contin.       0.1336\nt0_religion_identification_level_z                 Contin.       0.1557\nt0_kessler_latent_anxiety_z                        Contin.       0.1641\nt0_kessler_latent_depression_z                     Contin.       0.1164\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     181.71 334.58       79.73\n\n\n\n\nCode\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro PS: ebalance\")\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori PS: ebalance\")\n\n\nlibrary(patchwork)\n\n\nlove_plot_e / love_plot_m\n\n\n\n\n\nMore data wrangling\n\n\nCode\n# prepare data post-weighting ---------------------------------------------\n\n\n# prepare data\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n\n\nLet’s consider a pseudo experiment where someome moves from inactive to active.\n\n\nCode\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n\n\nCalculate E-values\n\n\nCode\n# read data\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all &lt;- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety &lt;- df_anxiety_all |&gt; \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nCode\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab &lt;- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n\nCode\ntest_tab |&gt; kbl(format = \"markdown\")\n\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.027\n-0.192\n0.112\n1.185\n1.000\n\n\nRD_e\n0.077\n0.023\n0.132\n1.352\n1.166"
  },
  {
    "objectID": "scripts/experiment_template.html#differences-by-subgroups",
    "href": "scripts/experiment_template.html#differences-by-subgroups",
    "title": "experiment_template.qmd",
    "section": "Differences by subgroups",
    "text": "Differences by subgroups\n\n\nCode\ndf_anxiety_all_plot &lt;- df_anxiety_all |&gt; \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n\n\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.014    0.205\nE[Y(active)]_m         0.138    0.062    0.218\nE[Y(very_active)]_m    0.134    0.016    0.260\nRD_m                  -0.027   -0.192    0.112\nE[Y(inactive)]_e       0.034    0.001    0.065\nE[Y(active)]_e        -0.021   -0.045    0.002\nE[Y(very_active)]_e   -0.043   -0.091    0.003\nRD_e                   0.077    0.023    0.132\nRD_m - RD_e           -0.104   -0.272    0.047\n\n\nCode\nplot_sub_forest(df_anxiety_all_plot)\n\n\n\n\n\nCode\ndf_anxiety_all_plot|&gt; \n  kbl(format = \"html\")\n\n\n\n\n\n\nestimate\nlower_ci\nupper_ci\n\n\n\n\nE[Y(inactive)]_m\n0.108\n0.014\n0.205\n\n\nE[Y(active)]_m\n0.138\n0.062\n0.218\n\n\nE[Y(very_active)]_m\n0.134\n0.016\n0.260\n\n\nRD_m\n-0.027\n-0.192\n0.112\n\n\nE[Y(inactive)]_e\n0.034\n0.001\n0.065\n\n\nE[Y(active)]_e\n-0.021\n-0.045\n0.002\n\n\nE[Y(very_active)]_e\n-0.043\n-0.091\n0.003\n\n\nRD_e\n0.077\n0.023\n0.132\n\n\nRD_m - RD_e\n-0.104\n-0.272\n0.047"
  },
  {
    "objectID": "scripts/experiment_template.html#depression",
    "href": "scripts/experiment_template.html#depression",
    "title": "experiment_template.qmd",
    "section": "Depression",
    "text": "Depression\n\n\nCode\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(est_all_d, here::here(\"data\", \"est_all_d\"))\n\n\n\n\nCode\nest_all_d &lt;- readRDS( here::here(\"data\", \"est_all_d\"))\n\n        \n# make dataframe\ndf_dep &lt;- data.frame( summary(est_all_d) )\n\ntable_estimates_depression &lt;- df_dep |&gt; \n    filter(row.names(df_dep) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)\n\n\nConfidence interval crosses the true value, so its E-value is 1.\nConfidence interval crosses the true value, so its E-value is 1.\n\n\nCode\ntable_depression |&gt; kbl(format = \"markdown\")\n\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.028\n-0.181\n0.130\n1.189\n1\n\n\nRD_e\n0.039\n-0.014\n0.095\n1.230\n1\n\n\n\n\n\nSummary\n\n\nCode\n# view summary\ndf_dep |&gt; \n  kbl(format = \"html\")\n\n\n\n\n\n\nEstimate\nX2.5..\nX97.5..\n\n\n\n\nE[Y(inactive)]_m\n0.1510358\n0.0654501\n0.2438887\n\n\nE[Y(active)]_m\n0.1877594\n0.0988091\n0.2840404\n\n\nE[Y(very_active)]_m\n0.1793980\n0.0631538\n0.2887767\n\n\nRD_m\n-0.0283622\n-0.1814928\n0.1302197\n\n\nE[Y(inactive)]_e\n0.0035095\n-0.0324006\n0.0381093\n\n\nE[Y(active)]_e\n-0.0168208\n-0.0406474\n0.0080300\n\n\nE[Y(very_active)]_e\n-0.0358775\n-0.0827228\n0.0064543\n\n\nRD_e\n0.0393869\n-0.0144998\n0.0949695\n\n\nRD_m - RD_e\n-0.0677491\n-0.2273110\n0.1043417\n\n\n\n\n\n\n\nCode\n# This table provides estimated levels of depression, in standard deviation units, for different levels of activity for two groups: Māori (indicated by \"_m\") and NZ Europeans (indicated by \"_e\").\n#\n# The expectations are named as `E[Y(&lt;level of activity&gt;)]_group`, where the level of activity can be `inactive`, `active`, or `very_active`.\n#\n# Here is a breakdown of the results.\n#\n#   1. For the Māori group (`_m`):\n#\n#   - `E[Y(inactive)]_m`: When inactive, the expected level of depression is 0.23 standard deviations, with a 95% confidence interval from 0.116 to 0.356.\n# - `E[Y(active)]_m`: When active, the expected level of depression decreases to 0.193 standard deviations, with a 95% confidence interval from 0.108 to 0.282.\n# - `E[Y(very_active)]_m`: When very active, the expected level of depression further decreases to 0.133 standard deviations, with a 95% confidence interval from 0.009 to 0.262.\n# - `RD_m`: The risk difference (RD) between inactive and very active Māori individuals is 0.097 standard deviations, with a 95% confidence interval from -0.068 to 0.274. This indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# 2. For the NZ European group (`_e`):\n#\n#   - `E[Y(inactive)]_e`: When inactive, the expected level of depression is 0.034 standard deviations, with a 95% confidence interval from -0.012 to 0.078.\n# - `E[Y(active)]_e`: When active, the expected level of depression slightly decreases to -0.006 standard deviations, with a 95% confidence interval from -0.03 to 0.016.\n# - `E[Y(very_active)]_e`: When very active, the expected level of depression further decreases to -0.046 standard deviations, with a 95% confidence interval from -0.086 to -0.007.\n# - `RD_e`: The risk difference (RD) between inactive and very active NZ European individuals is 0.081 standard deviations, with a 95% confidence interval from 0.02 to 0.138. Similar to the Māori group, this indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# The last row, `RD_m - RD_e`, represents the difference in risk differences between Māori and NZ Europeans. It's 0.017 standard deviations with a 95% confidence interval from -0.152 to 0.204. This is not statistically significant (the confidence interval contains 0), suggesting that the difference in depression reduction from being inactive to very active is not significantly different between the two groups.\n#\n# These are estimates and subject to statistical uncertainty. While they suggest a trend, the wide confidence intervals indicate that these estimates come with a degree of uncertainty.\n\n\n\n\nCode\ndf_dep_plot_data &lt;- df_dep |&gt; \n rename( lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) \n\n\ndf_dep\n\n\n                        Estimate      X2.5..     X97.5..\nE[Y(inactive)]_m     0.151035788  0.06545015 0.243888746\nE[Y(active)]_m       0.187759424  0.09880906 0.284040417\nE[Y(very_active)]_m  0.179397991  0.06315378 0.288776697\nRD_m                -0.028362203 -0.18149283 0.130219738\nE[Y(inactive)]_e     0.003509459 -0.03240062 0.038109331\nE[Y(active)]_e      -0.016820762 -0.04064738 0.008029976\nE[Y(very_active)]_e -0.035877479 -0.08272275 0.006454331\nRD_e                 0.039386939 -0.01449977 0.094969533\nRD_m - RD_e         -0.067749141 -0.22731101 0.104341702\n\n\nCode\nplot_sub_forest &lt;- function(df) {\n  require(ggplot2)\n\n  # Check if required packages are installed\n  required_packages &lt;- c(\"ggplot2\")\n  new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(df))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \", paste(missing_cols, collapse = \", \"))\n  \n  # Order the factor levels by the estimate column in decreasing order\n  \n  ggplot(df, aes(x=estimate, y=factor(row.names(df)))) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height=0.3) +\n    geom_vline(xintercept = 0, linetype=\"dashed\", color = \"red\") +\n    theme_bw() +\n    xlab(\"Estimate\") +\n    ylab(\"\")\n}\nplot_sub_forest(df_dep_plot_data)"
  },
  {
    "objectID": "scripts/experiment_template.html#preliminaries-source-functions-import-data.",
    "href": "scripts/experiment_template.html#preliminaries-source-functions-import-data.",
    "title": "experiment_template.qmd",
    "section": "Preliminaries: source functions, import data.",
    "text": "Preliminaries: source functions, import data.\nWe source our functions, load libraries, and important our (synthetic) data.\n\n\nCode\n# PSYCH 434: Example script for assessment 3 and 5.\n\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions \nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\nsource(\n  \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n)\n\n############## ############## ############## ############## ############## ############## ############## ########\n#########  ############## ############## IMPORT DATA ##############  ############## ############## ##############\n############## ############## ############## ############## ############## ############## ############## ########\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables. \ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2,], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n\n\n\nCode\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt’s a good idea to plot the data\n\nData Wrangling.\nNext, we’ll get the data into shape.\nConsider the following causal questions: “Does exercise affect well-being?” “Do such effects, if they exist, differ by ethnicity?”\nThese questions are not precise. What type of excercise? How regularly must one exercise? For how long must one exercise? Which ethnicities shall we compare? Why?\nIt helpst to think like an experimentalist… (say more)\nIn this exampe, we’ll do the following:\n\nCreate a Kessler 6 average score\nCreate a Kessler 6 sum score\nCreate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\nCreate a log Exercise score\nCreate a coarsened Exercise score.\n\nConsider: the NZAVS asks participants the following question. During the past week, list “Hours spent exercising/physical activity.” The question is inherently unclear about what sort of physical activity someone is doing. When participants respond to this question, what do they mean? John considers anything that is not sleep to be physical activity. He returns a high number. Jane only counts aerobic exercise. For Jane, walking an hour to work and back doesn’t count.\nRecall that an assumption of causal inference is consistency. (Say more … then leave to the side.)\nFor now, let’s create the indicators.\n\n\nCode\n# create sum score of kessler 6\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(kessler_6  = mean(\n    sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )))) |&gt;\n  mutate(kessler_6_sum = round(\n    sum(c (kessler_depressed,\n                   kessler_hopeless,\n                   kessler_nervous,\n                   kessler_effort,\n                   kessler_restless,\n                   kessler_worthless)),\n    digits = 0\n  )) |&gt;  ungroup() |&gt;\n# Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n       labels = c(\n        \"not_depressed\",\n        \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) |&gt; # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\n        \"inactive\",\n        \"active\",\n        \"very_active\"      ),\n      # Define thresholds for categories\n      levels = c(\"(-1,2]\", \"(2,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |&gt;\n\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\n\nWe next do some data checks. I will leave you to do these in your own time.\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax( dt_start$hours_exercise)\nmin( dt_start$hours_exercise)\n# checks\ntable(is.na(dt_start$kessler_6_coarsen))\ntable(is.na(dt_start$hours_exercise_coarsen))\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\n\n\n# inspect kessler 6\ntable(dt_start$kessler_6_coarsen)\ntable(dt_start$hours_exercise_coarsen)\n\nhist( as.numeric(dt_start$kessler_6_coarsen) )\nhist( as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nCFA for Kessler 6\nWe have learned how to do confirmatory factor analysis. Let’s put this knowledge to use, and consider whether Kessler 6 is one variable.\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\n\nCode\n# Suppose we have reason to think Kessler 6 isn't one thing.\n# Let's put our factor analysis skills to work\n# Here we will use the paramters and see packages for R (part of the Easystats suite)\n\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n\nLoading required package: psych\n\n\nCode\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n\nLoading required package: see\n\n\nCode\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\nLoading required package: lavaan\n\n\nThis is lavaan 0.6-15\nlavaan is FREE software! Please report any bugs.\n\n\nCode\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\nLoading required package: datawizard\n\n\nCode\n# select the columns we need. \ndt_only_k6 &lt;- dt_start |&gt; select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 70564.23, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.86). The individual KMO scores are: kessler_depressed (0.83), kessler_effort (0.89), kessler_hopeless (0.85), kessler_worthless (0.85), kessler_nervous (0.88), kessler_restless (0.85).\n\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors.\n\n\nCode\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) %&gt;%\n  model_parameters(sort = TRUE, threshold = \"max\")\n\n\nLoading required namespace: GPArotation\n\n\nCode\nefa\n\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          | MR1  | MR2  | MR3  | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_depressed | 0.85 |      |      |    1.01    |    0.33   \nkessler_worthless | 0.79 |      |      |    1.00    |    0.35   \nkessler_hopeless  | 0.75 |      |      |    1.02    |    0.33   \nkessler_nervous   |      | 1.00 |      |    1.00    |  5.00e-03 \nkessler_restless  |      |      | 0.69 |    1.02    |    0.52   \nkessler_effort    |      |      | 0.48 |    1.66    |    0.50   \n\nThe 3 latent factors (oblimin rotation) accounted for 66.05% of the total variance of the original data (MR1 = 35.14%, MR2 = 17.17%, MR3 = 13.73%).\n\n\nCode\n# This output presents the results of an exploratory factor analysis (EFA), a statistical method used to discover the underlying structure of a relatively large set of variables. It's often used when you don't have a specific hypothesis about what latent factors (unobservable variables) might be influencing the observed variables in your dataset.\n#\n# In this analysis, we've requested three factors (latent variables), and the table presents the loadings of each observed variable on each of these factors. The loadings can be interpreted as the correlations between the observed variables and the latent factors.\n#\n# Here's how to interpret the output:\n#\n#   The variables kessler_depressed, kessler_worthless, and kessler_hopeless load strongly on the first latent factor (MR1), and do not significantly load on the other two. This suggests that these three variables share some common underlying factor.\n#\n# The variable kessler_nervous loads exclusively on the second latent factor (MR2), suggesting it might represent a different latent construct.\n#\n# The variables kessler_restless and kessler_effort load on the third latent factor (MR3), which could represent yet another underlying construct.\n#\n# The \"Complexity\" column indicates the complexity of each item. Complexity 1 indicates that the item is influenced mostly by a single factor.\n#\n# The \"Uniqueness\" column represents the proportion of variance in each variable that is not explained by the factors. For example, the uniqueness of kessler_depressed is 0.33, which means that 33% of the variance in this variable is not accounted for by the three factors.\n#\n# Lastly, the total variance explained by the three latent factors is 66.05%, with MR1 explaining 35.14%, MR2 explaining 17.17%, and MR3 explaining 13.73%. This indicates that about two-thirds of the variance in the six observed variables can be explained by the three latent factors extracted in the analysis.\n\n\n# fa -- there is no agreed method!\n# method of agreement:\n\n\n\n\nCode\nn &lt;- n_factors(dt_only_k6)\n\n# # summary\n# as.data.frame(n)\n\n# plot of smmary\nplot(n) + theme_modern()\n\n\n\n\n\nCode\n## CFA\n\n\nNext try a CFA\n\n\nCode\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .07, seed = 123)\n\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n\n\nCode\n#|label: cfa_all\n# one factor\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three structure model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n\n\nCode\nstructure_k6_two\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n\n\nCode\nstructure_k6_three\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_restless\nMR3 =~ kessler_nervous + .row_id\n\n\n\n\nCode\n# fit and compare models\none_latent &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\ntwo_latents &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\nthree_latents &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\ncompare &lt;- performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent\nlavaan\n1359.7168\n14\n0\n159746.19\n21\n0\n0.9533955\n0.9067909\n0.9914883\n0.9873622\n0.9915748\n0.1033455\n0.0987385\n0.1080285\n0.0000000\n36.00334\n0.0493327\n0.9872324\n0.6609922\n0.9915752\n0.9915748\n-151483.7\n302995.3\n0\n303094.8\n0\n303050.3\n\n\ntwo_latents\nlavaan\n317.9709\n13\n0\n30915.77\n21\n0\n0.9900793\n0.9786322\n0.9897149\n0.9840541\n0.9901287\n0.0510548\n0.0462789\n0.0559908\n0.3499758\n36.31236\n0.0226983\n0.9833857\n0.6126807\n0.9901313\n0.9901287\n-150962.8\n301955.6\n1\n302062.2\n1\n302014.5\n\n\nthree_latents\nlavaan\n747.8723\n12\n0\n20903.30\n21\n0\n0.9763317\n0.9447739\n0.9642223\n0.9383317\n0.9647609\n0.0825447\n0.0775761\n0.0876237\n0.0000000\n37.13824\n0.0377955\n0.9373890\n0.5509842\n0.9647761\n0.9647609\n-151177.7\n302387.5\n0\n302501.2\n0\n302450.3\n\n\n\n\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\nOne_latent Model: This model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p&lt;0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can’t solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n\nTwo_latents Model: This model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n\nThree_latents Model: This model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It’s also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically.\n\n\n\nTry with multiple groups\n\n\nCode\n# select the columns we need + ethnicity\ndt_eth_k6_eth &lt;- dt_start |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless, eth_cat)\n\n\n\n# remove ethnicity for traning data \n\n\n\n\n# first partition the data \npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .07, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n\n# fit and compare models for configural equivalence\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n\n# fit and compare models for metric equivalence\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, \n                                                        two_latents_eth_metric, \n                                                        three_latents_eth_metric, \n                                                        verbose = FALSE)\n\n\n# fit and compare models for scalar equivalence\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\ncompare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, \n                                                        two_latents_eth_scalar, \n                                                        three_latents_eth_scalar, \n                                                        verbose = FALSE)\n\n\nRecall, in the context of measurement and factor analysis, the concepts of configural, metric, and scalar invariance relate to the comparability of a measurement instrument, such as a survey or test, across different groups.\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\nLet’s first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\nConfigural invariance refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying constructs (factors) are defined the same way for different groups. This doesn’t mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.\n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\nMetric invariance (also known as “weak invariance”) refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.\n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.\n\nScalar invariance (also known as “strong invariance”) involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\nConfigural invariance:\n\n\nCode\nas.data.frame(compare_eth_configural)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_configural\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_configural\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_configural\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model.\n\n\nMetric Equivalence\n\n\nCode\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_metric\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_metric\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_metric\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nScalar Equivalence\n\n\nCode\n# view as html table\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_scalar\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_scalar\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_scalar\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, one must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model.\nOverall it seems that we have good evidence for the two-factor model of Kessler-6.\nLet’s next get the data into shape for analysis. Here we create a variable for the two factors:\n\n\nCode\n# get two factors from data\ndt_start2 &lt;- dt_start |&gt;\n  arrange(id, wave) |&gt;\n  rowwise() |&gt;\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |&gt; ungroup()\n\n\nIt is useful toinspect histograms\n\n\nCode\nhist(dt_start2$kessler_latent_anxiety)\n\n\n\n\n\n\n\nCode\nhist(dt_start2$kessler_latent_depression)\n\n\n\n\n\n\n\nAssess change in the exposure\nNot this is just a description of the the summary scores. We do not assess change within indivuals\n\n\nCode\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start2 |&gt;\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n\nCode\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n\n\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n\n\nNext consider Māori only\n\n\nCode\n# Maori only\n\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n\nCode\nprint(t_tab_m$table)\n\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n\n\n\n\nCode\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\nt_tab_e &lt;- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n\nCode\n# table\nprint(t_tab_e$table)\n\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\n\nCreate wide data frame for analysis\n\n\nCode\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n# To find out more about our dataset go here:\n# https://github.com/go-bayes/psych-434-2023/blob/main/data/readme.qmd\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# note that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exclude_vars)\n\n  # Now:\n  data %&gt;% select(all_of(exclude_vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(t0_column_order)\n\n  # Now:\n  data %&gt;% select(all_of(t0_column_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;."
  },
  {
    "objectID": "scripts/experiment_template.html#questions",
    "href": "scripts/experiment_template.html#questions",
    "title": "experiment_template.qmd",
    "section": "Questions",
    "text": "Questions\nQuestion 1. Does exercise affect anxiety/depression?\nQuestion 2: Do these effects vary among NZ Europeans and Māori?\nAre these questions clearly stated? No, they are vague:\n\nHow much exercise?\nBy which measures of depression?\nWhen should the effects be observed?\n\nRecall we can help to clarify these questions by attempting to emulate an experiment.\nWe shall use the measure of exercise in the NZAVS: hours of activity per week (we shall see, the question remains vague).\nWe will assess the 1-year effect on Kessler-6 depression after initiating a change in exercise (intention-to-treat).\nWe will investigate effect-modification by NZ European and Māori ethnic identification."
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "",
    "text": "Today, we dive deep into data analysis for causal inference ast it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/10-content.html#questions",
    "href": "content/10-content.html#questions",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Questions",
    "text": "Questions\nQuestion 1. Does exercise affect anxiety/depression?\nQuestion 2: Do these effects vary among NZ Europeans and Māori?\nAre these questions clearly stated? No, they are vague:\n\nHow much exercise?\nBy which measures of depression?\nWhen should the effects be observed?\n\nRecall we can help to clarify these questions by attempting to emulate an experiment.\nWe shall use the measure of exercise in the NZAVS: hours of activity per week (we shall see, the question remains vague).\nWe will assess the 1-year effect on Kessler-6 depression after initiating a change in exercise (intention-to-treat).\nWe will investigate effect-modification by NZ European and Māori ethnic identification."
  },
  {
    "objectID": "content/10-content.html#preliminaries-source-functions-import-data.",
    "href": "content/10-content.html#preliminaries-source-functions-import-data.",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Preliminaries: source functions, import data.",
    "text": "Preliminaries: source functions, import data.\nWe source our functions, load libraries, and important our (synthetic) data.\n\n\nCode\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions \nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\nsource(\n  \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n)\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables. \ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2,], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n# extra packages we need\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt is alwasy a good idea to plot the data (do on your own time.)"
  },
  {
    "objectID": "content/10-content.html#descriptive-table",
    "href": "content/10-content.html#descriptive-table",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Descriptive table",
    "text": "Descriptive table\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n\n\n\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n# another method for making a table\n# x &lt;- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |&gt;\n#   kable_material(c(\"striped\", \"hover\"))\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n# view object\nskimr::skim(dt)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndt\n\n\n\n\nNumber of rows\n\n\n9462\n\n\n\n\nNumber of columns\n\n\n26\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n7\n\n\n\n\nnumeric\n\n\n19\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nid\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n9462\n\n\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\n\nt0_male\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nNot: 5767, Mal: 3695\n\n\n\n\nt0_eth_cat\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\neur: 8641, māo: 821\n\n\n\n\nt0_gen_cohort\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n5\n\n\nGen: 4107, Gen: 3311, Gen: 1716, Gen: 164\n\n\n\n\nt0_urban\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nurb: 7762, rur: 1700\n\n\n\n\nt0_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4131, ina: 3557, ver: 1774\n\n\n\n\nt1_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4281, ina: 3187, ver: 1994\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nt0_edu_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.29\n\n\n-1.05\n\n\n0.44\n\n\n0.82\n\n\n1.66\n\n\n▂▃▃▇▂\n\n\n\n\nt0_employed_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.26\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n▂▁▁▁▇\n\n\n\n\nt0_nz_dep2018_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.36\n\n\n-0.92\n\n\n-0.16\n\n\n0.63\n\n\n2.17\n\n\n▇▆▆▅▂\n\n\n\n\nt0_nzsei13_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.94\n\n\n-0.75\n\n\n0.25\n\n\n0.81\n\n\n2.07\n\n\n▁▃▅▇▁\n\n\n\n\nt0_partner_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.99\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n▂▁▁▁▇\n\n\n\n\nt0_parent_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.58\n\n\n-1.58\n\n\n0.63\n\n\n0.63\n\n\n0.63\n\n\n▃▁▁▁▇\n\n\n\n\nt0_pol_orient_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.87\n\n\n-1.02\n\n\n-0.28\n\n\n0.44\n\n\n2.62\n\n\n▇▆▇▅▂\n\n\n\n\nt0_agreeableness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.46\n\n\n-0.62\n\n\n0.12\n\n\n0.68\n\n\n1.79\n\n\n▁▁▃▇▆\n\n\n\n\nt0_conscientiousness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.13\n\n\n-0.65\n\n\n0.08\n\n\n0.76\n\n\n1.91\n\n\n▁▁▅▇▅\n\n\n\n\nt0_extraversion_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.48\n\n\n-0.71\n\n\n-0.04\n\n\n0.72\n\n\n2.67\n\n\n▂▆▇▅▁\n\n\n\n\nt0_honesty_humility_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.95\n\n\n-0.69\n\n\n0.17\n\n\n0.84\n\n\n1.45\n\n\n▁▁▃▆▇\n\n\n\n\nt0_openness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.76\n\n\n-0.71\n\n\n0.05\n\n\n0.81\n\n\n1.90\n\n\n▁▂▆▇▅\n\n\n\n\nt0_neuroticism_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.18\n\n\n-0.76\n\n\n-0.09\n\n\n0.71\n\n\n3.14\n\n\n▃▇▇▃▁\n\n\n\n\nt0_modesty_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.67\n\n\n-0.66\n\n\n0.19\n\n\n0.83\n\n\n1.26\n\n\n▁▁▂▅▇\n\n\n\n\nt0_religion_identification_level_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-0.56\n\n\n-0.56\n\n\n-0.56\n\n\n-0.08\n\n\n2.37\n\n\n▇▁▁▁▂\n\n\n\n\nt0_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.72\n\n\n-0.69\n\n\n-0.19\n\n\n0.70\n\n\n4.01\n\n\n▇▇▆▁▁\n\n\n\n\nt0_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.21\n\n\n-0.63\n\n\n-0.13\n\n\n0.42\n\n\n4.83\n\n\n▇▂▂▁▁\n\n\n\n\nt2_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.23\n\n\n-0.65\n\n\n-0.16\n\n\n0.39\n\n\n4.75\n\n\n▇▃▂▁▁\n\n\n\n\nt2_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.74\n\n\n-0.70\n\n\n-0.20\n\n\n0.68\n\n\n3.97\n\n\n▇▇▆▁▁\n\n\n\n\n\n\n\n\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))"
  },
  {
    "objectID": "content/10-content.html#propensity-scores",
    "href": "content/10-content.html#propensity-scores",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Propensity scores",
    "text": "Propensity scores\nNext we generate propensity scores. Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\nThe first step is to obtain the baseline variables. note that we must remove “t0_eth_cat” because we are performing separate weighting for each stratum within this variable.\n\n# read  data -- you may start here if you need to repeat the analysis\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use ps (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\nResults, first for Europeans\n\ndt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\ndt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n#bal.tab(dt_match_ps$euro)   #  not as good\n\nResults for Maori\n\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n#bal.tab(dt_match_ps$māori)   #  not good\n\n\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2310 |---------------------------| 7.0511\nactive      0.5769  |----|                       1.9603\nvery_active 0.1601 |----------------------|      5.9191\n\n- Units with the 5 most extreme weights by group:\n                                               \n               6560      9   7209   4878   5105\n    inactive 5.1084 5.1312 5.1642 5.3517 7.0511\n               3279   1867   4754   2783   7057\n      active 1.7467 1.7654 1.7701 1.8692 1.9603\n               5977   4293    700   2352   4765\n very_active 5.1495 5.3064 5.4829 5.7273 5.9191\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.743 0.536   0.212       0\nactive            0.270 0.248   0.036       0\nvery_active       0.862 0.637   0.302       0\n\n- Effective Sample Sizes:\n\n           inactive  active very_active\nUnweighted  2880.   3927.       1834.  \nWeighted    1855.89 3659.59     1052.01\n\n# summary maori\nsum_m\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2213  |---------------|            3.8101\nactive      0.3995   |-----|                     1.9800\nvery_active 0.0719 |---------------------------| 6.2941\n\n- Units with the 5 most extreme weights by group:\n                                               \n                296    322    355    758    812\n    inactive 3.0104  3.407 3.6372 3.7101 3.8101\n                 95    783    473    703    699\n      active 1.8319 1.8387 1.9395 1.9436   1.98\n                745    149     78    226    718\n very_active 4.0921 4.3405 4.4111 4.6833 6.2941\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.627 0.475   0.170       0\nactive            0.321 0.264   0.050       0\nvery_active       1.050 0.732   0.411       0\n\n- Effective Sample Sizes:\n\n           inactive active very_active\nUnweighted   307.   354.        160.  \nWeighted     220.54 321.09       76.39\n\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro PS: ebalance\")\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori PS: ebalance\")\n\n\nlibrary(patchwork)\n\n\nlove_plot_e / love_plot_m\n\n\n\n\n\nSummary NZ Euro Propensity scores.\nWe estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\nThe effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\nThe weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nWe also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nWe plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\nOverall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n\nSummary Maori Propensity scores.\nResults:\nThe entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\nThe effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\nThe weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nThe study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nIn conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n\nMore data wrangling\nNote that we need to attach the weights from the propensity score model back to the data.\nHowever, because our weighting analysis estimates a model for the exposure, we only need to do this analysis once, no matter how many outcomes we investigate. So there’s a little good news.\n\n# prepare nz_euro data\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe."
  },
  {
    "objectID": "content/10-content.html#differences-by-subgroups",
    "href": "content/10-content.html#differences-by-subgroups",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Differences by subgroups",
    "text": "Differences by subgroups\n\ndf_anxiety_all_plot &lt;- df_anxiety_all |&gt; \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.014    0.205\nE[Y(active)]_m         0.138    0.062    0.218\nE[Y(very_active)]_m    0.134    0.016    0.260\nRD_m                  -0.027   -0.192    0.112\nE[Y(inactive)]_e       0.034    0.001    0.065\nE[Y(active)]_e        -0.021   -0.045    0.002\nE[Y(very_active)]_e   -0.043   -0.091    0.003\nRD_e                   0.077    0.023    0.132\nRD_m - RD_e           -0.104   -0.272    0.047\n\nplot_sub_forest(df_anxiety_all_plot)\n\n\n\ndf_anxiety_all_plot|&gt; \n  kbl(format = \"html\")\n\n\n\n\n\nestimate\nlower_ci\nupper_ci\n\n\n\n\nE[Y(inactive)]_m\n0.108\n0.014\n0.205\n\n\nE[Y(active)]_m\n0.138\n0.062\n0.218\n\n\nE[Y(very_active)]_m\n0.134\n0.016\n0.260\n\n\nRD_m\n-0.027\n-0.192\n0.112\n\n\nE[Y(inactive)]_e\n0.034\n0.001\n0.065\n\n\nE[Y(active)]_e\n-0.021\n-0.045\n0.002\n\n\nE[Y(very_active)]_e\n-0.043\n-0.091\n0.003\n\n\nRD_e\n0.077\n0.023\n0.132\n\n\nRD_m - RD_e\n-0.104\n-0.272\n0.047"
  },
  {
    "objectID": "content/10-content.html#depression",
    "href": "content/10-content.html#depression",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Depression",
    "text": "Depression\n\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(est_all_d, here::here(\"data\", \"est_all_d\"))\n\n\nest_all_d &lt;- readRDS( here::here(\"data\", \"est_all_d\"))\n\n        \n# make dataframe\ndf_dep &lt;- data.frame( summary(est_all_d) )\n\ntable_estimates_depression &lt;- df_dep |&gt; \n    filter(row.names(df_dep) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)\n\nConfidence interval crosses the true value, so its E-value is 1.\nConfidence interval crosses the true value, so its E-value is 1.\n\ntable_depression |&gt; kbl(format = \"markdown\")\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.028\n-0.181\n0.130\n1.189\n1\n\n\nRD_e\n0.039\n-0.014\n0.095\n1.230\n1\n\n\n\n\n\nSummary\n\n# view summary\ndf_dep |&gt; \n  mutate_if(is.numeric, round, 3) |&gt; \n  kbl(format = \"html\")\n\n\n\n\n\nEstimate\nX2.5..\nX97.5..\n\n\n\n\nE[Y(inactive)]_m\n0.151\n0.065\n0.244\n\n\nE[Y(active)]_m\n0.188\n0.099\n0.284\n\n\nE[Y(very_active)]_m\n0.179\n0.063\n0.289\n\n\nRD_m\n-0.028\n-0.181\n0.130\n\n\nE[Y(inactive)]_e\n0.004\n-0.032\n0.038\n\n\nE[Y(active)]_e\n-0.017\n-0.041\n0.008\n\n\nE[Y(very_active)]_e\n-0.036\n-0.083\n0.006\n\n\nRD_e\n0.039\n-0.014\n0.095\n\n\nRD_m - RD_e\n-0.068\n-0.227\n0.104\n\n\n\n\n\n\n# This table provides estimated levels of depression, in standard deviation units, for different levels of activity for two groups: Māori (indicated by \"_m\") and NZ Europeans (indicated by \"_e\").\n#\n# The expectations are named as `E[Y(&lt;level of activity&gt;)]_group`, where the level of activity can be `inactive`, `active`, or `very_active`.\n#\n# Here is a breakdown of the results.\n#\n#   1. For the Māori group (`_m`):\n#\n#   - `E[Y(inactive)]_m`: When inactive, the expected level of depression is 0.23 standard deviations, with a 95% confidence interval from 0.116 to 0.356.\n# - `E[Y(active)]_m`: When active, the expected level of depression decreases to 0.193 standard deviations, with a 95% confidence interval from 0.108 to 0.282.\n# - `E[Y(very_active)]_m`: When very active, the expected level of depression further decreases to 0.133 standard deviations, with a 95% confidence interval from 0.009 to 0.262.\n# - `RD_m`: The risk difference (RD) between inactive and very active Māori individuals is 0.097 standard deviations, with a 95% confidence interval from -0.068 to 0.274. This indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# 2. For the NZ European group (`_e`):\n#\n#   - `E[Y(inactive)]_e`: When inactive, the expected level of depression is 0.034 standard deviations, with a 95% confidence interval from -0.012 to 0.078.\n# - `E[Y(active)]_e`: When active, the expected level of depression slightly decreases to -0.006 standard deviations, with a 95% confidence interval from -0.03 to 0.016.\n# - `E[Y(very_active)]_e`: When very active, the expected level of depression further decreases to -0.046 standard deviations, with a 95% confidence interval from -0.086 to -0.007.\n# - `RD_e`: The risk difference (RD) between inactive and very active NZ European individuals is 0.081 standard deviations, with a 95% confidence interval from 0.02 to 0.138. Similar to the Māori group, this indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# The last row, `RD_m - RD_e`, represents the difference in risk differences between Māori and NZ Europeans. It's 0.017 standard deviations with a 95% confidence interval from -0.152 to 0.204. This is not statistically significant (the confidence interval contains 0), suggesting that the difference in depression reduction from being inactive to very active is not significantly different between the two groups.\n#\n# These are estimates and subject to statistical uncertainty. While they suggest a trend, the wide confidence intervals indicate that these estimates come with a degree of uncertainty.\n\n\ndf_dep_plot_data &lt;- df_dep |&gt; \n rename( lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) \n\n\ndf_dep\n\n                        Estimate      X2.5..     X97.5..\nE[Y(inactive)]_m     0.151035788  0.06545015 0.243888746\nE[Y(active)]_m       0.187759424  0.09880906 0.284040417\nE[Y(very_active)]_m  0.179397991  0.06315378 0.288776697\nRD_m                -0.028362203 -0.18149283 0.130219738\nE[Y(inactive)]_e     0.003509459 -0.03240062 0.038109331\nE[Y(active)]_e      -0.016820762 -0.04064738 0.008029976\nE[Y(very_active)]_e -0.035877479 -0.08272275 0.006454331\nRD_e                 0.039386939 -0.01449977 0.094969533\nRD_m - RD_e         -0.067749141 -0.22731101 0.104341702\n\nplot_sub_forest &lt;- function(df) {\n  require(ggplot2)\n\n  # Check if required packages are installed\n  required_packages &lt;- c(\"ggplot2\")\n  new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(df))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \", paste(missing_cols, collapse = \", \"))\n  \n  # Order the factor levels by the estimate column in decreasing order\n  \n  ggplot(df, aes(x=estimate, y=factor(row.names(df)))) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height=0.3) +\n    geom_vline(xintercept = 0, linetype=\"dashed\", color = \"red\") +\n    theme_bw() +\n    xlab(\"Estimate\") +\n    ylab(\"\")\n}\nplot_sub_forest(df_dep_plot_data)"
  },
  {
    "objectID": "content/10-content.html#overview",
    "href": "content/10-content.html#overview",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "",
    "text": "Today, we dive deep into data analysis for causal inference ast it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/10-content.html#revisit-the-checklist",
    "href": "content/10-content.html#revisit-the-checklist",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Revisit the checklist",
    "text": "Revisit the checklist\nAs we delve deeper, it’s essential to remember our checklist:\n\nClearly state your question.\nExplain its relevance.\nEnsure your question is causal.\nDevelop a subgroup analysis question if applicable.\n\nOur discussion today revolves around two main questions:\n\nDoes exercise influence anxiety/depression?\nDo these effects differ among NZ Europeans and Māori?\n\nWhile these questions offer a starting point, they lack specificity. We need to clarify:\n\nThe amount, regularity, and duration of exercise\nThe measures of depression to be used\nThe expected timeline for observing the effects\n\nRemember, we can clarify these by emulating a hypothetical experiment, a concept we call the Target Trial.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification.\nThis analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification. This analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\n\nSculpting the Data: A Hands-On Approach\nAs we venture further, we’ll perform a series of transformations to shape our data according to our needs. Our process will involve:\n\nConstructing a Kessler 6 average score\nBuilding a Kessler 6 sum score\nGenerating a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\nCrafting a log Exercise score\nCoarsening the Exercise score\n\nConsider the ambiguity in the NZAVS exercise question: “During the past week, list ‘Hours spent exercising/physical activity’.” Different people interpret physical activity differently; John may consider any wakeful time as physical activity, while Jane counts only aerobic exercise. Such variation underlines the importance of the consistency assumption in causal inference. But we’ll delve deeper into that later.\nFor now, let’s transform our indicators.\n\n# create sum score of kessler 6\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(kessler_6  = mean(sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )\n  ))) |&gt;\n  mutate(kessler_6_sum = round(sum(\n    c (\n      kessler_depressed,\n      kessler_hopeless,\n      kessler_nervous,\n      kessler_effort,\n      kessler_restless,\n      kessler_worthless\n    )\n  ),\n  digits = 0)) |&gt;  ungroup() |&gt;\n  # Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n      labels = c(\"not_depressed\",\n                 \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) |&gt; # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n  \n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,2]\", \"(2,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |&gt;\n  \n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\nWhy do we coarsen the exposure? Recall the consistency assumption of causal inference:\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nWhat is th hypothetical experiment here for change in exercise?\nThrough data wrangling, we can answer our research questions more effectively by manipulating variables into more meaningful and digestible forms. We imagine an experiment in which people were within one band of the coarsened exercise band and we\nThese data checks will ensure the accuracy and reliability of our transformations, setting the foundation for solid data analysis.\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\ntable(is.na(dt_start$kessler_6_coarsen))\ntable(is.na(dt_start$hours_exercise_coarsen))\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\n\n\n# inspect kessler 6\ntable(dt_start$kessler_6_coarsen)\ntable(dt_start$hours_exercise_coarsen)\n\nhist(as.numeric(dt_start$kessler_6_coarsen))\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nCFA for Kessler 6\nWe have learned how to do confirmatory factor analysis. Let’s put this knowledge to use but clarifying the underlying factor structure of Kessler-6\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\n# select the columns we need. \ndt_only_k6 &lt;- dt_start |&gt; select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 70564.23, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.86). The individual KMO scores are: kessler_depressed (0.83), kessler_effort (0.89), kessler_hopeless (0.85), kessler_worthless (0.85), kessler_nervous (0.88), kessler_restless (0.85).\n\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors.\n\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) %&gt;%\n  model_parameters(sort = TRUE, threshold = \"max\")\n\nLoading required namespace: GPArotation\n\nefa\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          | MR1  | MR2  | MR3  | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_depressed | 0.85 |      |      |    1.01    |    0.33   \nkessler_worthless | 0.79 |      |      |    1.00    |    0.35   \nkessler_hopeless  | 0.75 |      |      |    1.02    |    0.33   \nkessler_nervous   |      | 1.00 |      |    1.00    |  5.00e-03 \nkessler_restless  |      |      | 0.69 |    1.02    |    0.52   \nkessler_effort    |      |      | 0.48 |    1.66    |    0.50   \n\nThe 3 latent factors (oblimin rotation) accounted for 66.05% of the total variance of the original data (MR1 = 35.14%, MR2 = 17.17%, MR3 = 13.73%).\n\n\nThis output describes an exploratory factor analysis (EFA) with 3 factors conducted on the Kessler 6 (K6) scale data. The K6 scale is used to measure psychological distress.\nThe analysis identifies three latent factors, labeled MR1, MR2, and MR3, which collectively account for 66.05% of the variance in the K6 data. The factors MR1, MR2, and MR3 explain 35.14%, 17.17%, and 13.73% of the variance respectively.\nFactor loadings, indicating the strength and direction of the relationship between the K6 items and the latent factors, are as follows:\n\nFactor MR1 is strongly associated with ‘kessler_depressed’, ‘kessler_worthless’, and ‘kessler_hopeless’ with loadings of 0.85, 0.79, and 0.75 respectively.\nFactor MR2 is exclusively linked with ‘kessler_nervous’ with a loading of 1.00.\nFactor MR3 relates to ‘kessler_restless’ and ‘kessler_effort’ with loadings of 0.69 and 0.48 respectively.\n\nThe ‘Uniqueness’ values show the proportion of each variable’s variance that isn’t shared with the other variables.\nThe ‘Complexity’ values give a measure of how each item loads on more than one factor. All the items are either loading exclusively on one factor (complexity=1.00) or slightly more than one factor. ‘kessler_effort’ with complexity of 1.66 shows it’s the item most shared between the factors.\nThe analysis suggests these K6 items measure may measure three somewhat distinct, yet related, factors of psychological distress.\nHowever, the meaning of these factors would need to be interpreted in the context of the variables and the theoretical framework of the study.\nNotably, there are many many theoretical frameworks for in measurement theory. Here is a brief description of the different conclusions one might make, depending on one’s preferred theory.\n\n\nCode\nn &lt;- n_factors(dt_only_k6)\n\n# plot\nplot(n) + theme_classic()\n\n\n\n\n\n\n\nConfirmatory factor analysis (ignoring groups)\n\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .07, seed = 123)\n\n\n# set up training data\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n# one factor model\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three factor model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_two\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_three\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_restless\nMR3 =~ kessler_nervous + .row_id\n\n\nNext we perform the confirmatory factor analysis.\n\n# fit and compare models\n\n# one latent model\none_latent &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\n\n# two latents model\ntwo_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\n\n# three latents model\nthree_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\n\n# compare models\ncompare &lt;-\n  performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare) |&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent\nlavaan\n1359.7168\n14\n0\n159746.19\n21\n0\n0.9533955\n0.9067909\n0.9914883\n0.9873622\n0.9915748\n0.1033455\n0.0987385\n0.1080285\n0.0000000\n36.00334\n0.0493327\n0.9872324\n0.6609922\n0.9915752\n0.9915748\n-151483.7\n302995.3\n0\n303094.8\n0\n303050.3\n\n\ntwo_latents\nlavaan\n317.9709\n13\n0\n30915.77\n21\n0\n0.9900793\n0.9786322\n0.9897149\n0.9840541\n0.9901287\n0.0510548\n0.0462789\n0.0559908\n0.3499758\n36.31236\n0.0226983\n0.9833857\n0.6126807\n0.9901313\n0.9901287\n-150962.8\n301955.6\n1\n302062.2\n1\n302014.5\n\n\nthree_latents\nlavaan\n747.8723\n12\n0\n20903.30\n21\n0\n0.9763317\n0.9447739\n0.9642223\n0.9383317\n0.9647609\n0.0825447\n0.0775761\n0.0876237\n0.0000000\n37.13824\n0.0377955\n0.9373890\n0.5509842\n0.9647761\n0.9647609\n-151177.7\n302387.5\n0\n302501.2\n0\n302450.3\n\n\n\n\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\nOne_latent CFA:\nThis model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p&lt;0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can’t solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n\nTwo_latents CFA\nThis model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n\nThree_latents CFA\nThis model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It’s also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically.\n\n\n\nMulti-group Confirmatory Factor Analysis\nThis script runs multi-group confirmatory factor analysis (MG-CFA)\n\n# select needed columns plus 'ethnicity'\n# filter dataset for only 'euro' and 'maori' ethnic categories\ndt_eth_k6_eth &lt;- dt_start |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n         kessler_worthless, kessler_nervous,\n         kessler_restless, eth_cat)\n\n# partition the dataset into training and test subsets\n# stratify by ethnic category to ensure balanced representation\npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .07, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n# run confirmatory factor analysis (CFA) models for configural invariance across ethnic groups\n# models specify one, two, and three latent variables\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\n# compare model performances for configural invariance\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n# run CFA models for metric invariance, holding factor loadings equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\n# compare model performances for metric invariance\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric, verbose = FALSE)\n\n# run CFA models for scalar invariance, holding factor loadings and intercepts equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\n# compare model performances for scalar invariance\ncompare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar, verbose = FALSE)\n\nRecall, in the context of measurement and factor analysis, the concepts of configural, metric, and scalar invariance relate to the comparability of a measurement instrument, such as a survey or test, across different groups.\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\nLet’s first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\nConfigural invariance refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying constructs (factors) are defined the same way for different groups. This doesn’t mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.\n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\nMetric invariance (also known as “weak invariance”) refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.\n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.\n\nScalar invariance (also known as “strong invariance”) involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\nConfigural invariance\n\nas.data.frame(compare_eth_configural)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_configural\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_configural\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_configural\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model.\n\n\nMetric Equivalence\n\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_metric\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_metric\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_metric\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nScalar Equivalence\n\n# view as html table\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_scalar\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_scalar\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_scalar\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, one must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model.\nOverall it seems that we have good evidence for the two-factor model of Kessler-6.\nLet’s next get the data into shape for analysis. Here we create a variable for the two factors:\n\n\nCreate variables for the latent factors\n\n# get two factors from data\ndt_start2 &lt;- dt_start |&gt;\n  arrange(id, wave) |&gt;\n  rowwise() |&gt;\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\nInspect the data\n\nhist(dt_start2$kessler_latent_anxiety)\n\n\n\n\n\nhist(dt_start2$kessler_latent_depression)"
  },
  {
    "objectID": "content/10-content.html#set-up-your-workspace-preparing-for-the-journey",
    "href": "content/10-content.html#set-up-your-workspace-preparing-for-the-journey",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Set up your workspace Preparing for the Journey",
    "text": "Set up your workspace Preparing for the Journey\nTo kick things off, we will set up our environment. We’ll source essential functions, load necessary libraries, and import synthetic data for our exploration.\n\n\nCode\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions \nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\nsource(\n  \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n)\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables. \ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2,], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n# extra packages we need\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\n\nImport the data\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt is alwasy a good idea to plot the data (do on your own time.)"
  },
  {
    "objectID": "content/10-content.html#investigate-assumption-of-positivity",
    "href": "content/10-content.html#investigate-assumption-of-positivity",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Investigate assumption of positivity:",
    "text": "Investigate assumption of positivity:\nRecall the positive assuption:\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nNot this is just a description of the the summary scores. We do not assess change within indivuals\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start2 |&gt;\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n\n\nNext consider Māori only\n\n# Maori only\n\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\nprint(t_tab_m$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n\n\n\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\nt_tab_e &lt;- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n# table\nprint(t_tab_e$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\nCreate wide data frame for analysis\nRecall, I wrote a function for you that will put the data into temporal order such that measurement of the exposure and outcome appear at baseline, along with a rich set of baseline confounders, the exposure appears in the following wave, and the outcome appears in the wave following the exposure.\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design\n\n\n\n\n\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exclude_vars)\n\n  # Now:\n  data %&gt;% select(all_of(exclude_vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(t0_column_order)\n\n  # Now:\n  data %&gt;% select(all_of(t0_column_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;."
  },
  {
    "objectID": "content/10-content.html#statistical-model",
    "href": "content/10-content.html#statistical-model",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Statistical Model",
    "text": "Statistical Model\nLet’s consider a pseudo experiment where someone moves from inactive to active.\nWe will use doubly robust analysis. This means that we estimate a model using both propensity score weights (obtained using the ebalance algortithm) and regression stratification.\nI wrote a function that will caluclate “E-values.” An E-value is defined as the\n\nE-values represent the minimum strength or magnitude of association that an unmeasured confounder would need to have with both treatment and outcome in order to explain the observed effect estimates conditional on measured covariates.\n\n\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n\nCalculate E-values\n\n# read data\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all &lt;- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety &lt;- df_anxiety_all |&gt; \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab &lt;- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n\nConfidence interval crosses the true value, so its E-value is 1.\n\ntest_tab |&gt; kbl(format = \"markdown\")\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.027\n-0.192\n0.112\n1.185\n1.000\n\n\nRD_e\n0.077\n0.023\n0.132\n1.352\n1.166"
  },
  {
    "objectID": "content/10-content.html#anxiety-model-and-results",
    "href": "content/10-content.html#anxiety-model-and-results",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Anxiety Model and Results",
    "text": "Anxiety Model and Results\nLet’s consider a pseudo experiment where someone moves from inactive to active.\nWe will use doubly robust analysis. This means that we estimate a model using both propensity score weights (obtained using the ebalance algortithm) and regression stratification.\nI wrote a function that will caluclate “E-values.” An E-value is defined as the\n\nE-values represent the minimum strength or magnitude of association that an unmeasured confounder would need to have with both treatment and outcome in order to explain the observed effect estimates conditional on measured covariates.\n\n\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n\nCalculate E-values\n\n# read data\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all &lt;- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety &lt;- df_anxiety_all |&gt; \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab &lt;- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n\nConfidence interval crosses the true value, so its E-value is 1.\n\ntest_tab |&gt; kbl(format = \"markdown\")\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.027\n-0.192\n0.112\n1.185\n1.000\n\n\nRD_e\n0.077\n0.023\n0.132\n1.352\n1.166\n\n\n\n\n\nAnxiety results\n\ndf_anxiety_all_plot &lt;- df_anxiety_all |&gt; \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.014    0.205\nE[Y(active)]_m         0.138    0.062    0.218\nE[Y(very_active)]_m    0.134    0.016    0.260\nRD_m                  -0.027   -0.192    0.112\nE[Y(inactive)]_e       0.034    0.001    0.065\nE[Y(active)]_e        -0.021   -0.045    0.002\nE[Y(very_active)]_e   -0.043   -0.091    0.003\nRD_e                   0.077    0.023    0.132\nRD_m - RD_e           -0.104   -0.272    0.047\n\nplot_sub_forest(df_anxiety_all_plot)\n\n\n\ndf_anxiety_all_plot|&gt; \n  kbl(format = \"html\")\n\n\n\n\n\nestimate\nlower_ci\nupper_ci\n\n\n\n\nE[Y(inactive)]_m\n0.108\n0.014\n0.205\n\n\nE[Y(active)]_m\n0.138\n0.062\n0.218\n\n\nE[Y(very_active)]_m\n0.134\n0.016\n0.260\n\n\nRD_m\n-0.027\n-0.192\n0.112\n\n\nE[Y(inactive)]_e\n0.034\n0.001\n0.065\n\n\nE[Y(active)]_e\n-0.021\n-0.045\n0.002\n\n\nE[Y(very_active)]_e\n-0.043\n-0.091\n0.003\n\n\nRD_e\n0.077\n0.023\n0.132\n\n\nRD_m - RD_e\n-0.104\n-0.272\n0.047"
  },
  {
    "objectID": "content/10-content.html#depression-analysis-and-results",
    "href": "content/10-content.html#depression-analysis-and-results",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Depression Analysis and Results",
    "text": "Depression Analysis and Results\n\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(est_all_d, here::here(\"data\", \"est_all_d\"))\n\n\nest_all_d &lt;- readRDS( here::here(\"data\", \"est_all_d\"))\n\n        \n# make dataframe\ndf_dep &lt;- data.frame( summary(est_all_d) )\n\ntable_estimates_depression &lt;- df_dep |&gt; \n    filter(row.names(df_dep) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)\n\nConfidence interval crosses the true value, so its E-value is 1.\nConfidence interval crosses the true value, so its E-value is 1.\n\ntable_depression |&gt; kbl(format = \"markdown\")\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.028\n-0.177\n0.123\n1.189\n1\n\n\nRD_e\n0.039\n-0.018\n0.099\n1.230\n1\n\n\n\n\n\nSummary\n\n# view summary\ndf_dep |&gt; \n  mutate_if(is.numeric, round, 3) |&gt; \n  kbl(format = \"html\")\n\n\n\n\n\nEstimate\nX2.5..\nX97.5..\n\n\n\n\nE[Y(inactive)]_m\n0.151\n0.050\n0.242\n\n\nE[Y(active)]_m\n0.188\n0.099\n0.276\n\n\nE[Y(very_active)]_m\n0.179\n0.061\n0.291\n\n\nRD_m\n-0.028\n-0.177\n0.123\n\n\nE[Y(inactive)]_e\n0.004\n-0.031\n0.039\n\n\nE[Y(active)]_e\n-0.017\n-0.041\n0.007\n\n\nE[Y(very_active)]_e\n-0.036\n-0.082\n0.012\n\n\nRD_e\n0.039\n-0.018\n0.099\n\n\nRD_m - RD_e\n-0.068\n-0.227\n0.099\n\n\n\n\n\n\n# This table provides estimated levels of depression, in standard deviation units, for different levels of activity for two groups: Māori (indicated by \"_m\") and NZ Europeans (indicated by \"_e\").\n#\n# The expectations are named as `E[Y(&lt;level of activity&gt;)]_group`, where the level of activity can be `inactive`, `active`, or `very_active`.\n#\n# Here is a breakdown of the results.\n#\n#   1. For the Māori group (`_m`):\n#\n#   - `E[Y(inactive)]_m`: When inactive, the expected level of depression is 0.23 standard deviations, with a 95% confidence interval from 0.116 to 0.356.\n# - `E[Y(active)]_m`: When active, the expected level of depression decreases to 0.193 standard deviations, with a 95% confidence interval from 0.108 to 0.282.\n# - `E[Y(very_active)]_m`: When very active, the expected level of depression further decreases to 0.133 standard deviations, with a 95% confidence interval from 0.009 to 0.262.\n# - `RD_m`: The risk difference (RD) between inactive and very active Māori individuals is 0.097 standard deviations, with a 95% confidence interval from -0.068 to 0.274. This indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# 2. For the NZ European group (`_e`):\n#\n#   - `E[Y(inactive)]_e`: When inactive, the expected level of depression is 0.034 standard deviations, with a 95% confidence interval from -0.012 to 0.078.\n# - `E[Y(active)]_e`: When active, the expected level of depression slightly decreases to -0.006 standard deviations, with a 95% confidence interval from -0.03 to 0.016.\n# - `E[Y(very_active)]_e`: When very active, the expected level of depression further decreases to -0.046 standard deviations, with a 95% confidence interval from -0.086 to -0.007.\n# - `RD_e`: The risk difference (RD) between inactive and very active NZ European individuals is 0.081 standard deviations, with a 95% confidence interval from 0.02 to 0.138. Similar to the Māori group, this indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# The last row, `RD_m - RD_e`, represents the difference in risk differences between Māori and NZ Europeans. It's 0.017 standard deviations with a 95% confidence interval from -0.152 to 0.204. This is not statistically significant (the confidence interval contains 0), suggesting that the difference in depression reduction from being inactive to very active is not significantly different between the two groups.\n#\n# These are estimates and subject to statistical uncertainty. While they suggest a trend, the wide confidence intervals indicate that these estimates come with a degree of uncertainty.\n\n\ndf_dep_plot_data &lt;- df_dep |&gt; \n rename( lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n  mutate_if(is.numeric, round, 3) \n\n\n\nplot_sub_forest &lt;- function(df) {\n  require(ggplot2)\n\n  # Check if required packages are installed\n  required_packages &lt;- c(\"ggplot2\")\n  new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(df))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \", paste(missing_cols, collapse = \", \"))\n  \n  # Order the factor levels by the estimate column in decreasing order\n  \n  ggplot(df, aes(x=estimate, y=factor(row.names(df)))) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height=0.3) +\n    geom_vline(xintercept = 0, linetype=\"dashed\", color = \"red\") +\n    theme_bw() +\n    xlab(\"Estimate\") +\n    ylab(\"\")\n}\nplot_sub_forest(df_dep_plot_data)"
  },
  {
    "objectID": "content/10-content.html#anxiety-analysis-and-results",
    "href": "content/10-content.html#anxiety-analysis-and-results",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Anxiety Analysis and Results",
    "text": "Anxiety Analysis and Results\nLet’s consider a pseudo experiment where someone moves from inactive to active.\nWe will use doubly robust analysis. This means that we estimate a model using both propensity score weights (obtained using the ebalance algortithm) and regression stratification.\nI wrote a function that will caluclate “E-values.” An E-value is defined as the\n\nE-values represent the minimum strength or magnitude of association that an unmeasured confounder would need to have with both treatment and outcome in order to explain the observed effect estimates conditional on measured covariates.\n\n\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n\nCalculate E-values\n\n# read data\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all &lt;- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety &lt;- df_anxiety_all |&gt; \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab &lt;- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n\nConfidence interval crosses the true value, so its E-value is 1.\n\ntest_tab |&gt; kbl(format = \"markdown\")\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n-0.027\n-0.192\n0.112\n1.185\n1.000\n\n\nRD_e\n0.077\n0.023\n0.132\n1.352\n1.166\n\n\n\n\n\nAnxiety results\n\ndf_anxiety_all_plot &lt;- df_anxiety_all |&gt; \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.014    0.205\nE[Y(active)]_m         0.138    0.062    0.218\nE[Y(very_active)]_m    0.134    0.016    0.260\nRD_m                  -0.027   -0.192    0.112\nE[Y(inactive)]_e       0.034    0.001    0.065\nE[Y(active)]_e        -0.021   -0.045    0.002\nE[Y(very_active)]_e   -0.043   -0.091    0.003\nRD_e                   0.077    0.023    0.132\nRD_m - RD_e           -0.104   -0.272    0.047\n\nplot_sub_forest(df_anxiety_all_plot)\n\n\n\ndf_anxiety_all_plot|&gt; \n  kbl(format = \"html\")\n\n\n\n\n\nestimate\nlower_ci\nupper_ci\n\n\n\n\nE[Y(inactive)]_m\n0.108\n0.014\n0.205\n\n\nE[Y(active)]_m\n0.138\n0.062\n0.218\n\n\nE[Y(very_active)]_m\n0.134\n0.016\n0.260\n\n\nRD_m\n-0.027\n-0.192\n0.112\n\n\nE[Y(inactive)]_e\n0.034\n0.001\n0.065\n\n\nE[Y(active)]_e\n-0.021\n-0.045\n0.002\n\n\nE[Y(very_active)]_e\n-0.043\n-0.091\n0.003\n\n\nRD_e\n0.077\n0.023\n0.132\n\n\nRD_m - RD_e\n-0.104\n-0.272\n0.047"
  }
]