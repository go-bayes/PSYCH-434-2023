---
title: "Causal inference: a step by step guide"
date: "2023-MAY-16"
bibliography: references.bib
theme:
  light: flatly
  dark: darkly
format: 
 html:      
  html-math-method: katex
---
  
```{r}
#| include: false
#| echo: false
#read libraries

library("tinytex")
library(extrafont)
loadfonts(device = "all")

# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
#source("/Users/joseph/GIT/templates/functions/funs.R")
```

  
## Overview
  

Recall that psychology begings with a question about cognition and behavior. What do we want to know? Before all else, we must ask, and motivate this question. 

Suppose we have defined a question. How can we address it using observational data? 

This is the topic of today's seminar.
  
  

## How do we do causal estimation?

There are two steps to causal estimation:

1. State a causal question
2. Answer that question [Cite Hernan]



## STEP 1: ASK A CAUSAL QUESTION

Stating a causal question requires describing: a. outcome(s), b. exposure, c. measured confounders, d. (suspected) unmeasured confounders, e. scale of causal contrasts, f. target population for whom the inferences apply. 

We consider each of these processes in turn.



### a. Identify the outcome(s) of interest

We use *Y* to denote an outcome of interest. This is the "effect" of interest.  

#### Consider:

- The outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).
- It is crucial to specify the units in which the outcome is measured. 
- Transforming the outcome into standard deviation units can be beneficial.
- The outcome must occur after the exposure or treatment.
- We must designate a time period within which the outcome occurs, e.g, "the one-year effect of a treatment on well-being as measured by Kessler-6."
- We may be interested in multiple outcomes. This is the rational behind outcomewide science [cite tyler]
- We must remember that the outcome might be measured with error, that such errors may be affected by the treatment or correlated with the measurement error of the treatment. (A topic of future seminars, which we will set to the side for now)


Here, imagine we are interested in understanding only one outcome in our study: well-being as measured by the Kessler-6 depression/anxiety scale. 


###  b. Define the exposure or intervention 

We use *A* to denote the the exposure or treatment. This is the variable that we hypothesise might affect the outcome.  This variable denotes the cause of interest. We will restrict our focus to consider in which there is only one treatment (i.e. we will not consider complex multi-treatment regimes.)

Here, imagine we are interested in understanding the causal effect of 'Church attendance'. 

#### Consider:

- To affect *Y*, *A* must occur before *Y*. 
- What does *A* indicate?  We may characterise 'Church Attendance' as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly). The exposure, in this instance, remains nebulous until we delineate the unit change in *A* we are interested in investigating.
- For theoretical and practical purpose we might want to trunctate the variable into categories: (none/some; none/less than weekly, weekly or more)... Why? Because our causal question requires stating the contrast between the states of the world in which we are interested. Additionally, experts often make decisions on the basis of discrete thresholds (has risk of depression/does not).
- We must remember that an exposure or treatment might be measured with error, that such errors might be correlated with the measurement error of the outcome or affect it. (Again, a topic of future seminars, which we will set to the side for now).
- It must be conceptually possible to intervene within all stratums of baseline confounders (strong positivity).
- It must be conceptually possible to understand an intervention on the exposure (consistency)
- The multiple version of the intervention must exchangeable within levels of measured baseline confounders (consistency).



### c. Identify pre-exposure covariates for confounding control

We use **L** to denote the set of measured baseline confounders of the the exposure-outcome association. For a three-wave panel design, we employ VanderWeele's modified disjunctive cause criterion, we advises: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.

#### Consider: 

  - To affect *Y*,  *L* must occur before *Y*.  
  - To affect *A*, *L* must occur before *A*. 
  - We must acheive balence within levels of the exposure such that $A\coprod Y| L$
  - Although we may gain precision by including an *L* that affects *Y* but is unrelated to *A*, including variables that occur after *A* will be hazardous if it is possible that *A* affects *L* (or its measurement).
  - A useful  set of default counfounders in NZAVS studies is given in your workbooks. 
  - Note we have left out the concept of "selection bias." There are indeed sources of bias that may occure after *A*. This is another topic for the week ahead. 


### d. Highlight unmeasured pre-treatment covariates

Let **U** denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between *A* and *Y* independently of the measured covariates. 

#### Consider:
  - To affect *Y* and *A*, *U* must occur before *A*.
  - It is useful to draw a causal diagramme to illustrate all potential sources of bias. 
  - Causal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.
  - A causal diagramme should include only as much information as is required to assess confounding. See @fig-dag-outcomewide for an example.
  - Because we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar. 

```{tikz}
#| label: fig-dag-outcomewide
#| fig-cap: "Causal graph: three-wave panel design." 
#| out-width: 100%
#| echo: false

\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]
\node [rectangle, draw=white] (U) at (0, 0) {U};
\node [rectangle, draw=black, align=left] (L) at (2, 0) {t0/L \\t0/A \\t0/Y};
\node [rectangle, draw=white] (A) at (4, 0) {t1/A};
\node [ellipse, draw=white] (Y) at (6, 0) {t2/Y};
\draw [-latex, draw=black] (U) to (L);
\draw [-latex, draw=black] (L) to (A);
\draw [-latex, draw=red, dotted] (A) to (Y);
\draw [-latex, bend left=50, draw =black] (L) to (Y);
\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);
\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);


\end{tikzpicture}
```


### e. Choose the scale for a causal contrast 

Average causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, $E[Y(A = a)]$, with the expected outcome under a different exposure level, $E[Y(A=a')]$. 

For a binary treatment with levels $A=0$ and $A=1$, the Average Treatment Effect (ATE), on the difference scale, is expressed:

$$ATE_{\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]$$

On the risk ratio scale, the ATE is expressed:

$$ATE_{\text{risk ratio}} = \frac{E[Y(1)|L]}{E[Y(0)|L]}$$

Other effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) : 

$$ATT_{\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]$$

$$ATT_{\text{risk ratio}} = \frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}$$


Another common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:

$$PATE_{\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)$$

$$PATE_{\text{risk ratio}} = f\left(\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\right)$$

where $f$ is a function that incorporates weights $W$ into the estimation of the expected outcomes. These weights may correspond to the inverse probability of being sampled or in the case of NZAVS data, the survey weights are given from census estimates for the wider population.  Note: I will show you how to use weights in future seminars.

We might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators.  Denote a stratum of interest by $G$. We may then compute: 

$$ATE_{G,\text{risk difference}} = E[Y(1) - Y(0)|G, L]$$

$$ATE_{G,\text{risk ratio}} = \frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}$$


#### Consider: 

- ** In this course, we are interested in stratum specific comparisons **
- In the causal inference literature, the concept we use to make sense of stratum specific comparisons is called "effect modification." 
- By inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement. 
- The logic of effect modification differs from that of intereaction. 




#### Aside: extensions

For continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):

$$ATE_{A,A'} = E[Y(A) - Y(A')| L]$$

This essentially denotes an average treatment effect comparing the outcome under treatment level $A$ to the outcome under treatment level $A'$.

Likewise:

$$ATE_{A/A'} = \frac{E[Y(A)| L]}{E[Y(A')| L]}$$

This defines the contrast of $A$ and $A'$ on a ratio scale. 

#### f. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations. 


Consider the following concepts:

- **Source population**: A source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.


- **Target population**: The target population is the larger group we aim to apply our study's results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.


  - **Generalisability** refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called "external validity".  

$$\text{Generalisability} = PATE \approx ATE_{\text{sample}}$$

  - **Transportability** refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.

$$\text{Transportability} = ATE_{\text{target}} \approx f(ATE_{\text{source}}, T)$$

where $f$ is a function and $T$ is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.

### Summary Step 1: Consider how much we need to do when asking a causal question!

We discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: "How may we answer this causal question?" 


## STEP 2: ANSWER A CAUSAL QUESTION

#### Obtain longitudinal data

Note that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study. 

Recall we have defined. 

- **A**: Our exposure or treatment variable, denoted as **A**.  Here we consider the example of 'Church attendance'.

- **Y**: The outcome variable we are interested in, represented by **Y**, is psychological distress. We operationalise this variable through the 'Kessler-6' distress scale.

- **L**: The confounding variables, collectively referred to as **L**, represent factors that can independently influence both **A** and **Y**. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.


Given the importance of temporal ordering, we must now define time: 


- **t $\in$ T**: Let $t$ denote within a multiwave panel study with **T** measurement intervals.
  
Where $t/\text{{exposure}}$ denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that: 

$$t_{confounders} < t_{exposure}< t_{outcome}$$ 

To minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of $\mathbf{L}$ preceding $A$ and the latest values of $A$ before $Y$.

Note in @fig-dag-outcomewide,  We use the prefixes "t0, t1, and t2" to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of *A* and *Y*. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.


### Include the measured exposure with baseline covariates

Controlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the "incidence exposure" rather than the "prevalence exposure" [Hernan Danaei, Tavakkoli and Hernán, 2012, Hernán, 2015]. It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to dismiss the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.

### State the eligibility criteria for participation

This step is invaluable for assessing whether we are answering the causal question that we have asked. 

#### Consider: 

  - Generalisability: we cannot evaluate inferences to a target group from the source popuation if we do not describe the source population
  - Eligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.

 For example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country's electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results? 

  - (Aside: discuss per protocol effects/ intention to treat effects) 

### Determine how missing data will be handled

  - As we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data. 
  

### State a statistical model  

The models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.  


### **G-computation for Subgroup Analysis Algorithm**

**Step 1** Estimate the outcome model. Fit a model for the outcome $Y$, conditional on both the exposure $A$, the covariates $L$, and subgroup indicator $G$. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.

$$ E(Y|A,L,G) = f_Y(A,L,G, \theta_Y) $$ 

This equation represents the expected value of the outcome $Y$ given the exposure $A$, covariates $L$, and subgroup $G$ as modeled by the function $f_Y$ with parameters $\theta_Y$. This formulation allows for the prediction of the average outcome $Y$ given certain values of $A$, $L$, and $G$. 

**Step 2** Simulate potential outcomes. For each individual $i$ in each subgroup $g$, predict their potential outcome $\hat{Y}_{i,g}(a)$ under the intervention $A=a$ using the estimated outcome model.

$$\hat{Y}_{i,g}(a) = E[Y|A=a,L_i,G=g; \hat{\theta}_Y]$$

and also under the intervention $A=a^{\prime}$:

$$\hat{Y}_{i,g}(a^{\prime}) = E[Y|A=a^{\prime},L_i,G=g; \hat{\theta}_Y]$$

This step involves simulating the outcomes under each intervention level for every individual within each subgroup, as if they were exposed to treatment level $a$ or $a^{\prime}$. 

**Step 3** Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup $g$:

$$\hat{E}[Y(a)|G=g] = \frac{1}{N_g}\sum_{i:G_i=g}^N \hat{Y}_{i,g}(a)$$

$$\hat{E}[Y(a^{\prime})|G=g] = \frac{1}{N_g}\sum_{i:G_i=g}^N \hat{Y}_{i,g}(a^{\prime})$$

and calculate the estimated difference for each subgroup $g$:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^{\prime})|G=g]$$

This difference represents the average causal effect of changing the exposure from level $a^{\prime}$ to level $a$ within subgroup $g$.

**Step 5** Compare differences in causal effects by subgroups. 


Compute the differences in the estimated causal effects between different subgroups:

$$\hat{\gamma}_s = \hat{\delta}_g - \hat{\delta}_{g^{\prime}}$$

This difference $\hat{\gamma}_s$ represents the difference in the average causal effects between the subgroups $g$ and $g^{\prime}$. 


Or more fully: 


$$\hat{\gamma}_{s} = \overbrace{\big( E[Y(a)|G=g] - E[Y(a^{\prime})|G=g] \big)}^{\hat{\delta_g}} - \overbrace{\big(E[Y(a^{\prime})|G=g^{\prime}]- E[Y(a)|G=g^{\prime}]\big)}^{\hat{\delta_{g^{\prime}}}} $$



This difference, $\gamma_s$, represents the difference in the average causal effect of changing the exposure from level $a^{\prime}$ to level $a$ between subgroup $g$ and subgroup $g^{\prime}$. It measures the interaction effect of the exposure $A$ and the subgroup $G$ on the outcome $Y$. 


We use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023] 



### **Inverse Probability of Treatment Weighting (IPTW) for Subgroup Analysis Algorithm**

**Step 1** Estimate the propensity score. The propensity score $e(L_i, G)$ is the conditional probability of the exposure $A_i = 1$, given the covariates $L_i$ and subgroup indicator $G$. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.

$$ e(L_i, G) = P(A_i = 1 | L_i, G) = f_A(L_i, G; \theta_A) $$

Here, $f_A(L_i, G; \theta_A)$ is a function (statistical model) that estimates the probability of the exposure $A_i = 1$ given covariates $L_i$ and subgroup $G$. The estimated propensity scores are denoted as $\hat{e}(L_i, G_i)$.

Then, we calculate the weights for each individual $i$, denoted as $v_i$, using the estimated propensity score:

$$v_i = \left(\frac{A_i}{\hat{e}(L_i)} + \frac{1 - A_i}{1 - \hat{e}(L_i)}\right)\bigg|_{G}$$

Note that we estimate propensity scores *separately* within strata of the subgroup for whom we are interested in effect modification."

$v_i$ is the weight for individual $i$ in a given subgroup $G$. It calculated using the propensity score estimated within the same subgroup, these weights are then recombined into the full data set with all groups. 

**Step 2** Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome $Y$, conditional on the exposure $A$, subgroup $G$, and covariates $L$:

$$ \hat{E}(Y|A, G, L) = f_Y(A, G; \theta_Y, V) $$ 

where $f_Y$ is a weighted function (such as a weighted regression model) with parameters $\theta_Y$, and $V$ represents the weights. This formulation allows for the prediction of the outcome $Y$ given certain values of $A$, $G$, and $L$, taking into account the inverse probability of treatment weights.

**Step 3** Simulate potential outcomes. For each individual $i$ in each subgroup $g$, simulate their potential outcome $\hat{Y}_{i,g}(a)$ under the hypothetical scenario where everyone in the subgroup is exposed to the intervention $A=a$ regardless of their actual exposure level:

$$\hat{Y}_{i,g}(a) = E[Y|A=a,G=g; \hat{\theta}_Y, v_i]$$

and also under the hypothetical scenario where everyone is exposed to intervention $A=a^{\prime}$:

$$\hat{Y}_{i,g}(a^{\prime}) = E[Y|A=a^{\prime},G=g; \hat{\theta}_Y, v_i]$$

**Step 4** Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup $g$:

$$\hat{E}[Y(a)|G=g] = \sum_{i:G_i=g}^N \hat{Y}_{i,g}(a)$$

$$\hat{E}[Y(a^{\prime})|G=g] = \sum_{i:G_i=g}^N \hat{Y}_{i,g}(a^{\prime})$$

Then calculate the estimated difference for each subgroup $g$:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^{\prime})|G=g]$$

The difference $\hat{\delta}_g$ represents the average causal effect of changing the exposure from level $a^{\prime}$ to level $a$ within each subgroup $g$. 


**Step 5** Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:

$$\hat{\gamma}_s = \hat{\delta}_g - \hat{\delta}_{g^{\prime}}$$

This difference $\hat{\gamma}_s$ represents the difference in the average causal effects between the subgroups $g$ and $g^{\prime}$. 

We compute standard errors and confidence intervals. Again, to obtain standard errors and confidence intervals for these differences in subgroup-specific causal effects, we use simulation-based inference methods [@greifer2023]. 


### Doubly Robust Estimation for Subgroup Analysis Algorithm

Doubly Robust Estimation is a powerful technique that combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.

**Step 1** Estimate the propensity score. The propensity score $e(L_i, G)$ is the conditional probability of the exposure $A_i = 1$, given the covariates $L_i$ and subgroup indicator $G$. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.

$$ e(L_i, G) = P(A_i = 1 | L_i, G) = f_A(L_i, G; \theta_A) $$

Here, $f_A(L_i, G; \theta_A)$ is a function (statistical model) that estimates the probability of the exposure $A_i = 1$ given covariates $L_i$ and subgroup $G$. The estimated propensity scores are denoted as $\hat{e}(L_i, G_i)$.

Then, we calculate the weights for each individual $i$, denoted as $v_i$, using the estimated propensity score:

$$v_i = \left(\frac{A_i}{\hat{e}(L_i)} + \frac{1 - A_i}{1 - \hat{e}(L_i)}\right)\bigg|_{G}$$

Note that we estimate propensity scores **separately** within strata of the subgroup for whom we are interested in effect modification."

$v_i$ is the weight for individual $i$ in a given subgroup $G$. It calculated using the propensity score estimated within the same subgroup, these weights are then recombined into the full data set with all groups.


**Step 2** Estimate the outcome model. 

Fit a model for the outcome $Y$, conditional on both the exposure $A$, the covariates $L$, subgroup indicator $G$, and the propensity scores $V$. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.

$$ E(Y|A,L,V,G) = f_Y(A,L,V,G, \theta_Y) $$ 

This equation represents the expected value of the outcome $Y$ given the exposure $A$, covariates $L$, the propensity score $v_i$, and subgroup $G$ as modeled by the function $f_Y$ with parameters $\theta_Y$. This formulation allows for the prediction of the average outcome $Y$ given certain values of $A$, $L$, $V$,and $G$. 


**Step 3** Simulate potential outcomes. For each individual $i$ in each subgroup $g$, simulate their potential outcome $\hat{Y}_{i,g}(a)$ under the hypothetical scenario where everyone in the subgroup is exposed to the intervention $A=a$ regardless of their actual exposure level:

$$\hat{Y}_{i,g}(a) = E[Y|A=a,G=g; \hat{\theta}_Y,L_i, v_i]$$

and also under the hypothetical scenario where everyone is exposed to intervention $A=a^{\prime}$:

$$\hat{Y}_{i,g}(a^{\prime}) = E[Y|A=a^{\prime},G=g; \hat{\theta}_Y,,L_i, v_i]$$


**Step 4** Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup $g$:

$$\hat{E}[Y(a)|G=g] = \sum_{i:G_i=g}^N \hat{Y}_{i,g}(a)$$

$$\hat{E}[Y(a^{\prime})|G=g] = \sum_{i:G_i=g}^N  \hat{Y}_{i,g}(a^{\prime})$$

Then calculate the estimated difference for each subgroup $g$:

$$\hat{\delta}_g = \hat{E}[Y(a)|G=g] - \hat{E}[Y(a^{\prime})|G=g]$$

The difference $\hat{\delta}_g$ represents the average causal effect of changing the exposure from level $a^{\prime}$ to level $a$ within each subgroup $g$. 


**Step 5** Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:

$$\hat{\gamma}_s = \hat{\delta}_g - \hat{\delta}_{g^{\prime}}$$

This difference $\hat{\gamma}_s$ represents the difference in the average causal effects between the subgroups $g$ and $g^{\prime}$. 

Again we can see this explicitly:


$$\hat{\gamma}_{s} = \overbrace{\big( E[Y(a)|G=g] - E[Y(a^{\prime})|G=g] \big)}^{\hat{\delta_g}} - \overbrace{\big(E[Y(a^{\prime})|G=g^{\prime}]- E[Y(a)|G=g^{\prime}]\big)}^{\hat{\delta_{g^{\prime}}}} $$


We again calculate standard errors and confidence intervals using simulation-based inference methods [@greifer2023]. This helps to quantify the uncertainty around the estimated causal effects.





### Provided Detailed Reporting: Checklist as follows

#### Have a question

  - Did I state my question? No$\to$state it. Yes$\to$proceed.
  - So what? Did I explain why this question is important? No-> explain. Yes $\to$ proceed.
  - Do I have a causal question. No $\to$ Stop. Your question might not be important, return to the previous step.   Yes$\to$Proceed.
  - Do you have a question about a subgroup -- for example a cultural group? Yes$\to$proceed. No that's okay but your assessment requires a subgroup analysis. Return to the start and develop a subgroup analysis question then $\to$ proceed. 
  - Are you familiar with the potential outcomes framework such that you can explain the following: why are individual causal effects not identified? how do experiments obtain average causal effects? Can you roughly explain the fundamental assumptions of causal inference? Can you roughly explain how causal effects are estimated in observational data by predicting potential outcomes for everyone under specific levels of an exposure, regardless of whether they recieved it? No $\to$ study your notes and the course materials. If you still can't understand $\to$ see me.  Yes $\to$ proceed.
  
#### Data  

  - Do I have experimental data? Yes $\to$ You are not in this course. No $\to$ you must emulate a target trial, proceed.
  - Do I have time-series data. No $\to$ stop, you should not ask a causal question. Yes $\to$ proceed. 
  - Do have at least three-waves of data.  No $\to$ stop, danger zone. Confounding control requires much stronger assumptions. Yes $\to$ proceed. 
  - Are the time-series data the NZAVS simulated data? No $\to$ Talk to me. Yes $\to$ proceed. 
  
#### Outcome 

  - Have you defined the outcome variable *Y*? No $\to$write it down. Yes $\to$ proceed. 
  - Is there more than one outcome? No $\to$ proceed. Yes, write them down, $\to$ proceed.
   - Can you explain how your outcome variable/s are relevant to your question?   Only if yes $\to$ proceed. 
  - Do you have a binary outcome? Yes $\to$ Is it rare? $\to$ you may use logistic regression, note this, then $\to$ proceed. No? Is your outcome continuous? Consider whether to z-transform it, consider whether to truncate it into categories (after expert consultation). Note this, then $\to$ proceed. 
  - Does your outcome appear after your exposure? Yes $\to$ proceed. 

  
#### Exposure 

 - Have you defined the exposure variable *A*? No $\to$write it down. Yes $\to$ proceed. 
  - Is there more than one exposure? No $\to$ stop, you're confused. Yes $\to$ proceed.
 - Can you explain how your exposure variable is relevant to your question?   Only if yes $\to$ proceed. 
 - Is it conceptually possible to intervene on your exposure at all levels of the covariates (positivity)? Only if yes $\to$ proceed. 
 - Can you interpret what it would mean to intervene on the exposure (consistency)?   Only if yes $\to$ proceed. 
 - Are the different versions of the exposure conditionally exchangeable conditional on measured baseline confounders.   Only if yes $\to$ proceed. 
 - Do you have a binary exposure? Yes note this, then $\to$ proceed. No? Is your exposure continuous?  Z-transform it, Consider whether to truncate it into categories (after expert consultation). Note this, then $\to$ proceed. 
 - Does your exposure appear after before your outcome? Only if yes $\to$ proceed. 
 


#### Confounders 

- Have you defined your baseline confounders $L$ Only if yes $\to$ proceed. 
- Have you justified why the baseline confounders may affect both $A$ and $Y$. Only if yes $\to$ proceed. 
- Are the baseline confounders measured before the exposure. Only if yes $\to$ proceed. 
- Is the baseline measure of the exposure included in the set of baseline confounders? Only if yes $\to$ proceed. 
Is the baseline outcome included in the set of baseline confounders? Only if yes $\to$ proceed. 
- Are the baseline confounders sufficient to ensure balance on the exposure, such that $A \coprod Y|L$. If not, write this down and clarify how you will address the problem in your sensitivity analysis, then $\to$ proceed. 
- Are the confounders continuous? $\to$ convert them to z-scores. 
- Are the confounders binary? You may convert them to z-scores
- Are the confounders categorical and have three or more levels? Do not convert them to z-scores.  You may now $\to$ proceed. 


#### Causal Diagramme with Unmeasured Confounders

 
- Do previous science suggestion there may be unmeasured confounders? No $\to$ learn more science. Yes $\to$ briefly explain and $\to$ proceed. 
- Have you drawn a causal diagramme (DAG) to clarify both measured and unmeasured sources of confounding? Only if yes $\to$ proceed. 
- Is there a possiblity of M-Bias?  I don't know what M-bias is. That's OK, we'll discuss later $\to$ proceed. 
- Have I described sources of bias from measurement error differential or correlated measurement errors between the exposure and outcome?  That's OK, we'll discuss later$\to$ proceed. 
- Do the nodes on my DAG have time indicators so that I can ensure temporal order is correct? Only if yes $\to$ proceed? 
- Is my DAG organised so that time follows in consistent direction? No?  That's OK, but as you get more experienced you will do better, for now $\to$ proceed? 


#### Estimand


- Is my causal estimand one of these: 



$$ATE_{G,\text{risk difference}} = E[Y(1) - Y(0)|G, L]$$

$$ATE_{G,\text{risk ratio}} = \frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}$$


if yes, good you are in the right course. You may $\to$ proceed.


#### What is my source and target population? 

-  Have I distinguished between my source and target population? Only if yes $\to$ proceed.
-  Have I considered whether my results generalise to the source population? Have I considered whether my results transport to a different population? Only if yes $\to$ proceed.

#### Eligibility criteria? 

- Have I stated eligibility criteria for the study? Only if yes $\to$ proceed.

### Desription of the sample characteristics

- Have I provided the desciptive statistics for demographic information taken at baseline? Only if yes $\to$ proceed.
- Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? Only if yes $\to$ proceed.
- Have I included references to where my audience can find more information about the sample? Only if yes $\to$ proceed.
- 


### Inference
- Take care when describing for whome the results generalise. 


<!-- ### What we have not done -->

<!-- We have not addressed:  -->

<!-- - measurement error. Our graphs do not incorporate it.  -->
<!-- - selection bias -->
<!-- - Outcome-wide studies -->
<!-- - Sampling weights.  -->




